{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Serialization and Deserialization in PySpark\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **What serialization and deserialization are** and why they matter\n",
        "2. **Why serialization is critical** in distributed systems like Spark\n",
        "3. **How data moves** between processes, machines, and storage\n",
        "4. **Serialization formats** used in Spark (Java vs Kryo)\n",
        "5. **Performance implications** of serialization choices\n",
        "6. **Best practices** for serialization in Spark applications\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Basic understanding of Python programming\n",
        "- Understanding of Spark architecture (executors, cores, tasks) - see `08_a_Spark_Architecture.ipynb`\n",
        "- Understanding of caching and persist - see `8_d_Caching_Persist.ipynb`\n",
        "- Basic familiarity with distributed computing concepts\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:** This notebook explains a fundamental concept that underlies many Spark operations. Understanding serialization will help you optimize performance and debug issues in distributed systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction: What is Serialization?\n",
        "\n",
        "### The Real-World Analogy\n",
        "\n",
        "**Think of serialization like packing for a trip:**\n",
        "\n",
        "**Serialization (Packing):**\n",
        "- You have items in your room (data in memory)\n",
        "- You pack them into a suitcase (convert to bytes)\n",
        "- The suitcase can be transported (sent over network or saved to disk)\n",
        "\n",
        "**Deserialization (Unpacking):**\n",
        "- You receive a suitcase (bytes from network/disk)\n",
        "- You unpack it (convert back to objects)\n",
        "- Items are back in your room (data in memory)\n",
        "\n",
        "### Technical Definition\n",
        "\n",
        "**Serialization:**\n",
        "> The process of converting an object (data structure) into a format that can be stored (disk) or transmitted (network) and later reconstructed.\n",
        "\n",
        "**Deserialization:**\n",
        "> The process of converting serialized data back into an object (data structure) that can be used in your program.\n",
        "\n",
        "### Why Does This Matter?\n",
        "\n",
        "**In a single computer:**\n",
        "- Objects exist in memory\n",
        "- You can pass references directly\n",
        "- No serialization needed\n",
        "\n",
        "**In distributed systems (like Spark):**\n",
        "- Data needs to move between machines\n",
        "- Data needs to be stored on disk\n",
        "- Data needs to be sent over network\n",
        "- **Serialization is essential!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Serialization Matters in Spark\n",
        "\n",
        "### Spark's Distributed Nature\n",
        "\n",
        "**Spark runs on multiple machines:**\n",
        "- **Driver:** Your main program (on one machine)\n",
        "- **Executors:** Worker processes (on multiple machines)\n",
        "- **Network:** Data must travel between them\n",
        "\n",
        "### When Serialization Happens in Spark\n",
        "\n",
        "**1. Shuffling Data (Network Transfer)**\n",
        "```python\n",
        "# When you do a join or groupBy, data moves between executors\n",
        "df1.join(df2, on=\"key\")  # Data serialized â†’ sent over network â†’ deserialized\n",
        "```\n",
        "\n",
        "**2. Caching Data (Storage)**\n",
        "```python\n",
        "# When you cache, data is serialized for storage\n",
        "df.cache()  # Data serialized â†’ stored in memory/disk â†’ deserialized when used\n",
        "```\n",
        "\n",
        "**3. Broadcasting Variables**\n",
        "```python\n",
        "# Small data sent to all executors\n",
        "broadcast_var = spark.sparkContext.broadcast(small_data)\n",
        "# Serialized â†’ sent to all executors â†’ deserialized on each\n",
        "```\n",
        "\n",
        "**4. User-Defined Functions (UDFs)**\n",
        "```python\n",
        "# Function code and data serialized to executors\n",
        "from pyspark.sql.functions import udf\n",
        "my_udf = udf(lambda x: x * 2)\n",
        "# Function serialized â†’ sent to executors â†’ deserialized\n",
        "```\n",
        "\n",
        "### The Cost of Serialization\n",
        "\n",
        "**Serialization overhead:**\n",
        "- â±ï¸ **Time:** Converting objects to bytes takes time\n",
        "- ðŸ’¾ **Space:** Serialized data may be larger or smaller than original\n",
        "- ðŸ”„ **CPU:** Both serialization and deserialization use CPU\n",
        "\n",
        "**Key Insight:**\n",
        "> **In distributed systems, serialization happens constantly. Choosing the right serialization format can significantly impact performance!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Serialization Process\n",
        "\n",
        "### Step-by-Step: What Happens When Data Moves\n",
        "\n",
        "**Scenario: Shuffling data during a join**\n",
        "\n",
        "```\n",
        "Step 1: Data in Executor 1 (Python objects in memory)\n",
        "        â†“\n",
        "        [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n",
        "        \n",
        "Step 2: Serialization (Convert to bytes)\n",
        "        â†“\n",
        "        b'\\x00\\x01\\x00\\x02...'  (binary representation)\n",
        "        \n",
        "Step 3: Network Transfer\n",
        "        â†“\n",
        "        Bytes sent over network to Executor 2\n",
        "        \n",
        "Step 4: Deserialization (Convert back to objects)\n",
        "        â†“\n",
        "        [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n",
        "        \n",
        "Step 5: Data in Executor 2 (Python objects in memory)\n",
        "        â†“\n",
        "        Ready to use!\n",
        "```\n",
        "\n",
        "### Visual Representation\n",
        "\n",
        "```\n",
        "Executor 1                    Network                    Executor 2\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Python      â”‚                                    â”‚             â”‚\n",
        "â”‚ Objects     â”‚  Serialize  â†’  [Bytes]  â†’  Network â”‚             â”‚\n",
        "â”‚ in Memory   â”‚                                    â”‚             â”‚\n",
        "â”‚             â”‚                                    â”‚             â”‚\n",
        "â”‚ [Data]      â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  [Bytes]  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  [Data]     â”‚\n",
        "â”‚             â”‚                                    â”‚             â”‚\n",
        "â”‚             â”‚  Deserialize                       â”‚             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Key Points\n",
        "\n",
        "1. **Serialization is one-way:** Objects â†’ Bytes\n",
        "2. **Deserialization is reverse:** Bytes â†’ Objects\n",
        "3. **Format matters:** Different formats have different trade-offs\n",
        "4. **Happens automatically:** Spark handles it, but you can optimize it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serialization Formats in Spark\n",
        "\n",
        "### Two Main Options\n",
        "\n",
        "**1. Java Serialization (Default)**\n",
        "- Built into Java\n",
        "- Works with any Java object\n",
        "- **Slower and produces larger files**\n",
        "\n",
        "**2. Kryo Serialization (Recommended)**\n",
        "- Faster and more efficient\n",
        "- Produces smaller serialized data\n",
        "- **Requires registration of custom classes**\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| Aspect | Java Serialization | Kryo Serialization |\n",
        "|--------|-------------------|-------------------|\n",
        "| **Speed** | ðŸŒ Slower | ðŸš€ Faster (2-10Ã—) |\n",
        "| **Size** | ðŸ“¦ Larger files | ðŸ“¦ Smaller files (2-5Ã—) |\n",
        "| **Setup** | âœ… No setup needed | âš ï¸ Requires registration |\n",
        "| **Compatibility** | âœ… Works with everything | âš ï¸ Need to register custom classes |\n",
        "| **Default** | âœ… Yes (default) | âŒ No (must enable) |\n",
        "\n",
        "### When to Use Each\n",
        "\n",
        "**Use Java Serialization when:**\n",
        "- You're prototyping or learning\n",
        "- You have complex custom classes\n",
        "- You don't want to register classes\n",
        "\n",
        "**Use Kryo Serialization when:**\n",
        "- Performance matters (production)\n",
        "- You're doing lots of shuffling\n",
        "- You're caching large datasets\n",
        "- You can register your custom classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Example: Understanding Serialization in Action\n",
        "\n",
        "Let's see serialization in action with a practical example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/03 06:22:42 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
            "26/01/03 06:22:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "26/01/03 06:22:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/01/03 06:22:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SPARK SESSION INITIALIZED\n",
            "======================================================================\n",
            "Spark Version: 3.5.1\n",
            "Default Serializer: Java Serialization\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create Spark session with default serialization\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SerializationDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SPARK SESSION INITIALIZED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Default Serializer: Java Serialization\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Understanding When Serialization Happens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "UNDERSTANDING WHEN SERIALIZATION HAPPENS\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 0:>                                                        (0 + 11) / 11]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataFrame 1: 1000 rows\n",
            "DataFrame 2: 1000 rows\n",
            "\n",
            "ðŸ’¡ When we join these DataFrames:\n",
            "   1. Data is serialized on each executor\n",
            "   2. Serialized data is sent over network (shuffle)\n",
            "   3. Data is deserialized on receiving executors\n",
            "   4. Join operation happens\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create sample data\n",
        "print(\"=\" * 70)\n",
        "print(\"UNDERSTANDING WHEN SERIALIZATION HAPPENS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create DataFrames\n",
        "data1 = [(i, f\"Product_{i}\", 100.0 + i) for i in range(1000)]\n",
        "data2 = [(i, f\"Category_{i % 10}\") for i in range(1000)]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, [\"id\", \"product\", \"price\"])\n",
        "df2 = spark.createDataFrame(data2, [\"id\", \"category\"])\n",
        "\n",
        "print(f\"\\nDataFrame 1: {df1.count()} rows\")\n",
        "print(f\"DataFrame 2: {df2.count()} rows\")\n",
        "\n",
        "print(\"\\nðŸ’¡ When we join these DataFrames:\")\n",
        "print(\"   1. Data is serialized on each executor\")\n",
        "print(\"   2. Serialized data is sent over network (shuffle)\")\n",
        "print(\"   3. Data is deserialized on receiving executors\")\n",
        "print(\"   4. Join operation happens\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Demonstrating Serialization in Shuffles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DEMONSTRATING SERIALIZATION IN SHUFFLES\n",
            "======================================================================\n",
            "\n",
            "Performing join (triggers shuffle with serialization)...\n",
            "\n",
            "âœ… Join completed!\n",
            "   â€¢ Result: 1,000 rows\n",
            "   â€¢ Time: 0.733 seconds\n",
            "   â€¢ During this operation:\n",
            "     - Data was serialized on source executors\n",
            "     - Serialized data was shuffled over network\n",
            "     - Data was deserialized on target executors\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Join operation - this triggers serialization/deserialization\n",
        "print(\"=\" * 70)\n",
        "print(\"DEMONSTRATING SERIALIZATION IN SHUFFLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nPerforming join (triggers shuffle with serialization)...\")\n",
        "start = time.time()\n",
        "joined_df = df1.join(df2, on=\"id\", how=\"inner\")\n",
        "result_count = joined_df.count()\n",
        "join_time = time.time() - start\n",
        "\n",
        "print(f\"\\nâœ… Join completed!\")\n",
        "print(f\"   â€¢ Result: {result_count:,} rows\")\n",
        "print(f\"   â€¢ Time: {join_time:.3f} seconds\")\n",
        "print(f\"   â€¢ During this operation:\")\n",
        "print(f\"     - Data was serialized on source executors\")\n",
        "print(f\"     - Serialized data was shuffled over network\")\n",
        "print(f\"     - Data was deserialized on target executors\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Serialization in Caching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SERIALIZATION IN CACHING\n",
            "======================================================================\n",
            "\n",
            "Caching DataFrame (involves serialization)...\n",
            "Triggering cache with action...\n",
            "\n",
            "âœ… Cache operation completed!\n",
            "   â€¢ Time: 1.137 seconds\n",
            "   â€¢ What happened:\n",
            "     1. Data was serialized (based on storage level)\n",
            "     2. Serialized data was stored in memory/disk\n",
            "     3. When you use it again, it will be deserialized\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, product: string, price: double, category: string]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Caching also involves serialization\n",
        "print(\"=\" * 70)\n",
        "print(\"SERIALIZATION IN CACHING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Cache the joined DataFrame\n",
        "print(\"\\nCaching DataFrame (involves serialization)...\")\n",
        "cached_df = joined_df.cache()\n",
        "\n",
        "# First action - triggers serialization and storage\n",
        "print(\"Triggering cache with action...\")\n",
        "start = time.time()\n",
        "_ = cached_df.count()\n",
        "cache_time = time.time() - start\n",
        "\n",
        "print(f\"\\nâœ… Cache operation completed!\")\n",
        "print(f\"   â€¢ Time: {cache_time:.3f} seconds\")\n",
        "print(f\"   â€¢ What happened:\")\n",
        "print(f\"     1. Data was serialized (based on storage level)\")\n",
        "print(f\"     2. Serialized data was stored in memory/disk\")\n",
        "print(f\"     3. When you use it again, it will be deserialized\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Clean up\n",
        "cached_df.unpersist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storage Levels and Serialization\n",
        "\n",
        "### âš ï¸ Important: PySpark-Specific Behavior\n",
        "\n",
        "**In PySpark, DataFrames are already stored in a serialized format internally:**\n",
        "- PySpark DataFrames are built on RDDs of Row objects\n",
        "- These are already serialized for efficient storage\n",
        "- The storage level determines **WHERE** data is stored, not the format\n",
        "\n",
        "**Note:** The `MEMORY_ONLY_SER` and `MEMORY_AND_DISK_SER` storage levels are available in Scala/Java Spark but are not commonly used in PySpark because DataFrames are already serialized.\n",
        "\n",
        "### How Storage Levels Affect Data Storage\n",
        "\n",
        "**Available storage levels in PySpark:**\n",
        "\n",
        "| Storage Level | Storage Location | Characteristics |\n",
        "|--------------|------------------|-----------------|\n",
        "| `MEMORY_ONLY` | Memory only | Fastest access, but data lost if memory full |\n",
        "| `MEMORY_AND_DISK` | Memory + Disk fallback | Fast if in memory, safe fallback to disk |\n",
        "| `DISK_ONLY` | Disk only | Slower, but always available |\n",
        "\n",
        "### Understanding Storage Levels\n",
        "\n",
        "**MEMORY_ONLY:**\n",
        "```python\n",
        "df.persist(StorageLevel.MEMORY_ONLY)\n",
        "```\n",
        "- âœ… Fastest access (data in memory)\n",
        "- âŒ If memory full, data is lost (not cached)\n",
        "- âŒ No fallback to disk\n",
        "- Use when: Data fits in memory, you need maximum speed\n",
        "\n",
        "**MEMORY_AND_DISK (Default for cache()):**\n",
        "```python\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)  # Same as df.cache()\n",
        "```\n",
        "- âœ… Fast access when in memory\n",
        "- âœ… Falls back to disk if memory full\n",
        "- âœ… No data loss\n",
        "- Use when: Default choice, production-safe\n",
        "\n",
        "**DISK_ONLY:**\n",
        "```python\n",
        "df.persist(StorageLevel.DISK_ONLY)\n",
        "```\n",
        "- âœ… Always available (no memory pressure)\n",
        "- âŒ Slower access (disk I/O)\n",
        "- Use when: Memory is constrained, data is too large for memory\n",
        "\n",
        "### When to Use Each\n",
        "\n",
        "**Use MEMORY_ONLY when:**\n",
        "- Data fits comfortably in memory\n",
        "- You need maximum speed\n",
        "- You can tolerate data loss if memory fills up\n",
        "\n",
        "**Use MEMORY_AND_DISK when:**\n",
        "- Default choice for most cases\n",
        "- You want safety (disk fallback)\n",
        "- Production environments\n",
        "\n",
        "**Use DISK_ONLY when:**\n",
        "- Memory is very limited\n",
        "- Data is too large for memory\n",
        "- You can tolerate slower access\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Example: Comparing Storage Levels\n",
        "\n",
        "Let's see the difference between serialized and deserialized storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "COMPARING DIFFERENT STORAGE LEVELS\n",
            "======================================================================\n",
            "\n",
            "Dataset: 50,000 rows\n",
            "\n",
            "1ï¸âƒ£  Testing MEMORY_ONLY (Memory only, fast access)...\n",
            "   â€¢ Cache time: 0.350s\n",
            "   â€¢ Access time: 0.058s (fast - data in memory)\n",
            "   â€¢ Storage: Memory only (fastest, but may be lost if memory full)\n",
            "\n",
            "2ï¸âƒ£  Testing MEMORY_AND_DISK (Memory + Disk fallback)...\n",
            "   â€¢ Cache time: 0.258s\n",
            "   â€¢ Access time: 0.043s (fast if in memory, slower if on disk)\n",
            "   â€¢ Storage: Memory first, spills to disk if needed (safer)\n",
            "\n",
            "3ï¸âƒ£  Testing DISK_ONLY (Disk only, slower but reliable)...\n",
            "   â€¢ Cache time: 0.227s\n",
            "   â€¢ Access time: 0.057s (slower - disk access)\n",
            "   â€¢ Storage: Disk only (slower, but always available)\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¡ Key Insight:\n",
            "   â€¢ MEMORY_ONLY: Fastest, but data lost if memory full\n",
            "   â€¢ MEMORY_AND_DISK: Fast if in memory, safe fallback to disk\n",
            "   â€¢ DISK_ONLY: Slower, but always available\n",
            "   â€¢ In PySpark, DataFrames are serialized internally regardless of storage level\n",
            "   â€¢ Choose based on your memory constraints and reliability needs\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "from pyspark import StorageLevel\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPARING DIFFERENT STORAGE LEVELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create a larger dataset\n",
        "large_data = [(i, f\"Product_{i}\", 100.0 + i, f\"Region_{i % 5}\") \n",
        "              for i in range(50000)]\n",
        "df_large = spark.createDataFrame(large_data, [\"id\", \"product\", \"price\", \"region\"])\n",
        "\n",
        "print(f\"\\nDataset: {df_large.count():,} rows\")\n",
        "\n",
        "# Note: In PySpark, DataFrames are already stored in a serialized format internally\n",
        "# The storage level determines WHERE data is stored (memory vs disk), not the format\n",
        "\n",
        "# Test MEMORY_ONLY (memory only, fast access)\n",
        "print(\"\\n1ï¸âƒ£  Testing MEMORY_ONLY (Memory only, fast access)...\")\n",
        "df_memory = df_large.persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "start = time.time()\n",
        "_ = df_memory.count()  # Trigger cache\n",
        "cache_time1 = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "_ = df_memory.agg({\"price\": \"sum\"}).collect()  # Use cache\n",
        "access_time1 = time.time() - start\n",
        "\n",
        "print(f\"   â€¢ Cache time: {cache_time1:.3f}s\")\n",
        "print(f\"   â€¢ Access time: {access_time1:.3f}s (fast - data in memory)\")\n",
        "print(f\"   â€¢ Storage: Memory only (fastest, but may be lost if memory full)\")\n",
        "\n",
        "df_memory.unpersist()\n",
        "\n",
        "# Test MEMORY_AND_DISK (memory with disk fallback)\n",
        "print(\"\\n2ï¸âƒ£  Testing MEMORY_AND_DISK (Memory + Disk fallback)...\")\n",
        "df_memory_disk = df_large.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "start = time.time()\n",
        "_ = df_memory_disk.count()  # Trigger cache\n",
        "cache_time2 = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "_ = df_memory_disk.agg({\"price\": \"sum\"}).collect()  # Use cache\n",
        "access_time2 = time.time() - start\n",
        "\n",
        "print(f\"   â€¢ Cache time: {cache_time2:.3f}s\")\n",
        "print(f\"   â€¢ Access time: {access_time2:.3f}s (fast if in memory, slower if on disk)\")\n",
        "print(f\"   â€¢ Storage: Memory first, spills to disk if needed (safer)\")\n",
        "\n",
        "df_memory_disk.unpersist()\n",
        "\n",
        "# Test DISK_ONLY (disk only, slower but reliable)\n",
        "print(\"\\n3ï¸âƒ£  Testing DISK_ONLY (Disk only, slower but reliable)...\")\n",
        "df_disk = df_large.persist(StorageLevel.DISK_ONLY)\n",
        "\n",
        "start = time.time()\n",
        "_ = df_disk.count()  # Trigger cache\n",
        "cache_time3 = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "_ = df_disk.agg({\"price\": \"sum\"}).collect()  # Use cache\n",
        "access_time3 = time.time() - start\n",
        "\n",
        "print(f\"   â€¢ Cache time: {cache_time3:.3f}s\")\n",
        "print(f\"   â€¢ Access time: {access_time3:.3f}s (slower - disk access)\")\n",
        "print(f\"   â€¢ Storage: Disk only (slower, but always available)\")\n",
        "\n",
        "df_disk.unpersist()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ’¡ Key Insight:\")\n",
        "print(\"   â€¢ MEMORY_ONLY: Fastest, but data lost if memory full\")\n",
        "print(\"   â€¢ MEMORY_AND_DISK: Fast if in memory, safe fallback to disk\")\n",
        "print(\"   â€¢ DISK_ONLY: Slower, but always available\")\n",
        "print(\"   â€¢ In PySpark, DataFrames are serialized internally regardless of storage level\")\n",
        "print(\"   â€¢ Choose based on your memory constraints and reliability needs\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serialization in User-Defined Functions (UDFs)\n",
        "\n",
        "### The Problem with UDFs\n",
        "\n",
        "**When you use a UDF, Spark must:**\n",
        "1. Serialize the function code\n",
        "2. Serialize any data the function needs\n",
        "3. Send to executors\n",
        "4. Deserialize on executors\n",
        "5. Execute the function\n",
        "\n",
        "**This is expensive!**\n",
        "\n",
        "### Example: UDF Serialization\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# This function must be serialized and sent to executors\n",
        "def multiply_by_two(x):\n",
        "    return x * 2\n",
        "\n",
        "my_udf = udf(multiply_by_two)\n",
        "result = df.select(my_udf(df.price).alias(\"doubled_price\"))\n",
        "```\n",
        "\n",
        "**What happens:**\n",
        "- Function code is serialized\n",
        "- Sent to all executors\n",
        "- Deserialized on each executor\n",
        "- Applied to data\n",
        "\n",
        "### Best Practice: Avoid UDFs When Possible\n",
        "\n",
        "**Instead of UDFs, use built-in Spark functions:**\n",
        "```python\n",
        "# âŒ BAD: UDF (requires serialization)\n",
        "from pyspark.sql.functions import udf\n",
        "multiply_udf = udf(lambda x: x * 2)\n",
        "df.select(multiply_udf(df.price))\n",
        "\n",
        "# âœ… GOOD: Built-in function (no serialization needed)\n",
        "from pyspark.sql.functions import col\n",
        "df.select((col(\"price\") * 2).alias(\"doubled_price\"))\n",
        "```\n",
        "\n",
        "**Why?**\n",
        "- Built-in functions are optimized\n",
        "- No serialization overhead\n",
        "- Faster execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Broadcasting and Serialization\n",
        "\n",
        "### What is Broadcasting?\n",
        "\n",
        "**Broadcasting sends small data to all executors:**\n",
        "- Serializes the data once\n",
        "- Sends to all executors\n",
        "- Each executor deserializes once\n",
        "- Data is reused for multiple operations\n",
        "\n",
        "### Example: Broadcast Join\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Small table (will be broadcast)\n",
        "small_df = spark.read.parquet(\"small_table.parquet\")  # 100 MB\n",
        "\n",
        "# Large table\n",
        "large_df = spark.read.parquet(\"large_table.parquet\")  # 10 GB\n",
        "\n",
        "# Broadcast join\n",
        "result = large_df.join(broadcast(small_df), on=\"key\")\n",
        "```\n",
        "\n",
        "**What happens:**\n",
        "1. Small table is serialized\n",
        "2. Sent to all executors (broadcast)\n",
        "3. Deserialized on each executor\n",
        "4. Stored in memory on each executor\n",
        "5. Used for join operations\n",
        "\n",
        "**Benefits:**\n",
        "- Small table serialized only once\n",
        "- Avoids shuffling large table\n",
        "- Much more efficient than regular join\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring Kryo Serialization\n",
        "\n",
        "### Why Use Kryo?\n",
        "\n",
        "**Kryo is faster and more efficient:**\n",
        "- 2-10Ã— faster serialization\n",
        "- 2-5Ã— smaller serialized data\n",
        "- Better performance for shuffles and caching\n",
        "\n",
        "### How to Enable Kryo\n",
        "\n",
        "```python\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "conf.set(\"spark.kryo.registrationRequired\", \"false\")  # Auto-register classes\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(conf=conf) \\\n",
        "    .appName(\"KryoDemo\") \\\n",
        "    .getOrCreate()\n",
        "```\n",
        "\n",
        "### Registering Custom Classes (Advanced)\n",
        "\n",
        "**If you have custom classes, register them:**\n",
        "\n",
        "```python\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "conf.set(\"spark.kryo.registrationRequired\", \"true\")\n",
        "conf.set(\"spark.kryo.classesToRegister\", \"com.example.MyClass\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(conf=conf) \\\n",
        "    .appName(\"KryoDemo\") \\\n",
        "    .getOrCreate()\n",
        "```\n",
        "\n",
        "### When to Use Kryo\n",
        "\n",
        "**Use Kryo when:**\n",
        "- âœ… You're in production\n",
        "- âœ… Performance is critical\n",
        "- âœ… You're doing lots of shuffling\n",
        "- âœ… You're caching large datasets\n",
        "- âœ… You can register custom classes (if needed)\n",
        "\n",
        "**Stick with Java Serialization when:**\n",
        "- âš ï¸ You're prototyping\n",
        "- âš ï¸ You have complex custom classes\n",
        "- âš ï¸ You don't want to manage registrations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Serialization Scenarios\n",
        "\n",
        "### Scenario 1: Shuffling Data (Join, GroupBy)\n",
        "\n",
        "**What happens:**\n",
        "```python\n",
        "df1.join(df2, on=\"key\")\n",
        "```\n",
        "\n",
        "1. Data on each executor is serialized\n",
        "2. Serialized data is shuffled over network\n",
        "3. Data is deserialized on receiving executors\n",
        "4. Join operation happens\n",
        "\n",
        "**Optimization:**\n",
        "- Use broadcast joins for small tables\n",
        "- Minimize shuffles when possible\n",
        "- Consider using Kryo serialization\n",
        "\n",
        "### Scenario 2: Caching Data\n",
        "\n",
        "**What happens:**\n",
        "```python\n",
        "df.cache()  # Uses MEMORY_AND_DISK by default\n",
        "```\n",
        "\n",
        "1. Data is serialized (based on storage level)\n",
        "2. Serialized data is stored in memory/disk\n",
        "3. When accessed, data is deserialized\n",
        "4. Subsequent accesses use cached (deserialized) data\n",
        "\n",
        "**Optimization:**\n",
        "- Use MEMORY_ONLY for fastest access (if memory allows)\n",
        "- Use MEMORY_AND_DISK for safety with disk fallback (default)\n",
        "- Use DISK_ONLY if memory is constrained\n",
        "- Choose based on your memory constraints and reliability needs\n",
        "\n",
        "### Scenario 3: User-Defined Functions\n",
        "\n",
        "**What happens:**\n",
        "```python\n",
        "from pyspark.sql.functions import udf\n",
        "my_udf = udf(lambda x: x * 2)\n",
        "df.select(my_udf(df.column))\n",
        "```\n",
        "\n",
        "1. Function code is serialized\n",
        "2. Serialized function sent to all executors\n",
        "3. Function deserialized on each executor\n",
        "4. Function applied to data\n",
        "\n",
        "**Optimization:**\n",
        "- Avoid UDFs when possible\n",
        "- Use built-in Spark functions\n",
        "- If UDFs are necessary, keep them simple\n",
        "\n",
        "### Scenario 4: Broadcasting Variables\n",
        "\n",
        "**What happens:**\n",
        "```python\n",
        "broadcast_var = spark.sparkContext.broadcast(large_list)\n",
        "```\n",
        "\n",
        "1. Variable is serialized once\n",
        "2. Sent to all executors\n",
        "3. Deserialized on each executor\n",
        "4. Stored in memory on each executor\n",
        "5. Reused for multiple operations\n",
        "\n",
        "**Optimization:**\n",
        "- Use broadcasting for small, frequently-used data\n",
        "- Avoid broadcasting very large data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### âœ… DO\n",
        "\n",
        "1. **Understand when serialization happens**\n",
        "   - Shuffles, caching, broadcasting, UDFs\n",
        "   - Be aware of the overhead\n",
        "\n",
        "2. **Use appropriate storage levels**\n",
        "   - MEMORY_ONLY for fastest access (if memory allows)\n",
        "   - MEMORY_AND_DISK for safety with disk fallback (default)\n",
        "   - DISK_ONLY if memory is constrained\n",
        "   - Choose based on your needs\n",
        "\n",
        "3. **Use Kryo in production**\n",
        "   - Faster and more efficient\n",
        "   - Better for performance-critical applications\n",
        "\n",
        "4. **Minimize serialization overhead**\n",
        "   - Avoid unnecessary shuffles\n",
        "   - Use broadcast joins for small tables\n",
        "   - Prefer built-in functions over UDFs\n",
        "\n",
        "5. **Monitor serialization costs**\n",
        "   - Check Spark UI for shuffle sizes\n",
        "   - Monitor cache hit rates\n",
        "   - Watch for serialization errors\n",
        "\n",
        "### âŒ DON'T\n",
        "\n",
        "1. **Don't ignore serialization overhead**\n",
        "   - It's a real cost in distributed systems\n",
        "   - Can significantly impact performance\n",
        "\n",
        "2. **Don't use UDFs unnecessarily**\n",
        "   - Built-in functions are faster\n",
        "   - Avoid serialization overhead\n",
        "\n",
        "3. **Don't cache everything**\n",
        "   - Serialization has costs\n",
        "   - Cache only when beneficial\n",
        "\n",
        "4. **Don't forget to register custom classes (Kryo)**\n",
        "   - If using Kryo with custom classes\n",
        "   - Must register them explicitly\n",
        "\n",
        "5. **Don't broadcast large data**\n",
        "   - Broadcasting has serialization overhead\n",
        "   - Only broadcast small, frequently-used data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Mistakes and How to Avoid Them\n",
        "\n",
        "### Mistake 1: Not Understanding Serialization Overhead\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# Thinking UDFs are free\n",
        "from pyspark.sql.functions import udf\n",
        "my_udf = udf(lambda x: x * 2)\n",
        "df.select(my_udf(df.price))  # âŒ Serialization overhead!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# Use built-in functions\n",
        "from pyspark.sql.functions import col\n",
        "df.select((col(\"price\") * 2).alias(\"doubled\"))  # âœ… No serialization!\n",
        "```\n",
        "\n",
        "### Mistake 2: Using Wrong Storage Level\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# Using MEMORY_ONLY when memory is limited\n",
        "huge_df.persist(StorageLevel.MEMORY_ONLY)  # âŒ May not fit!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# Use disk storage for large data\n",
        "huge_df.persist(StorageLevel.DISK_ONLY)  # âœ… No memory pressure\n",
        "# Or use MEMORY_AND_DISK for fallback\n",
        "huge_df.persist(StorageLevel.MEMORY_AND_DISK)  # âœ… Safe with disk fallback\n",
        "```\n",
        "\n",
        "### Mistake 3: Not Using Kryo in Production\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# Using default Java serialization in production\n",
        "spark = SparkSession.builder.appName(\"Prod\").getOrCreate()  # âŒ Slower\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# Enable Kryo for better performance\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "spark = SparkSession.builder.config(conf=conf).appName(\"Prod\").getOrCreate()  # âœ… Faster\n",
        "```\n",
        "\n",
        "### Mistake 4: Broadcasting Large Data\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# Broadcasting huge dataset\n",
        "huge_list = [1, 2, 3, ..., 1000000]  # 1 million items\n",
        "broadcast_var = spark.sparkContext.broadcast(huge_list)  # âŒ Expensive!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# Only broadcast small, frequently-used data\n",
        "small_lookup = {\"key1\": \"value1\", \"key2\": \"value2\"}  # Small dict\n",
        "broadcast_var = spark.sparkContext.broadcast(small_lookup)  # âœ… Efficient\n",
        "```\n",
        "\n",
        "### Mistake 5: Not Understanding When Serialization Happens\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# Thinking caching doesn't involve serialization\n",
        "df.cache()  # Actually does serialize based on storage level!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# Understand that caching involves storage decisions\n",
        "# MEMORY_ONLY: Stores in memory (fastest, but may be lost)\n",
        "# MEMORY_AND_DISK: Stores in memory, falls back to disk (safer)\n",
        "# DISK_ONLY: Stores on disk (slower, but always available)\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)  # Safe default with disk fallback\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### The Core Concept\n",
        "\n",
        "**Serialization:**\n",
        "- âœ… Converts objects to bytes (for storage/transmission)\n",
        "- âœ… Essential in distributed systems\n",
        "- âœ… Happens automatically in Spark\n",
        "- âš ï¸ Has performance overhead\n",
        "\n",
        "**Deserialization:**\n",
        "- âœ… Converts bytes back to objects\n",
        "- âœ… Happens when data is accessed\n",
        "- âœ… Required to use cached/shuffled data\n",
        "- âš ï¸ Has performance overhead\n",
        "\n",
        "### When Serialization Happens\n",
        "\n",
        "1. **Shuffling data** (joins, groupBy)\n",
        "2. **Caching data** (based on storage level)\n",
        "3. **Broadcasting variables**\n",
        "4. **User-defined functions (UDFs)**\n",
        "5. **Writing to disk**\n",
        "\n",
        "### Storage Level Choices (PySpark)\n",
        "\n",
        "- **MEMORY_ONLY:** Memory only (fastest, but may be lost if memory full)\n",
        "- **MEMORY_AND_DISK:** Memory with disk fallback (default, safest)\n",
        "- **DISK_ONLY:** Disk only (slower, but always available)\n",
        "\n",
        "**Note:** In PySpark, DataFrames are already serialized internally. Storage levels determine WHERE data is stored, not the format.\n",
        "\n",
        "### Serialization Formats\n",
        "\n",
        "- **Java Serialization:** Default, slower, larger\n",
        "- **Kryo Serialization:** Faster, smaller, requires setup\n",
        "\n",
        "### The Golden Rules\n",
        "\n",
        "1. **Serialization is unavoidable in distributed systems**\n",
        "2. **Choose appropriate storage levels based on memory constraints**\n",
        "3. **Use Kryo in production for better performance**\n",
        "4. **Minimize serialization overhead (avoid UDFs, minimize shuffles)**\n",
        "5. **Understand when serialization happens to optimize your code**\n",
        "\n",
        "### Remember\n",
        "\n",
        "1. **Serialization = Objects â†’ Bytes**\n",
        "2. **Deserialization = Bytes â†’ Objects**\n",
        "3. **Happens automatically, but you can optimize it**\n",
        "4. **Storage level determines serialization format**\n",
        "5. **Kryo is faster but requires setup**\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Practice using different storage levels\n",
        "- Monitor Spark UI to see serialization in action\n",
        "- Experiment with Kryo serialization\n",
        "- Review `8_d_Caching_Persist.ipynb` to see serialization in caching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **What serialization and deserialization are**\n",
        "   - Converting objects to bytes and back\n",
        "   - Essential for distributed systems\n",
        "   - Happens automatically in Spark\n",
        "\n",
        "2. **Why serialization matters in Spark**\n",
        "   - Data moves between machines\n",
        "   - Data is stored on disk\n",
        "   - Data is sent over network\n",
        "   - All require serialization\n",
        "\n",
        "3. **When serialization happens**\n",
        "   - Shuffling data (joins, groupBy)\n",
        "   - Caching data (based on storage level)\n",
        "   - Broadcasting variables\n",
        "   - User-defined functions\n",
        "   - Writing to disk\n",
        "\n",
        "4. **Storage levels and serialization**\n",
        "   - MEMORY_ONLY: Memory only (fastest, but may be lost)\n",
        "   - MEMORY_AND_DISK: Memory with disk fallback (default, safest)\n",
        "   - DISK_ONLY: Disk only (slower, but always available)\n",
        "   - In PySpark, DataFrames are already serialized internally\n",
        "   - Choose based on memory constraints and reliability needs\n",
        "\n",
        "5. **Serialization formats**\n",
        "   - Java Serialization: Default, slower\n",
        "   - Kryo Serialization: Faster, requires setup\n",
        "\n",
        "6. **Best practices**\n",
        "   - Use appropriate storage levels\n",
        "   - Use Kryo in production\n",
        "   - Minimize serialization overhead\n",
        "   - Avoid UDFs when possible\n",
        "\n",
        "### The Bottom Line\n",
        "\n",
        "> **Serialization is a fundamental concept in distributed systems. Understanding when and how it happens helps you write more efficient Spark applications. Choose the right storage levels, use Kryo in production, and minimize serialization overhead to optimize performance.**\n",
        "\n",
        "---\n",
        "\n",
        "**Related Notebooks:**\n",
        "- `08_a_Spark_Architecture.ipynb` - Understanding executors, cores, and tasks\n",
        "- `8_d_Caching_Persist.ipynb` - Understanding caching and storage levels\n",
        "- `08_performance_optimization.ipynb` - Comprehensive performance optimization guide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "# Clean up\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
