{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e4bfe7",
   "metadata": {},
   "source": [
    "# Data Quality and Governance in Production-Grade Pipelines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Data Quality and Governance are critical pillars of production-grade data pipelines.**\n",
    "\n",
    "In production environments, data pipelines must be:\n",
    "- ‚úÖ **Reliable** - Run consistently without failures\n",
    "- ‚úÖ **Accurate** - Produce correct and trustworthy data\n",
    "- ‚úÖ **Compliant** - Meet regulatory and business requirements\n",
    "- ‚úÖ **Observable** - Provide visibility into data health\n",
    "- ‚úÖ **Maintainable** - Easy to debug and update\n",
    "\n",
    "**The Problem:** Without proper data quality and governance:\n",
    "- üî¥ **Bad data** propagates through systems, causing incorrect business decisions\n",
    "- üî¥ **Compliance violations** result in legal issues and fines\n",
    "- üî¥ **Pipeline failures** go undetected, breaking downstream systems\n",
    "- üî¥ **Data lineage** is lost, making debugging impossible\n",
    "- üî¥ **Security breaches** expose sensitive information\n",
    "\n",
    "**What you'll learn:**\n",
    "- Understanding Data Quality dimensions and metrics\n",
    "- Data Governance principles and frameworks\n",
    "- How to ensure data quality in production pipelines\n",
    "- Data validation and testing strategies\n",
    "- Monitoring and alerting for data quality\n",
    "- Popular tools and platforms for data quality and governance\n",
    "- Best practices for implementing DQ and governance\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## What is Data Quality?\n",
    "\n",
    "**Data Quality** refers to the fitness of data for its intended use. High-quality data is:\n",
    "- **Accurate** - Correctly represents real-world entities\n",
    "- **Complete** - Contains all required information\n",
    "- **Consistent** - Uniform across different systems\n",
    "- **Timely** - Available when needed\n",
    "- **Valid** - Conforms to defined business rules\n",
    "- **Unique** - No duplicate records\n",
    "- **Reliable** - Can be trusted for decision-making\n",
    "\n",
    "### The Six Dimensions of Data Quality\n",
    "\n",
    "1. **Completeness** - Are all required fields populated?\n",
    "2. **Accuracy** - Does the data correctly represent reality?\n",
    "3. **Consistency** - Is data uniform across systems?\n",
    "4. **Validity** - Does data conform to business rules?\n",
    "5. **Timeliness** - Is data available when needed?\n",
    "6. **Uniqueness** - Are there duplicate records?\n",
    "\n",
    "---\n",
    "\n",
    "## What is Data Governance?\n",
    "\n",
    "**Data Governance** is the overall management of data availability, usability, integrity, and security in an organization. It includes:\n",
    "\n",
    "- **Data Policies** - Rules and standards for data management\n",
    "- **Data Standards** - Naming conventions, formats, schemas\n",
    "- **Data Ownership** - Who is responsible for data assets\n",
    "- **Data Security** - Access controls and encryption\n",
    "- **Data Privacy** - Compliance with regulations (GDPR, CCPA, etc.)\n",
    "- **Data Lineage** - Tracking data flow from source to destination\n",
    "- **Data Catalog** - Inventory of all data assets\n",
    "- **Data Stewardship** - Ongoing management and maintenance\n",
    "\n",
    "### Key Components of Data Governance\n",
    "\n",
    "1. **Data Catalog** - Centralized inventory of data assets\n",
    "2. **Data Lineage** - Tracking data transformations and flow\n",
    "3. **Access Control** - Who can access what data\n",
    "4. **Data Classification** - Categorizing data by sensitivity\n",
    "5. **Compliance Management** - Meeting regulatory requirements\n",
    "6. **Data Quality Monitoring** - Continuous assessment of data health\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose of Data Quality and Governance in Production Pipelines\n",
    "\n",
    "### 1. **Trust and Reliability**\n",
    "- Ensures stakeholders can trust the data for decision-making\n",
    "- Prevents \"garbage in, garbage out\" scenarios\n",
    "- Builds confidence in data-driven initiatives\n",
    "\n",
    "### 2. **Risk Mitigation**\n",
    "- Prevents costly errors from bad data\n",
    "- Reduces compliance violations and legal risks\n",
    "- Protects against security breaches\n",
    "\n",
    "### 3. **Operational Efficiency**\n",
    "- Reduces time spent debugging data issues\n",
    "- Prevents pipeline failures and downtime\n",
    "- Enables faster problem resolution\n",
    "\n",
    "### 4. **Regulatory Compliance**\n",
    "- Meets requirements for GDPR, HIPAA, SOX, etc.\n",
    "- Provides audit trails and documentation\n",
    "- Ensures data privacy and security standards\n",
    "\n",
    "### 5. **Cost Reduction**\n",
    "- Prevents rework from data errors\n",
    "- Reduces storage costs from duplicate data\n",
    "- Minimizes support and maintenance overhead\n",
    "\n",
    "### 6. **Business Value**\n",
    "- Enables accurate analytics and reporting\n",
    "- Supports better business decisions\n",
    "- Facilitates data monetization opportunities\n",
    "\n",
    "---\n",
    "\n",
    "## How Data Quality is Ensured in Production Pipelines\n",
    "\n",
    "### 1. **Data Validation at Ingestion**\n",
    "\n",
    "**Schema Validation:**\n",
    "- Verify data structure matches expected schema\n",
    "- Check data types and formats\n",
    "- Validate required fields are present\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Validate schema before loading\n",
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(customer_id) as records_with_customer_id,\n",
    "    COUNT(email) as records_with_email,\n",
    "    COUNT(CASE WHEN email LIKE '%@%.%' THEN 1 END) as valid_emails\n",
    "FROM staging_customers\n",
    "WHERE load_date = CURRENT_DATE();\n",
    "```\n",
    "\n",
    "### 2. **Data Profiling**\n",
    "\n",
    "**Statistical Analysis:**\n",
    "- Identify data distributions\n",
    "- Detect outliers and anomalies\n",
    "- Understand data patterns\n",
    "- Measure completeness and uniqueness\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Data profiling query\n",
    "SELECT \n",
    "    'customers' as table_name,\n",
    "    COUNT(*) as total_rows,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(*) - COUNT(email) as missing_emails,\n",
    "    COUNT(*) - COUNT(phone) as missing_phones,\n",
    "    MIN(created_date) as earliest_record,\n",
    "    MAX(created_date) as latest_record,\n",
    "    COUNT(CASE WHEN age < 0 OR age > 120 THEN 1 END) as invalid_ages\n",
    "FROM customers;\n",
    "```\n",
    "\n",
    "### 3. **Data Quality Checks**\n",
    "\n",
    "**Automated Testing:**\n",
    "- Row count checks (expected vs actual)\n",
    "- Null checks (required fields)\n",
    "- Range checks (values within acceptable bounds)\n",
    "- Referential integrity checks (foreign keys)\n",
    "- Uniqueness checks (no duplicates)\n",
    "- Business rule validation\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Comprehensive data quality check\n",
    "WITH quality_checks AS (\n",
    "    SELECT \n",
    "        -- Completeness checks\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(customer_id) as non_null_customer_ids,\n",
    "        COUNT(email) as non_null_emails,\n",
    "        \n",
    "        -- Validity checks\n",
    "        COUNT(CASE WHEN email NOT LIKE '%@%.%' THEN 1 END) as invalid_emails,\n",
    "        COUNT(CASE WHEN age < 0 OR age > 120 THEN 1 END) as invalid_ages,\n",
    "        \n",
    "        -- Uniqueness checks\n",
    "        COUNT(*) - COUNT(DISTINCT customer_id) as duplicate_customer_ids,\n",
    "        \n",
    "        -- Consistency checks\n",
    "        COUNT(CASE WHEN created_date > updated_date THEN 1 END) as inconsistent_dates\n",
    "        \n",
    "    FROM customers\n",
    "    WHERE load_date = CURRENT_DATE()\n",
    ")\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN non_null_customer_ids < total_rows * 0.95 THEN 'FAIL'\n",
    "        WHEN invalid_emails > total_rows * 0.01 THEN 'FAIL'\n",
    "        WHEN invalid_ages > 0 THEN 'FAIL'\n",
    "        WHEN duplicate_customer_ids > 0 THEN 'FAIL'\n",
    "        ELSE 'PASS'\n",
    "    END as quality_status\n",
    "FROM quality_checks;\n",
    "```\n",
    "\n",
    "### 4. **Data Quality Monitoring**\n",
    "\n",
    "**Continuous Monitoring:**\n",
    "- Track data quality metrics over time\n",
    "- Set up alerts for quality degradation\n",
    "- Dashboard for real-time visibility\n",
    "- Trend analysis to predict issues\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Daily data quality monitoring\n",
    "CREATE OR REPLACE TABLE data_quality_metrics AS\n",
    "SELECT \n",
    "    CURRENT_DATE() as check_date,\n",
    "    'customers' as table_name,\n",
    "    COUNT(*) as row_count,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(*) - COUNT(email) as missing_emails,\n",
    "    COUNT(CASE WHEN email NOT LIKE '%@%.%' THEN 1 END) as invalid_emails,\n",
    "    ROUND((COUNT(email) / COUNT(*)) * 100, 2) as email_completeness_pct,\n",
    "    ROUND((COUNT(CASE WHEN email LIKE '%@%.%' THEN 1 END) / COUNT(*)) * 100, 2) as email_validity_pct\n",
    "FROM customers\n",
    "WHERE load_date = CURRENT_DATE();\n",
    "```\n",
    "\n",
    "### 5. **Data Lineage Tracking**\n",
    "\n",
    "**Track Data Flow:**\n",
    "- Document source systems\n",
    "- Track transformations\n",
    "- Map dependencies\n",
    "- Enable impact analysis\n",
    "\n",
    "### 6. **Automated Testing Framework**\n",
    "\n",
    "**Test-Driven Development:**\n",
    "- Unit tests for transformations\n",
    "- Integration tests for pipelines\n",
    "- Regression tests for schema changes\n",
    "- Performance tests for query optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Data Governance Implementation\n",
    "\n",
    "### 1. **Data Catalog**\n",
    "\n",
    "**Centralized Metadata:**\n",
    "- Document all data assets\n",
    "- Define data dictionaries\n",
    "- Tag data by domain and purpose\n",
    "- Maintain ownership information\n",
    "\n",
    "### 2. **Access Control**\n",
    "\n",
    "**Role-Based Access:**\n",
    "- Define user roles and permissions\n",
    "- Implement row-level security\n",
    "- Audit access logs\n",
    "- Enforce least privilege principle\n",
    "\n",
    "**Example (Snowflake):**\n",
    "```sql\n",
    "-- Create role for data analysts\n",
    "CREATE ROLE data_analyst;\n",
    "\n",
    "-- Grant read access to specific tables\n",
    "GRANT SELECT ON TABLE customers TO ROLE data_analyst;\n",
    "GRANT SELECT ON TABLE orders TO ROLE data_analyst;\n",
    "\n",
    "-- Deny access to sensitive tables\n",
    "REVOKE SELECT ON TABLE customer_pii FROM ROLE data_analyst;\n",
    "\n",
    "-- Assign role to user\n",
    "GRANT ROLE data_analyst TO USER analyst1;\n",
    "```\n",
    "\n",
    "### 3. **Data Classification**\n",
    "\n",
    "**Categorize by Sensitivity:**\n",
    "- Public\n",
    "- Internal\n",
    "- Confidential\n",
    "- Restricted\n",
    "\n",
    "### 4. **Data Retention Policies**\n",
    "\n",
    "**Lifecycle Management:**\n",
    "- Define retention periods\n",
    "- Implement archival strategies\n",
    "- Automate data purging\n",
    "- Comply with legal requirements\n",
    "\n",
    "### 5. **Audit and Compliance**\n",
    "\n",
    "**Tracking and Reporting:**\n",
    "- Log all data access\n",
    "- Track data changes\n",
    "- Generate compliance reports\n",
    "- Maintain audit trails\n",
    "\n",
    "---\n",
    "\n",
    "## Popular Tools for Data Quality and Governance\n",
    "\n",
    "### 1. **Open Source Tools**\n",
    "\n",
    "#### **Great Expectations**\n",
    "- **Purpose:** Data validation and testing framework\n",
    "- **Features:**\n",
    "  - Declarative data quality checks\n",
    "  - Integration with data pipelines\n",
    "  - Automated documentation\n",
    "  - Data profiling\n",
    "- **Use Cases:** Data validation, testing, monitoring\n",
    "\n",
    "#### **Apache Airflow**\n",
    "- **Purpose:** Workflow orchestration with DQ capabilities\n",
    "- **Features:**\n",
    "  - Pipeline orchestration\n",
    "  - Data quality operators\n",
    "  - Monitoring and alerting\n",
    "  - Task dependencies\n",
    "- **Use Cases:** ETL orchestration, data quality checks\n",
    "\n",
    "#### **dbt (data build tool)**\n",
    "- **Purpose:** Data transformation with built-in testing\n",
    "- **Features:**\n",
    "  - SQL-based transformations\n",
    "  - Built-in data tests\n",
    "  - Documentation generation\n",
    "  - Version control\n",
    "- **Use Cases:** Data transformation, testing, documentation\n",
    "\n",
    "#### **DataHub**\n",
    "- **Purpose:** Metadata platform and data catalog\n",
    "- **Features:**\n",
    "  - Data discovery\n",
    "  - Data lineage\n",
    "  - Metadata management\n",
    "  - Access control\n",
    "- **Use Cases:** Data cataloging, lineage tracking\n",
    "\n",
    "#### **Amundsen**\n",
    "- **Purpose:** Data discovery and metadata engine\n",
    "- **Features:**\n",
    "  - Data search and discovery\n",
    "  - Metadata management\n",
    "  - Usage statistics\n",
    "  - Data lineage\n",
    "- **Use Cases:** Data discovery, cataloging\n",
    "\n",
    "### 2. **Cloud-Native Solutions**\n",
    "\n",
    "#### **Snowflake Data Quality**\n",
    "- **Purpose:** Native data quality features\n",
    "- **Features:**\n",
    "  - Data profiling\n",
    "  - Quality metrics\n",
    "  - Monitoring dashboards\n",
    "  - Integration with Snowflake ecosystem\n",
    "- **Use Cases:** Data quality in Snowflake environments\n",
    "\n",
    "#### **AWS Glue Data Quality**\n",
    "- **Purpose:** Data quality for AWS data lakes\n",
    "- **Features:**\n",
    "  - Automated data profiling\n",
    "  - Quality rules engine\n",
    "  - Monitoring and alerting\n",
    "  - Integration with AWS services\n",
    "- **Use Cases:** AWS-based data pipelines\n",
    "\n",
    "#### **Azure Purview**\n",
    "- **Purpose:** Unified data governance platform\n",
    "- **Features:**\n",
    "  - Data catalog\n",
    "  - Data lineage\n",
    "  - Data classification\n",
    "  - Access control\n",
    "  - Compliance management\n",
    "- **Use Cases:** Enterprise data governance on Azure\n",
    "\n",
    "#### **Google Cloud Data Catalog**\n",
    "- **Purpose:** Metadata management service\n",
    "- **Features:**\n",
    "  - Data discovery\n",
    "  - Metadata management\n",
    "  - Tag templates\n",
    "  - Integration with GCP services\n",
    "- **Use Cases:** Data cataloging on GCP\n",
    "\n",
    "### 3. **Commercial Platforms**\n",
    "\n",
    "#### **Collibra**\n",
    "- **Purpose:** Enterprise data governance platform\n",
    "- **Features:**\n",
    "  - Data catalog\n",
    "  - Data lineage\n",
    "  - Policy management\n",
    "  - Workflow automation\n",
    "  - Compliance management\n",
    "- **Use Cases:** Enterprise governance, compliance\n",
    "\n",
    "#### **Informatica Data Quality**\n",
    "- **Purpose:** Enterprise data quality solution\n",
    "- **Features:**\n",
    "  - Data profiling\n",
    "  - Data cleansing\n",
    "  - Data matching\n",
    "  - Monitoring and reporting\n",
    "- **Use Cases:** Enterprise data quality management\n",
    "\n",
    "#### **Talend Data Quality**\n",
    "- **Purpose:** Data integration with quality features\n",
    "- **Features:**\n",
    "  - Data profiling\n",
    "  - Data cleansing\n",
    "  - Real-time monitoring\n",
    "  - Integration with Talend platform\n",
    "- **Use Cases:** Data integration with quality\n",
    "\n",
    "#### **Monte Carlo**\n",
    "- **Purpose:** Data observability platform\n",
    "- **Features:**\n",
    "  - Automated anomaly detection\n",
    "  - Data quality monitoring\n",
    "  - Incident management\n",
    "  - Root cause analysis\n",
    "- **Use Cases:** Data observability, monitoring\n",
    "\n",
    "#### **Datafold**\n",
    "- **Purpose:** Data diff and quality testing\n",
    "- **Features:**\n",
    "  - Data diffing\n",
    "  - Regression testing\n",
    "  - Data quality checks\n",
    "  - CI/CD integration\n",
    "- **Use Cases:** Data testing, regression detection\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices for Data Quality and Governance\n",
    "\n",
    "### 1. **Start Early**\n",
    "- Implement DQ checks from the beginning\n",
    "- Don't retrofit quality after problems occur\n",
    "- Build quality into the pipeline design\n",
    "\n",
    "### 2. **Automate Everything**\n",
    "- Automated validation checks\n",
    "- Automated testing\n",
    "- Automated monitoring and alerting\n",
    "- Automated documentation\n",
    "\n",
    "### 3. **Define Clear Metrics**\n",
    "- Establish SLAs for data quality\n",
    "- Set thresholds for alerts\n",
    "- Track metrics over time\n",
    "- Report to stakeholders\n",
    "\n",
    "### 4. **Fail Fast**\n",
    "- Validate data early in the pipeline\n",
    "- Stop processing on critical failures\n",
    "- Alert immediately on quality issues\n",
    "- Prevent bad data from propagating\n",
    "\n",
    "### 5. **Document Everything**\n",
    "- Document data sources and schemas\n",
    "- Maintain data dictionaries\n",
    "- Document business rules\n",
    "- Keep audit trails\n",
    "\n",
    "### 6. **Monitor Continuously**\n",
    "- Real-time quality monitoring\n",
    "- Trend analysis\n",
    "- Predictive alerts\n",
    "- Regular quality reports\n",
    "\n",
    "### 7. **Establish Ownership**\n",
    "- Assign data stewards\n",
    "- Define clear responsibilities\n",
    "- Create escalation paths\n",
    "- Foster data culture\n",
    "\n",
    "### 8. **Iterate and Improve**\n",
    "- Review quality metrics regularly\n",
    "- Update rules based on learnings\n",
    "- Refine thresholds\n",
    "- Continuously improve processes\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Example: E-Commerce Data Pipeline\n",
    "\n",
    "**Scenario:** Daily customer and order data pipeline\n",
    "\n",
    "**Data Quality Checks:**\n",
    "1. **Ingestion Validation:**\n",
    "   - Row count matches source system\n",
    "   - Schema matches expected structure\n",
    "   - Required fields are present\n",
    "\n",
    "2. **Business Rule Validation:**\n",
    "   - Customer IDs are unique\n",
    "   - Email addresses are valid format\n",
    "   - Order amounts are positive\n",
    "   - Order dates are not in the future\n",
    "\n",
    "3. **Referential Integrity:**\n",
    "   - All orders have valid customer IDs\n",
    "   - All products exist in product catalog\n",
    "   - All categories are valid\n",
    "\n",
    "4. **Completeness Checks:**\n",
    "   - Customer email completeness > 95%\n",
    "   - Order shipping address completeness = 100%\n",
    "   - Product description completeness > 80%\n",
    "\n",
    "5. **Monitoring:**\n",
    "   - Daily quality dashboard\n",
    "   - Alerts on quality degradation\n",
    "   - Weekly quality reports\n",
    "   - Monthly trend analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Data Quality** ensures data is fit for purpose through validation, profiling, and monitoring\n",
    "2. **Data Governance** manages data assets through policies, standards, and controls\n",
    "3. **Both are essential** for production-grade pipelines to be reliable, compliant, and trustworthy\n",
    "4. **Multiple tools** are available, from open-source to enterprise platforms\n",
    "5. **Best practices** include automation, monitoring, documentation, and continuous improvement\n",
    "\n",
    "**Remember:** \n",
    "- üî¥ **Bad data** leads to bad decisions\n",
    "- ‚úÖ **Good governance** prevents problems before they occur\n",
    "- üìä **Continuous monitoring** catches issues early\n",
    "- üõ†Ô∏è **Right tools** make implementation manageable\n",
    "- üë• **People and processes** are as important as technology\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Implement basic data quality checks** in your pipelines\n",
    "2. **Set up monitoring** for key quality metrics\n",
    "3. **Document your data** assets and lineage\n",
    "4. **Establish governance** policies and procedures\n",
    "5. **Choose appropriate tools** for your use case\n",
    "6. **Build a data quality culture** in your organization\n",
    "\n",
    "**Resources:**\n",
    "- Great Expectations documentation\n",
    "- dbt testing best practices\n",
    "- Data governance frameworks (DAMA, DCAM)\n",
    "- Industry-specific compliance requirements (GDPR, HIPAA, SOX)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
