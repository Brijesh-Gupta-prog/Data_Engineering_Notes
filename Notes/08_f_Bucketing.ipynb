{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Bucketing in PySpark\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **What bucketing is** and how it differs from partitioning\n",
        "2. **Why bucketing is important** for optimizing joins and queries\n",
        "3. **How to create bucketed tables** in Spark\n",
        "4. **When to use bucketing** vs when to use partitioning\n",
        "5. **Best practices** for bucketing in production\n",
        "6. **Common mistakes** and how to avoid them\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Understanding of Spark architecture (executors, cores, tasks) - see `08_a_Spark_Architecture.ipynb`\n",
        "- Understanding of partitions - see `08_b_Partitions_Concepts.ipynb`\n",
        "- Understanding of joins - see `06_joins.ipynb`\n",
        "- Basic familiarity with Spark DataFrame operations\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:** This notebook builds on the concepts from `08_b_Partitions_Concepts.ipynb` and `06_joins.ipynb`. Make sure you understand partitions and joins before proceeding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction: What is Bucketing?\n",
        "\n",
        "### The Real-World Analogy\n",
        "\n",
        "**Think of bucketing like organizing books in a library:**\n",
        "\n",
        "**Partitioning (by topic):**\n",
        "- Books are organized by topic (Science, History, Fiction)\n",
        "- Each topic has its own section\n",
        "- You know which section to go to\n",
        "\n",
        "**Bucketing (by author within topic):**\n",
        "- Within each topic, books are organized by author\n",
        "- Authors are distributed into buckets (A-F, G-M, N-Z)\n",
        "- When you join two tables, matching buckets are processed together\n",
        "\n",
        "### Technical Definition\n",
        "\n",
        "**Bucketing:**\n",
        "> A technique that divides data into a fixed number of buckets based on the hash value of one or more columns. Data with the same hash value goes into the same bucket.\n",
        "\n",
        "**Key Characteristics:**\n",
        "- Fixed number of buckets (e.g., 32, 64, 128)\n",
        "- Based on hash function (deterministic)\n",
        "- Same values always go to the same bucket\n",
        "- Optimizes joins and aggregations\n",
        "\n",
        "### Why Does This Matter?\n",
        "\n",
        "**Without bucketing:**\n",
        "- Joins require shuffling all data\n",
        "- Expensive network transfers\n",
        "- Slow query performance\n",
        "\n",
        "**With bucketing:**\n",
        "- Joins only need to match corresponding buckets\n",
        "- Minimal shuffling required\n",
        "- Much faster query performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bucketing vs Partitioning: Understanding the Difference\n",
        "\n",
        "### Partitioning\n",
        "\n",
        "**Partitioning divides data by column values:**\n",
        "```python\n",
        "# Partitioned by date\n",
        "df.write.partitionBy(\"date\").parquet(\"path/\")\n",
        "# Creates: path/date=2024-01-01/, date=2024-01-02/, etc.\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- Creates separate directories/folders\n",
        "- Based on actual column values\n",
        "- Number of partitions = number of distinct values\n",
        "- Used for filtering and pruning\n",
        "\n",
        "### Bucketing\n",
        "\n",
        "**Bucketing divides data by hash values:**\n",
        "```python\n",
        "# Bucketed by customer_id into 32 buckets\n",
        "df.write.bucketBy(32, \"customer_id\").saveAsTable(\"bucketed_table\")\n",
        "# Creates: 32 files, data distributed by hash\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- Fixed number of buckets (you specify)\n",
        "- Based on hash function (not actual values)\n",
        "- Number of buckets = fixed (e.g., 32)\n",
        "- Used for optimizing joins\n",
        "\n",
        "### Visual Comparison\n",
        "\n",
        "**Partitioning (by region):**\n",
        "```\n",
        "data/\n",
        "  ‚îú‚îÄ‚îÄ region=North/\n",
        "  ‚îú‚îÄ‚îÄ region=South/\n",
        "  ‚îú‚îÄ‚îÄ region=East/\n",
        "  ‚îî‚îÄ‚îÄ region=West/\n",
        "```\n",
        "- 4 partitions (one per region)\n",
        "- Each partition is a separate directory\n",
        "\n",
        "**Bucketing (by customer_id, 4 buckets):**\n",
        "```\n",
        "data/\n",
        "  ‚îú‚îÄ‚îÄ part-00000.parquet  (bucket 0: hash % 4 == 0)\n",
        "  ‚îú‚îÄ‚îÄ part-00001.parquet  (bucket 1: hash % 4 == 1)\n",
        "  ‚îú‚îÄ‚îÄ part-00002.parquet  (bucket 2: hash % 4 == 2)\n",
        "  ‚îî‚îÄ‚îÄ part-00003.parquet  (bucket 3: hash % 4 == 3)\n",
        "```\n",
        "- 4 buckets (fixed number)\n",
        "- All buckets in same directory\n",
        "- Data distributed by hash\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Aspect | Partitioning | Bucketing |\n",
        "|--------|-------------|-----------|\n",
        "| **Purpose** | Filtering, pruning | Join optimization |\n",
        "| **Number** | Variable (based on values) | Fixed (you specify) |\n",
        "| **Based on** | Actual column values | Hash of column values |\n",
        "| **Storage** | Separate directories | Same directory, different files |\n",
        "| **Use case** | Time-series, categorical data | Join keys, frequently joined columns |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Bucketing Matters: The Join Problem\n",
        "\n",
        "### The Problem: Expensive Joins\n",
        "\n",
        "**Scenario: Joining two large tables**\n",
        "\n",
        "```python\n",
        "# Large table 1: Sales data (100 GB)\n",
        "sales_df = spark.read.parquet(\"sales.parquet\")\n",
        "\n",
        "# Large table 2: Customer data (10 GB)\n",
        "customers_df = spark.read.parquet(\"customers.parquet\")\n",
        "\n",
        "# Join on customer_id\n",
        "result = sales_df.join(customers_df, on=\"customer_id\")\n",
        "```\n",
        "\n",
        "**What happens without bucketing:**\n",
        "1. All data from both tables is shuffled\n",
        "2. Data is redistributed across the cluster\n",
        "3. Network I/O is massive (100+ GB)\n",
        "4. Join is slow and expensive\n",
        "\n",
        "### The Solution: Bucketing\n",
        "\n",
        "**With bucketing:**\n",
        "```python\n",
        "# Both tables bucketed by customer_id (same number of buckets)\n",
        "sales_df.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales_bucketed\")\n",
        "customers_df.write.bucketBy(32, \"customer_id\").saveAsTable(\"customers_bucketed\")\n",
        "\n",
        "# Join only matches corresponding buckets\n",
        "sales_bucketed = spark.table(\"sales_bucketed\")\n",
        "customers_bucketed = spark.table(\"customers_bucketed\")\n",
        "result = sales_bucketed.join(customers_bucketed, on=\"customer_id\")\n",
        "```\n",
        "\n",
        "**What happens with bucketing:**\n",
        "1. Only matching buckets are joined (bucket 0 with bucket 0, etc.)\n",
        "2. No shuffling needed (data already co-located)\n",
        "3. Minimal network I/O\n",
        "4. Join is fast and efficient\n",
        "\n",
        "### Visual Representation\n",
        "\n",
        "**Without Bucketing:**\n",
        "```\n",
        "Sales Table          Customers Table\n",
        "[All data]    ‚Üí     [All data]\n",
        "     ‚Üì                   ‚Üì\n",
        "   Shuffle ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Shuffle\n",
        "     ‚Üì                   ‚Üì\n",
        "   Join (expensive!)\n",
        "```\n",
        "\n",
        "**With Bucketing:**\n",
        "```\n",
        "Sales Table          Customers Table\n",
        "Bucket 0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Bucket 0  (join locally)\n",
        "Bucket 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Bucket 1  (join locally)\n",
        "Bucket 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Bucket 2  (join locally)\n",
        "...\n",
        "No shuffle needed!\n",
        "```\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "> **Bucketing pre-organizes data so that rows with the same join key values are in the same bucket. When you join two bucketed tables on the same key, Spark only needs to join corresponding buckets, avoiding expensive shuffles.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How Bucketing Works\n",
        "\n",
        "### The Hash Function\n",
        "\n",
        "**Bucketing uses a hash function:**\n",
        "```python\n",
        "bucket_number = hash(column_value) % number_of_buckets\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# 4 buckets, customer_id = 123\n",
        "bucket = hash(123) % 4\n",
        "# Result: Always bucket 2 (for customer_id = 123)\n",
        "```\n",
        "\n",
        "**Key Properties:**\n",
        "- **Deterministic:** Same value always goes to same bucket\n",
        "- **Distributed:** Values are evenly distributed across buckets\n",
        "- **Fast:** Hash computation is very fast\n",
        "\n",
        "### Step-by-Step: Creating Bucketed Data\n",
        "\n",
        "**Step 1: Write data with bucketing**\n",
        "```python\n",
        "df.write.bucketBy(32, \"customer_id\").saveAsTable(\"bucketed_table\")\n",
        "```\n",
        "\n",
        "**Step 2: What Spark does:**\n",
        "1. Reads each row\n",
        "2. Computes hash of `customer_id`\n",
        "3. Determines bucket: `hash(customer_id) % 32`\n",
        "4. Writes row to corresponding bucket file\n",
        "\n",
        "**Step 3: Result:**\n",
        "- 32 files (one per bucket)\n",
        "- Each file contains rows with same hash values\n",
        "- Data is pre-organized for joins\n",
        "\n",
        "### Visual Example\n",
        "\n",
        "**Input Data:**\n",
        "```\n",
        "customer_id | name    | amount\n",
        "------------|---------|--------\n",
        "100         | Alice   | 1000\n",
        "101         | Bob     | 2000\n",
        "102         | Charlie | 1500\n",
        "103         | David   | 3000\n",
        "```\n",
        "\n",
        "**After Bucketing (4 buckets):**\n",
        "```\n",
        "Bucket 0 (hash % 4 == 0): customer_id 100, 104, 108, ...\n",
        "Bucket 1 (hash % 4 == 1): customer_id 101, 105, 109, ...\n",
        "Bucket 2 (hash % 4 == 2): customer_id 102, 106, 110, ...\n",
        "Bucket 3 (hash % 4 == 3): customer_id 103, 107, 111, ...\n",
        "```\n",
        "\n",
        "**When joining:**\n",
        "- Bucket 0 from table 1 joins with Bucket 0 from table 2\n",
        "- Bucket 1 from table 1 joins with Bucket 1 from table 2\n",
        "- No cross-bucket joins needed!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Example: Demonstrating Bucketing\n",
        "\n",
        "Let's see bucketing in action with a practical example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/03 06:38:20 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
            "26/01/03 06:38:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "26/01/03 06:38:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/01/03 06:38:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SPARK SESSION INITIALIZED\n",
            "======================================================================\n",
            "Spark Version: 3.5.1\n",
            "Default Parallelism: 11\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BucketingDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SPARK SESSION INITIALIZED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Create Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CREATING SAMPLE DATA\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 0:>                                                        (0 + 11) / 11]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sales DataFrame: 10,000 rows\n",
            "Customers DataFrame: 1,000 rows\n",
            "\n",
            "Both tables have 'customer_id' column for joining\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create sample data for demonstration\n",
        "print(\"=\" * 70)\n",
        "print(\"CREATING SAMPLE DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Sales data\n",
        "sales_data = [(i, f\"Product_{i % 100}\", 100.0 + i, i % 1000) \n",
        "              for i in range(10000)]\n",
        "sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"product\", \"amount\", \"customer_id\"])\n",
        "\n",
        "# Customer data\n",
        "customer_data = [(i, f\"Customer_{i}\", f\"Region_{i % 5}\") \n",
        "                 for i in range(1000)]\n",
        "customers_df = spark.createDataFrame(customer_data, [\"customer_id\", \"customer_name\", \"region\"])\n",
        "\n",
        "print(f\"\\nSales DataFrame: {sales_df.count():,} rows\")\n",
        "print(f\"Customers DataFrame: {customers_df.count():,} rows\")\n",
        "print(f\"\\nBoth tables have 'customer_id' column for joining\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Join Without Bucketing (The Problem)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "JOIN WITHOUT BUCKETING\n",
            "======================================================================\n",
            "\n",
            "Performing join on non-bucketed tables...\n",
            "‚ö†Ô∏è  This will trigger a shuffle operation!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Join completed!\n",
            "   ‚Ä¢ Result: 10,000 rows\n",
            "   ‚Ä¢ Time: 1.037 seconds\n",
            "   ‚Ä¢ What happened:\n",
            "     - All data from both tables was shuffled\n",
            "     - Data was redistributed across the cluster\n",
            "     - Expensive network I/O occurred\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Join without bucketing - requires shuffle\n",
        "print(\"=\" * 70)\n",
        "print(\"JOIN WITHOUT BUCKETING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nPerforming join on non-bucketed tables...\")\n",
        "print(\"‚ö†Ô∏è  This will trigger a shuffle operation!\")\n",
        "\n",
        "start = time.time()\n",
        "joined_df = sales_df.join(customers_df, on=\"customer_id\", how=\"inner\")\n",
        "result_count = joined_df.count()\n",
        "join_time = time.time() - start\n",
        "\n",
        "print(f\"\\n‚úÖ Join completed!\")\n",
        "print(f\"   ‚Ä¢ Result: {result_count:,} rows\")\n",
        "print(f\"   ‚Ä¢ Time: {join_time:.3f} seconds\")\n",
        "print(f\"   ‚Ä¢ What happened:\")\n",
        "print(f\"     - All data from both tables was shuffled\")\n",
        "print(f\"     - Data was redistributed across the cluster\")\n",
        "print(f\"     - Expensive network I/O occurred\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create Bucketed Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CREATING BUCKETED TABLES\n",
            "======================================================================\n",
            "\n",
            "Creating bucketed tables with 32 buckets...\n",
            "Both tables will be bucketed by 'customer_id'\n",
            "\n",
            "1Ô∏è‚É£  Creating bucketed sales table...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/03 06:38:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
            "26/01/03 06:38:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
            "26/01/03 06:38:54 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
            "26/01/03 06:38:54 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore rohityadav@127.0.0.1\n",
            "26/01/03 06:38:54 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 95.00% for 8 writers\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 84.44% for 9 writers\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 76.00% for 10 writers\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 69.09% for 11 writers\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 76.00% for 10 writers\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 84.44% for 9 writers\n",
            "26/01/03 06:38:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
            "Scaling row group sizes to 95.00% for 8 writers\n",
            "26/01/03 06:39:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
            "26/01/03 06:39:17 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
            "26/01/03 06:39:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
            "26/01/03 06:39:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
            "26/01/03 06:39:17 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Sales table bucketed by customer_id\n",
            "\n",
            "2Ô∏è‚É£  Creating bucketed customers table...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Customers table bucketed by customer_id\n",
            "\n",
            "üí° Key Points:\n",
            "   ‚Ä¢ Both tables have 32 buckets\n",
            "   ‚Ä¢ Both are bucketed by the same column (customer_id)\n",
            "   ‚Ä¢ Same customer_id values will be in the same bucket number\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create bucketed tables\n",
        "print(\"=\" * 70)\n",
        "print(\"CREATING BUCKETED TABLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Number of buckets (should be a power of 2, typically 32, 64, 128, etc.)\n",
        "num_buckets = 32\n",
        "\n",
        "print(f\"\\nCreating bucketed tables with {num_buckets} buckets...\")\n",
        "print(\"Both tables will be bucketed by 'customer_id'\")\n",
        "\n",
        "# Write sales data as bucketed table\n",
        "print(\"\\n1Ô∏è‚É£  Creating bucketed sales table...\")\n",
        "sales_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .bucketBy(num_buckets, \"customer_id\") \\\n",
        "    .sortBy(\"customer_id\") \\\n",
        "    .saveAsTable(\"sales_bucketed\")\n",
        "\n",
        "print(\"   ‚úÖ Sales table bucketed by customer_id\")\n",
        "\n",
        "# Write customers data as bucketed table\n",
        "print(\"\\n2Ô∏è‚É£  Creating bucketed customers table...\")\n",
        "customers_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .bucketBy(num_buckets, \"customer_id\") \\\n",
        "    .sortBy(\"customer_id\") \\\n",
        "    .saveAsTable(\"customers_bucketed\")\n",
        "\n",
        "print(\"   ‚úÖ Customers table bucketed by customer_id\")\n",
        "\n",
        "print(f\"\\nüí° Key Points:\")\n",
        "print(f\"   ‚Ä¢ Both tables have {num_buckets} buckets\")\n",
        "print(f\"   ‚Ä¢ Both are bucketed by the same column (customer_id)\")\n",
        "print(f\"   ‚Ä¢ Same customer_id values will be in the same bucket number\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Join With Bucketing (The Solution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "JOIN WITH BUCKETING\n",
            "======================================================================\n",
            "\n",
            "Reading bucketed tables...\n",
            "Performing join on bucketed tables...\n",
            "‚úÖ This should avoid shuffle!\n",
            "\n",
            "‚úÖ Join completed!\n",
            "   ‚Ä¢ Result: 10,000 rows\n",
            "   ‚Ä¢ Time: 0.782 seconds\n",
            "   ‚Ä¢ What happened:\n",
            "     - Only corresponding buckets were joined (bucket 0 with bucket 0, etc.)\n",
            "     - No shuffle was needed (data already co-located)\n",
            "     - Minimal network I/O\n",
            "\n",
            "üöÄ Performance Comparison:\n",
            "   ‚Ä¢ Without bucketing: 1.037s\n",
            "   ‚Ä¢ With bucketing: 0.782s\n",
            "   ‚Ä¢ Bucketing is 1.33√ó faster!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Read bucketed tables and join\n",
        "print(\"=\" * 70)\n",
        "print(\"JOIN WITH BUCKETING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Read the bucketed tables\n",
        "sales_bucketed = spark.table(\"sales_bucketed\")\n",
        "customers_bucketed = spark.table(\"customers_bucketed\")\n",
        "\n",
        "print(\"\\nReading bucketed tables...\")\n",
        "print(\"Performing join on bucketed tables...\")\n",
        "print(\"‚úÖ This should avoid shuffle!\")\n",
        "\n",
        "start = time.time()\n",
        "joined_bucketed = sales_bucketed.join(customers_bucketed, on=\"customer_id\", how=\"inner\")\n",
        "result_count_bucketed = joined_bucketed.count()\n",
        "join_time_bucketed = time.time() - start\n",
        "\n",
        "print(f\"\\n‚úÖ Join completed!\")\n",
        "print(f\"   ‚Ä¢ Result: {result_count_bucketed:,} rows\")\n",
        "print(f\"   ‚Ä¢ Time: {join_time_bucketed:.3f} seconds\")\n",
        "print(f\"   ‚Ä¢ What happened:\")\n",
        "print(f\"     - Only corresponding buckets were joined (bucket 0 with bucket 0, etc.)\")\n",
        "print(f\"     - No shuffle was needed (data already co-located)\")\n",
        "print(f\"     - Minimal network I/O\")\n",
        "\n",
        "# Compare performance\n",
        "if join_time > 0:\n",
        "    speedup = join_time / join_time_bucketed\n",
        "    print(f\"\\nüöÄ Performance Comparison:\")\n",
        "    print(f\"   ‚Ä¢ Without bucketing: {join_time:.3f}s\")\n",
        "    print(f\"   ‚Ä¢ With bucketing: {join_time_bucketed:.3f}s\")\n",
        "    if speedup > 1:\n",
        "        print(f\"   ‚Ä¢ Bucketing is {speedup:.2f}√ó faster!\")\n",
        "    else:\n",
        "        print(f\"   ‚Ä¢ Note: For small datasets, the difference may not be significant\")\n",
        "        print(f\"     Bucketing shows more benefit with larger datasets and clusters\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Bucket Requirements\n",
        "\n",
        "### Critical Requirements for Bucketing to Work\n",
        "\n",
        "**For bucketing to optimize joins, you MUST:**\n",
        "\n",
        "1. **Same number of buckets**\n",
        "   - Both tables must have the same number of buckets\n",
        "   - Example: Both tables must have 32 buckets\n",
        "\n",
        "2. **Same bucketing column**\n",
        "   - Both tables must be bucketed by the same column(s)\n",
        "   - Example: Both bucketed by `customer_id`\n",
        "\n",
        "3. **Join on bucketing column**\n",
        "   - The join must be on the bucketing column\n",
        "   - Example: `df1.join(df2, on=\"customer_id\")` where both are bucketed by `customer_id`\n",
        "\n",
        "### What Happens If Requirements Aren't Met?\n",
        "\n",
        "**If requirements aren't met:**\n",
        "- Spark will still perform the join\n",
        "- But it will fall back to regular shuffle join\n",
        "- No benefit from bucketing\n",
        "- You'll get a warning in Spark UI\n",
        "\n",
        "### Example: Requirements Met ‚úÖ\n",
        "\n",
        "```python\n",
        "# Both tables: 32 buckets, bucketed by customer_id\n",
        "sales_df.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "customers_df.write.bucketBy(32, \"customer_id\").saveAsTable(\"customers\")\n",
        "\n",
        "# Join on customer_id\n",
        "result = spark.table(\"sales\").join(spark.table(\"customers\"), on=\"customer_id\")\n",
        "# ‚úÖ Bucket join - no shuffle!\n",
        "```\n",
        "\n",
        "### Example: Requirements NOT Met ‚ùå\n",
        "\n",
        "```python\n",
        "# Different number of buckets\n",
        "sales_df.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "customers_df.write.bucketBy(64, \"customer_id\").saveAsTable(\"customers\")\n",
        "\n",
        "# Join on customer_id\n",
        "result = spark.table(\"sales\").join(spark.table(\"customers\"), on=\"customer_id\")\n",
        "# ‚ùå Regular shuffle join - bucketing doesn't help!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing the Number of Buckets\n",
        "\n",
        "### How Many Buckets Should You Use?\n",
        "\n",
        "**General Guidelines:**\n",
        "\n",
        "1. **Power of 2** (recommended)\n",
        "   - 16, 32, 64, 128, 256, 512, 1024\n",
        "   - Easier for Spark to optimize\n",
        "\n",
        "2. **Based on data size**\n",
        "   - Small data (< 1 GB): 16-32 buckets\n",
        "   - Medium data (1-100 GB): 32-128 buckets\n",
        "   - Large data (> 100 GB): 128-512 buckets\n",
        "\n",
        "3. **Based on cluster size**\n",
        "   - Should be multiple of number of cores\n",
        "   - Example: 16 cores ‚Üí 32, 64, or 128 buckets\n",
        "\n",
        "4. **Avoid too many buckets**\n",
        "   - Too many small files (overhead)\n",
        "   - Generally avoid > 1000 buckets\n",
        "\n",
        "### Common Choices\n",
        "\n",
        "| Data Size | Recommended Buckets | Reason |\n",
        "|-----------|-------------------|--------|\n",
        "| < 1 GB | 16-32 | Small data, fewer buckets sufficient |\n",
        "| 1-10 GB | 32-64 | Balanced performance |\n",
        "| 10-100 GB | 64-128 | More buckets for better distribution |\n",
        "| > 100 GB | 128-512 | Large data needs more buckets |\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "> **Start with 32 or 64 buckets. Adjust based on your data size and performance requirements. Use powers of 2 for best results.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bucketing with Sorting\n",
        "\n",
        "### Adding Sort Within Buckets\n",
        "\n",
        "**You can also sort data within each bucket:**\n",
        "\n",
        "```python\n",
        "df.write \\\n",
        "    .bucketBy(32, \"customer_id\") \\\n",
        "    .sortBy(\"customer_id\", \"order_date\") \\\n",
        "    .saveAsTable(\"bucketed_sorted\")\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Data within each bucket is sorted\n",
        "- Can optimize range queries\n",
        "- Can optimize merge joins\n",
        "- Better compression\n",
        "\n",
        "**Trade-offs:**\n",
        "- Takes longer to write (sorting overhead)\n",
        "- More CPU during write\n",
        "- Use when you frequently query by sorted columns\n",
        "\n",
        "### When to Use Sorting\n",
        "\n",
        "**Use sorting when:**\n",
        "- You frequently filter by sorted columns\n",
        "- You do range queries\n",
        "- You want better compression\n",
        "- Write time is not critical\n",
        "\n",
        "**Skip sorting when:**\n",
        "- You only do equality joins\n",
        "- Write time is critical\n",
        "- Data is already well-distributed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use Bucketing\n",
        "\n",
        "### ‚úÖ Good Use Cases for Bucketing\n",
        "\n",
        "**1. Frequent Joins on Same Key**\n",
        "```python\n",
        "# You frequently join on customer_id\n",
        "sales.join(customers, on=\"customer_id\")\n",
        "orders.join(customers, on=\"customer_id\")\n",
        "# Bucket all tables by customer_id\n",
        "```\n",
        "\n",
        "**2. Large Tables**\n",
        "```python\n",
        "# Large tables that are joined frequently\n",
        "large_table1.write.bucketBy(128, \"join_key\").saveAsTable(\"table1\")\n",
        "large_table2.write.bucketBy(128, \"join_key\").saveAsTable(\"table2\")\n",
        "```\n",
        "\n",
        "**3. Multiple Joins on Same Column**\n",
        "```python\n",
        "# Multiple tables joined on same key\n",
        "df1.join(df2, on=\"key\").join(df3, on=\"key\")\n",
        "# Bucket all by \"key\"\n",
        "```\n",
        "\n",
        "**4. Aggregations After Joins**\n",
        "```python\n",
        "# Join then aggregate\n",
        "joined = df1.join(df2, on=\"key\")\n",
        "result = joined.groupBy(\"key\").agg(...)\n",
        "# Bucketing helps both join and aggregation\n",
        "```\n",
        "\n",
        "### ‚ùå When NOT to Use Bucketing\n",
        "\n",
        "**1. Small Tables**\n",
        "```python\n",
        "# Small tables don't benefit much\n",
        "small_df.write.bucketBy(32, \"key\")  # ‚ùå Overhead not worth it\n",
        "```\n",
        "\n",
        "**2. Frequently Changing Data**\n",
        "```python\n",
        "# If data changes frequently, bucketing overhead is high\n",
        "# Each write requires rebucketing\n",
        "```\n",
        "\n",
        "**3. Different Join Keys**\n",
        "```python\n",
        "# If you join on different keys, bucketing doesn't help\n",
        "df1.join(df2, on=\"key1\")\n",
        "df1.join(df3, on=\"key2\")  # Different key!\n",
        "```\n",
        "\n",
        "**4. One-Time Queries**\n",
        "```python\n",
        "# If you only query once, bucketing setup cost isn't worth it\n",
        "```\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "```\n",
        "Will you join these tables frequently?\n",
        "‚îÇ\n",
        "‚îú‚îÄ NO ‚Üí Don't bucket ‚ùå\n",
        "‚îÇ\n",
        "‚îî‚îÄ YES\n",
        "   ‚îÇ\n",
        "   ‚îú‚îÄ Are tables large (> 1 GB)?\n",
        "   ‚îÇ  ‚îÇ\n",
        "   ‚îÇ  ‚îú‚îÄ NO ‚Üí Don't bucket ‚ùå (overhead not worth it)\n",
        "   ‚îÇ  ‚îÇ\n",
        "   ‚îÇ  ‚îî‚îÄ YES ‚Üí Bucket ‚úÖ\n",
        "   ‚îÇ     ‚îÇ\n",
        "   ‚îÇ     ‚îî‚îÄ Will you join on the same key?\n",
        "   ‚îÇ        ‚îÇ\n",
        "   ‚îÇ        ‚îú‚îÄ NO ‚Üí Don't bucket ‚ùå (won't help)\n",
        "   ‚îÇ        ‚îÇ\n",
        "   ‚îÇ        ‚îî‚îÄ YES ‚Üí Bucket ‚úÖ\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bucketing vs Partitioning: When to Use Each\n",
        "\n",
        "### Use Partitioning When:\n",
        "\n",
        "**1. Time-Series Data**\n",
        "```python\n",
        "# Partition by date for time-based queries\n",
        "df.write.partitionBy(\"date\").parquet(\"path/\")\n",
        "```\n",
        "\n",
        "**2. Categorical Filtering**\n",
        "```python\n",
        "# Partition by region for region-based queries\n",
        "df.write.partitionBy(\"region\").parquet(\"path/\")\n",
        "```\n",
        "\n",
        "**3. Data Pruning**\n",
        "```python\n",
        "# Partitioning allows Spark to skip entire partitions\n",
        "spark.read.parquet(\"path/\").filter(col(\"date\") == \"2024-01-01\")\n",
        "# Only reads date=2024-01-01/ partition\n",
        "```\n",
        "\n",
        "### Use Bucketing When:\n",
        "\n",
        "**1. Join Optimization**\n",
        "```python\n",
        "# Bucket for join performance\n",
        "df1.write.bucketBy(32, \"join_key\").saveAsTable(\"table1\")\n",
        "df2.write.bucketBy(32, \"join_key\").saveAsTable(\"table2\")\n",
        "```\n",
        "\n",
        "**2. High Cardinality Columns**\n",
        "```python\n",
        "# Customer IDs, order IDs, etc. (too many distinct values for partitioning)\n",
        "df.write.bucketBy(64, \"customer_id\").saveAsTable(\"customers\")\n",
        "```\n",
        "\n",
        "**3. Multiple Joins on Same Key**\n",
        "```python\n",
        "# Multiple tables joined on same key\n",
        "# Bucket all by that key\n",
        "```\n",
        "\n",
        "### Use Both Together:\n",
        "\n",
        "**You can combine partitioning and bucketing:**\n",
        "\n",
        "```python\n",
        "# Partition by date, bucket by customer_id\n",
        "df.write \\\n",
        "    .partitionBy(\"date\") \\\n",
        "    .bucketBy(32, \"customer_id\") \\\n",
        "    .saveAsTable(\"sales\")\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Partitioning: Prunes by date (skips irrelevant dates)\n",
        "- Bucketing: Optimizes joins on customer_id\n",
        "\n",
        "**Use when:**\n",
        "- You filter by partition column (date)\n",
        "- You join on bucket column (customer_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### ‚úÖ DO\n",
        "\n",
        "1. **Use same number of buckets for joined tables**\n",
        "   ```python\n",
        "   # Both tables must have same bucket count\n",
        "   df1.write.bucketBy(32, \"key\").saveAsTable(\"table1\")\n",
        "   df2.write.bucketBy(32, \"key\").saveAsTable(\"table2\")  # Same: 32\n",
        "   ```\n",
        "\n",
        "2. **Use powers of 2 for bucket count**\n",
        "   ```python\n",
        "   # Good: 16, 32, 64, 128, 256\n",
        "   df.write.bucketBy(32, \"key\").saveAsTable(\"table\")\n",
        "   ```\n",
        "\n",
        "3. **Bucket by join keys**\n",
        "   ```python\n",
        "   # Bucket by columns you frequently join on\n",
        "   df.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "   ```\n",
        "\n",
        "4. **Use appropriate bucket count**\n",
        "   ```python\n",
        "   # Based on data size: 32-128 for most cases\n",
        "   df.write.bucketBy(64, \"key\").saveAsTable(\"table\")\n",
        "   ```\n",
        "\n",
        "5. **Combine with sorting when beneficial**\n",
        "   ```python\n",
        "   # Sort within buckets for range queries\n",
        "   df.write.bucketBy(32, \"key\").sortBy(\"key\", \"date\").saveAsTable(\"table\")\n",
        "   ```\n",
        "\n",
        "### ‚ùå DON'T\n",
        "\n",
        "1. **Don't use different bucket counts**\n",
        "   ```python\n",
        "   # ‚ùå BAD: Different bucket counts\n",
        "   df1.write.bucketBy(32, \"key\").saveAsTable(\"table1\")\n",
        "   df2.write.bucketBy(64, \"key\").saveAsTable(\"table2\")  # Won't work!\n",
        "   ```\n",
        "\n",
        "2. **Don't bucket small tables**\n",
        "   ```python\n",
        "   # ‚ùå BAD: Small table, overhead not worth it\n",
        "   small_df.write.bucketBy(32, \"key\")  # Overhead > benefit\n",
        "   ```\n",
        "\n",
        "3. **Don't use too many buckets**\n",
        "   ```python\n",
        "   # ‚ùå BAD: Too many small files\n",
        "   df.write.bucketBy(10000, \"key\")  # Creates 10000 tiny files!\n",
        "   ```\n",
        "\n",
        "4. **Don't bucket by wrong column**\n",
        "   ```python\n",
        "   # ‚ùå BAD: Bucketing by column you don't join on\n",
        "   df.write.bucketBy(32, \"product_name\")  # But you join on customer_id!\n",
        "   ```\n",
        "\n",
        "5. **Don't forget to bucket both tables**\n",
        "   ```python\n",
        "   # ‚ùå BAD: Only one table bucketed\n",
        "   df1.write.bucketBy(32, \"key\").saveAsTable(\"table1\")\n",
        "   df2.write.saveAsTable(\"table2\")  # Not bucketed - won't help!\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Mistakes and How to Avoid Them\n",
        "\n",
        "### Mistake 1: Different Bucket Counts\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "df1.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "df2.write.bucketBy(64, \"customer_id\").saveAsTable(\"customers\")\n",
        "# ‚ùå Different bucket counts - bucketing won't work!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "df1.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "df2.write.bucketBy(32, \"customer_id\").saveAsTable(\"customers\")\n",
        "# ‚úÖ Same bucket count - bucketing will work!\n",
        "```\n",
        "\n",
        "### Mistake 2: Bucketing Only One Table\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "df1.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "df2.write.saveAsTable(\"customers\")  # ‚ùå Not bucketed!\n",
        "# Join won't benefit from bucketing\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "df1.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "df2.write.bucketBy(32, \"customer_id\").saveAsTable(\"customers\")\n",
        "# ‚úÖ Both bucketed - join will be optimized!\n",
        "```\n",
        "\n",
        "### Mistake 3: Joining on Different Column\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# Bucketed by customer_id\n",
        "df1.write.bucketBy(32, \"customer_id\").saveAsTable(\"sales\")\n",
        "df2.write.bucketBy(32, \"customer_id\").saveAsTable(\"customers\")\n",
        "\n",
        "# But joining on different column\n",
        "result = spark.table(\"sales\").join(spark.table(\"customers\"), on=\"order_id\")\n",
        "# ‚ùå Join on order_id, but bucketed by customer_id - no benefit!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# Bucket by the join key\n",
        "df1.write.bucketBy(32, \"order_id\").saveAsTable(\"sales\")\n",
        "df2.write.bucketBy(32, \"order_id\").saveAsTable(\"customers\")\n",
        "\n",
        "# Join on same column\n",
        "result = spark.table(\"sales\").join(spark.table(\"customers\"), on=\"order_id\")\n",
        "# ‚úÖ Join on order_id, bucketed by order_id - works!\n",
        "```\n",
        "\n",
        "### Mistake 4: Too Many Buckets\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# ‚ùå BAD: Too many buckets for small data\n",
        "small_df.write.bucketBy(1000, \"key\").saveAsTable(\"table\")\n",
        "# Creates 1000 tiny files - overhead!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# ‚úÖ GOOD: Appropriate number of buckets\n",
        "small_df.write.bucketBy(32, \"key\").saveAsTable(\"table\")\n",
        "# Creates 32 reasonably-sized files\n",
        "```\n",
        "\n",
        "### Mistake 5: Not Using Hive Tables\n",
        "\n",
        "**Wrong:**\n",
        "```python\n",
        "# ‚ùå BAD: Writing to Parquet (bucketing metadata lost)\n",
        "df.write.bucketBy(32, \"key\").parquet(\"path/\")\n",
        "# Bucketing information not preserved!\n",
        "```\n",
        "\n",
        "**Correct:**\n",
        "```python\n",
        "# ‚úÖ GOOD: Using saveAsTable (preserves bucketing)\n",
        "df.write.bucketBy(32, \"key\").saveAsTable(\"table_name\")\n",
        "# Bucketing information preserved in Hive metastore\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### The Core Concept\n",
        "\n",
        "**Bucketing:**\n",
        "- ‚úÖ Divides data into fixed number of buckets based on hash\n",
        "- ‚úÖ Pre-organizes data for efficient joins\n",
        "- ‚úÖ Avoids expensive shuffles during joins\n",
        "- ‚úÖ Requires same bucket count and column for joined tables\n",
        "\n",
        "**Partitioning:**\n",
        "- ‚úÖ Divides data by actual column values\n",
        "- ‚úÖ Creates separate directories\n",
        "- ‚úÖ Used for filtering and data pruning\n",
        "- ‚úÖ Variable number of partitions\n",
        "\n",
        "### When to Use Bucketing\n",
        "\n",
        "**Use bucketing when:**\n",
        "- You frequently join large tables\n",
        "- Join is on the same column(s)\n",
        "- Tables are large (> 1 GB)\n",
        "- You can control how data is written\n",
        "\n",
        "**Don't use bucketing when:**\n",
        "- Tables are small\n",
        "- You join on different columns\n",
        "- Data changes frequently\n",
        "- One-time queries\n",
        "\n",
        "### Requirements for Bucket Joins\n",
        "\n",
        "1. **Same number of buckets** in both tables\n",
        "2. **Same bucketing column(s)** in both tables\n",
        "3. **Join on bucketing column(s)**\n",
        "4. **Tables must be saved as Hive tables** (saveAsTable)\n",
        "\n",
        "### The Golden Rules\n",
        "\n",
        "1. **Bucket count must match** for joined tables\n",
        "2. **Bucket by join keys** for maximum benefit\n",
        "3. **Use powers of 2** for bucket count (16, 32, 64, 128)\n",
        "4. **Choose bucket count** based on data size\n",
        "5. **Use saveAsTable** to preserve bucketing metadata\n",
        "\n",
        "### Remember\n",
        "\n",
        "1. **Bucketing = Hash-based organization for joins**\n",
        "2. **Partitioning = Value-based organization for filtering**\n",
        "3. **Both tables must have same bucket count and column**\n",
        "4. **Bucketing avoids shuffles, partitioning avoids reads**\n",
        "5. **You can combine partitioning and bucketing**\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Practice creating bucketed tables\n",
        "- Monitor Spark UI to see bucket joins in action\n",
        "- Experiment with different bucket counts\n",
        "- Review `06_joins.ipynb` to understand join optimization\n",
        "- Review `08_b_Partitions_Concepts.ipynb` to understand partitioning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **What bucketing is**\n",
        "   - Divides data into fixed number of buckets based on hash\n",
        "   - Pre-organizes data for efficient joins\n",
        "   - Different from partitioning (which uses actual values)\n",
        "\n",
        "2. **Why bucketing matters**\n",
        "   - Optimizes joins by avoiding shuffles\n",
        "   - Pre-organizes data so matching rows are co-located\n",
        "   - Significantly improves join performance\n",
        "\n",
        "3. **How to create bucketed tables**\n",
        "   - Use `bucketBy(num_buckets, \"column\")`\n",
        "   - Must use `saveAsTable()` to preserve metadata\n",
        "   - Can combine with `sortBy()` for additional optimization\n",
        "\n",
        "4. **Requirements for bucket joins**\n",
        "   - Same number of buckets in both tables\n",
        "   - Same bucketing column(s)\n",
        "   - Join on bucketing column(s)\n",
        "   - Tables saved as Hive tables\n",
        "\n",
        "5. **When to use bucketing**\n",
        "   - Frequent joins on same key\n",
        "   - Large tables (> 1 GB)\n",
        "   - Multiple joins on same column\n",
        "   - When you can control data writing\n",
        "\n",
        "6. **Best practices**\n",
        "   - Use powers of 2 for bucket count\n",
        "   - Choose bucket count based on data size\n",
        "   - Bucket by join keys\n",
        "   - Ensure both tables have same bucket configuration\n",
        "\n",
        "### The Bottom Line\n",
        "\n",
        "> **Bucketing is a powerful optimization technique that pre-organizes data for efficient joins. By ensuring rows with the same join key values are in the same bucket, Spark can join corresponding buckets without expensive shuffles. Use bucketing when you frequently join large tables on the same key, but remember that both tables must have the same bucket count and bucketing column for it to work.**\n",
        "\n",
        "---\n",
        "\n",
        "**Related Notebooks:**\n",
        "- `08_b_Partitions_Concepts.ipynb` - Understanding partitioning\n",
        "- `06_joins.ipynb` - Understanding joins and join optimization\n",
        "- `08_a_Spark_Architecture.ipynb` - Understanding executors, cores, and tasks\n",
        "- `08_performance_optimization.ipynb` - Comprehensive performance optimization guide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "# Clean up - drop tables\n",
        "spark.sql(\"DROP TABLE IF EXISTS sales_bucketed\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS customers_bucketed\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
