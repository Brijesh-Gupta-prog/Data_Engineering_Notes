{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 2 - Reading and Writing Data\n",
        "\n",
        "## Introduction\n",
        "\n",
        "One of the most common tasks in data engineering is reading data from various file formats and writing processed data back. PySpark supports many file formats including CSV, JSON, TXT, Parquet, and more.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Reading CSV files\n",
        "- Reading JSON files\n",
        "- Reading text files\n",
        "- Writing DataFrames to files\n",
        "- Understanding file format options\n",
        "- Best practices for reading and writing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "import os\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Reading and Writing Files\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading CSV Files\n",
        "\n",
        "CSV (Comma-Separated Values) is one of the most common file formats. PySpark can read CSV files with various options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample CSV file created!\n"
          ]
        }
      ],
      "source": [
        "# First, let's create a sample CSV file for demonstration\n",
        "# Create a sample CSV file\n",
        "csv_content = \"\"\"Name,Age,City,Salary\n",
        "Alice,25,New York,50000\n",
        "Bob,30,London,60000\n",
        "Charlie,35,Tokyo,70000\n",
        "Diana,28,Paris,55000\n",
        "Eve,32,Sydney,65000\"\"\"\n",
        "\n",
        "# Write to file\n",
        "with open(\"data/sample_data.csv\", \"w\") as f:\n",
        "    f.write(csv_content)\n",
        "\n",
        "print(\"Sample CSV file created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file read successfully!\n",
            "+-------+---+--------+------+\n",
            "|   Name|Age|    City|Salary|\n",
            "+-------+---+--------+------+\n",
            "|  Alice| 25|New York| 50000|\n",
            "|    Bob| 30|  London| 60000|\n",
            "|Charlie| 35|   Tokyo| 70000|\n",
            "|  Diana| 28|   Paris| 55000|\n",
            "|    Eve| 32|  Sydney| 65000|\n",
            "+-------+---+--------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read CSV file - basic usage\n",
        "df_csv = spark.read.csv(\"data/sample_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(\"CSV file read successfully!\")\n",
        "df_csv.show()\n",
        "df_csv.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSV Reading Options\n",
        "\n",
        "**Common Options:**\n",
        "- `header=True`: First row contains column names\n",
        "- `inferSchema=True`: Automatically detect data types (slower but convenient)\n",
        "- `sep`: Delimiter (default: comma)\n",
        "- `nullValue`: String to treat as null\n",
        "- `dateFormat`: Date format string\n",
        "\n",
        "**Note**: `inferSchema=True` scans the entire file, which can be slow for large files. For production, define schema explicitly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two Ways to Enforce Schema\n",
        "\n",
        "When reading data, you can enforce schema in two ways:\n",
        "\n",
        "### 1. Schema Option - Schema DDL (String Format)\n",
        "\n",
        "You can specify schema as a DDL (Data Definition Language) string directly in the `.schema()` method:\n",
        "\n",
        "```python\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(\"Name STRING, Age INT, City STRING, Salary INT\") \\\n",
        "    .load(\"path/to/file\")\n",
        "```\n",
        "\n",
        "**Advantages:**\n",
        "- Simple and concise\n",
        "- Easy to read and write\n",
        "- Good for quick prototyping\n",
        "\n",
        "### 2. StructType (Programmatic Schema Definition)\n",
        "\n",
        "You can define schema using `StructType` and `StructField` from `pyspark.sql.types`:\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"Salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(\"path/to/file\")\n",
        "```\n",
        "\n",
        "**Advantages:**\n",
        "- More explicit and type-safe\n",
        "- Better for complex schemas\n",
        "- Preferred for production code\n",
        "- Allows for more control over nullable fields and metadata\n",
        "\n",
        "**Note:** Both methods enforce schema at read time, which is faster and more reliable than using `inferSchema=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method 1: Schema DDL\n",
            "+-------+---+--------+------+\n",
            "|   Name|Age|    City|Salary|\n",
            "+-------+---+--------+------+\n",
            "|  Alice| 25|New York| 50000|\n",
            "|    Bob| 30|  London| 60000|\n",
            "|Charlie| 35|   Tokyo| 70000|\n",
            "|  Diana| 28|   Paris| 55000|\n",
            "|    Eve| 32|  Sydney| 65000|\n",
            "+-------+---+--------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Method 2: StructType\n",
            "+-------+---+--------+------+\n",
            "|   Name|Age|    City|Salary|\n",
            "+-------+---+--------+------+\n",
            "|  Alice| 25|New York| 50000|\n",
            "|    Bob| 30|  London| 60000|\n",
            "|Charlie| 35|   Tokyo| 70000|\n",
            "|  Diana| 28|   Paris| 55000|\n",
            "|    Eve| 32|  Sydney| 65000|\n",
            "+-------+---+--------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Using Schema DDL (String Format)\n",
        "df_csv_ddl = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(\"Name STRING, Age INT, City STRING, Salary INT\") \\\n",
        "    .load(\"data/sample_data.csv\")\n",
        "\n",
        "print(\"Method 1: Schema DDL\")\n",
        "df_csv_ddl.show()\n",
        "df_csv_ddl.printSchema()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Method 2: Using StructType (Programmatic Schema)\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"Salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df_csv_struct = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(\"data/sample_data.csv\")\n",
        "\n",
        "print(\"Method 2: StructType\")\n",
        "df_csv_struct.show()\n",
        "df_csv_struct.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Date Types\n",
        "\n",
        "### Default Date Format in Spark\n",
        "\n",
        "The default format of date type in Spark is **`yyyy-MM-dd`** (e.g., `2024-12-28`). If the date format in your data is different from this format, Spark will fail to parse it and you'll get a **parse error**.\n",
        "\n",
        "**Important Note**: In case of parse issues, the **complete date column shows up as null**. This means all values in that column will be `null` if Spark cannot parse the dates according to the expected format.\n",
        "\n",
        "### Two Ways to Handle Different Date Formats\n",
        "\n",
        "#### Method 1: Use `dateFormat` Option While Reading\n",
        "\n",
        "You can specify the date format explicitly using the `.option(\"dateFormat\", \"...\")` parameter when reading the DataFrame:\n",
        "\n",
        "```python\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(\"Name STRING, BirthDate DATE, Salary INT\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"dateFormat\", \"dd/MM/yyyy\") \\\n",
        "    .load(\"path/to/file\")\n",
        "```\n",
        "\n",
        "**Advantages:**\n",
        "- Date is parsed directly during read\n",
        "- Column is already of `DATE` type\n",
        "- No additional transformation needed\n",
        "\n",
        "#### Method 2: Load Date as String and Convert Later\n",
        "\n",
        "If you're unsure about the date format or need more flexibility, you can:\n",
        "1. Load the date column as a `STRING` type\n",
        "2. Apply transformation to convert it to `DATE` type using `to_date()` function\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Read date as string\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(\"Name STRING, BirthDate STRING, Salary INT\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"path/to/file\")\n",
        "\n",
        "# Convert string to date\n",
        "df = df.withColumn(\"BirthDate\", to_date(\"BirthDate\", \"dd/MM/yyyy\"))\n",
        "```\n",
        "\n",
        "**Advantages:**\n",
        "- More flexible - can handle multiple date formats\n",
        "- Can inspect the string values before conversion\n",
        "- Useful when date format is inconsistent in the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample CSV file with dates created!\n",
            "Date format: dd/MM/yyyy (e.g., 15/03/1999)\n"
          ]
        }
      ],
      "source": [
        "# Create a sample CSV file with dates in non-standard format (dd/MM/yyyy)\n",
        "csv_with_dates = \"\"\"Name,Age,BirthDate,Salary\n",
        "Alice,25,15/03/1999,50000\n",
        "Bob,30,22/07/1994,60000\n",
        "Charlie,35,10/11/1989,70000\n",
        "Diana,28,05/01/1996,55000\n",
        "Eve,32,18/09/1992,65000\"\"\"\n",
        "\n",
        "with open(\"data/sample_data_with_dates.csv\", \"w\") as f:\n",
        "    f.write(csv_with_dates)\n",
        "\n",
        "print(\"Sample CSV file with dates created!\")\n",
        "print(\"Date format: dd/MM/yyyy (e.g., 15/03/1999)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Example 1: Reading without specifying dateFormat\n",
            "============================================================\n",
            "\n",
            "Notice: All BirthDate values are null due to parse error!\n",
            "+-------+---+---------+------+\n",
            "|   Name|Age|BirthDate|Salary|\n",
            "+-------+---+---------+------+\n",
            "|  Alice| 25|     NULL| 50000|\n",
            "|    Bob| 30|     NULL| 60000|\n",
            "|Charlie| 35|     NULL| 70000|\n",
            "|  Diana| 28|     NULL| 55000|\n",
            "|    Eve| 32|     NULL| 65000|\n",
            "+-------+---+---------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- BirthDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1: What happens when date format doesn't match (all dates become null)\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Reading without specifying dateFormat\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to read with DATE type but without dateFormat option\n",
        "# Since format is dd/MM/yyyy but Spark expects yyyy-MM-dd, all dates will be null\n",
        "df_wrong_format = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(\"Name STRING, Age INT, BirthDate DATE, Salary INT\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"data/sample_data_with_dates.csv\")\n",
        "\n",
        "print(\"\\nNotice: All BirthDate values are null due to parse error!\")\n",
        "df_wrong_format.show()\n",
        "df_wrong_format.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Method 1: Using dateFormat option\n",
            "============================================================\n",
            "\n",
            "Successfully parsed dates using dateFormat option!\n",
            "+-------+---+----------+------+\n",
            "|   Name|Age| BirthDate|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|1999-03-15| 50000|\n",
            "|    Bob| 30|1994-07-22| 60000|\n",
            "|Charlie| 35|1989-11-10| 70000|\n",
            "|  Diana| 28|1996-01-05| 55000|\n",
            "|    Eve| 32|1992-09-18| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- BirthDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Method 1 - Using dateFormat option while reading\n",
        "print(\"=\" * 60)\n",
        "print(\"Method 1: Using dateFormat option\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_method1 = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(\"Name STRING, Age INT, BirthDate DATE, Salary INT\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"dateFormat\", \"dd/MM/yyyy\") \\\n",
        "    .load(\"data/sample_data_with_dates.csv\")\n",
        "\n",
        "print(\"\\nSuccessfully parsed dates using dateFormat option!\")\n",
        "df_method1.show()\n",
        "df_method1.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Method 2: Load as string and convert using to_date()\n",
            "============================================================\n",
            "\n",
            "Step 1: Date column read as STRING\n",
            "+-------+---+----------+------+\n",
            "|   Name|Age| BirthDate|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|15/03/1999| 50000|\n",
            "|    Bob| 30|22/07/1994| 60000|\n",
            "|Charlie| 35|10/11/1989| 70000|\n",
            "|  Diana| 28|05/01/1996| 55000|\n",
            "|    Eve| 32|18/09/1992| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- BirthDate: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "\n",
            "Step 2: After converting STRING to DATE using to_date()\n",
            "+-------+---+----------+------+\n",
            "|   Name|Age| BirthDate|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|1999-03-15| 50000|\n",
            "|    Bob| 30|1994-07-22| 60000|\n",
            "|Charlie| 35|1989-11-10| 70000|\n",
            "|  Diana| 28|1996-01-05| 55000|\n",
            "|    Eve| 32|1992-09-18| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- BirthDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Method 2 - Load as string and convert later\n",
        "print(\"=\" * 60)\n",
        "print(\"Method 2: Load as string and convert using to_date()\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Step 1: Read date column as STRING\n",
        "df_method2_string = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(\"Name STRING, Age INT, BirthDate STRING, Salary INT\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"data/sample_data_with_dates.csv\")\n",
        "\n",
        "print(\"\\nStep 1: Date column read as STRING\")\n",
        "df_method2_string.show()\n",
        "df_method2_string.printSchema()\n",
        "\n",
        "# Step 2: Convert string to date using to_date()\n",
        "df_method2 = df_method2_string.withColumn(\n",
        "    \"BirthDate\", \n",
        "    to_date(\"BirthDate\", \"dd/MM/yyyy\")\n",
        ")\n",
        "\n",
        "print(\"\\nStep 2: After converting STRING to DATE using to_date()\")\n",
        "df_method2.show()\n",
        "df_method2.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional Notes on Date Formats\n",
        "\n",
        "**Common Date Format Patterns:**\n",
        "- `yyyy-MM-dd` - Default Spark format (e.g., `2024-12-28`)\n",
        "- `dd/MM/yyyy` - European format (e.g., `28/12/2024`)\n",
        "- `MM/dd/yyyy` - US format (e.g., `12/28/2024`)\n",
        "- `dd-MM-yyyy` - Alternative format (e.g., `28-12-2024`)\n",
        "- `yyyy/MM/dd` - Alternative format (e.g., `2024/12/28`)\n",
        "\n",
        "**Using StructType with dateFormat:**\n",
        "\n",
        "You can also use `StructType` schema definition with the `dateFormat` option:\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"BirthDate\", DateType(), True),  # DateType() for dates\n",
        "    StructField(\"Salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"dateFormat\", \"dd/MM/yyyy\") \\\n",
        "    .load(\"path/to/file\")\n",
        "```\n",
        "\n",
        "**Key Takeaway**: Always specify the `dateFormat` option when your data doesn't match Spark's default `yyyy-MM-dd` format, otherwise all date values will be `null`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Example: Using StructType schema with dateFormat\n",
            "============================================================\n",
            "\n",
            "Using StructType with DateType and dateFormat option:\n",
            "+-------+---+----------+------+\n",
            "|   Name|Age| BirthDate|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|1999-03-15| 50000|\n",
            "|    Bob| 30|1994-07-22| 60000|\n",
            "|Charlie| 35|1989-11-10| 70000|\n",
            "|  Diana| 28|1996-01-05| 55000|\n",
            "|    Eve| 32|1992-09-18| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- BirthDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 4: Using StructType with dateFormat option\n",
        "print(\"=\" * 60)\n",
        "print(\"Example: Using StructType schema with dateFormat\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"BirthDate\", DateType(), True),  # DateType() for dates\n",
        "    StructField(\"Salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df_struct_date = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"dateFormat\", \"dd/MM/yyyy\") \\\n",
        "    .load(\"data/sample_data_with_dates.csv\")\n",
        "\n",
        "print(\"\\nUsing StructType with DateType and dateFormat option:\")\n",
        "df_struct_date.show()\n",
        "df_struct_date.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading JSON Files\n",
        "\n",
        "JSON (JavaScript Object Notation) is another common format, especially for APIs and semi-structured data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample JSON file created!\n"
          ]
        }
      ],
      "source": [
        "# Create a sample JSON file\n",
        "json_content = \"\"\"{\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\", \"salary\": 50000}\n",
        "{\"name\": \"Bob\", \"age\": 30, \"city\": \"London\", \"salary\": 60000}\n",
        "{\"name\": \"Charlie\", \"age\": 35, \"city\": \"Tokyo\", \"salary\": 70000}\n",
        "{\"name\": \"Diana\", \"age\": 28, \"city\": \"Paris\", \"salary\": 55000}\n",
        "{\"name\": \"Eve\", \"age\": 32, \"city\": \"Sydney\", \"salary\": 65000}\"\"\"\n",
        "\n",
        "# Write to file (JSON Lines format - one JSON object per line)\n",
        "with open(\"data/sample_data.json\", \"w\") as f:\n",
        "    f.write(json_content)\n",
        "\n",
        "print(\"Sample JSON file created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON file read successfully!\n",
            "+---+--------+-------+------+\n",
            "|age|    city|   name|salary|\n",
            "+---+--------+-------+------+\n",
            "| 25|New York|  Alice| 50000|\n",
            "| 30|  London|    Bob| 60000|\n",
            "| 35|   Tokyo|Charlie| 70000|\n",
            "| 28|   Paris|  Diana| 55000|\n",
            "| 32|  Sydney|    Eve| 65000|\n",
            "+---+--------+-------+------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read JSON file\n",
        "df_json = spark.read.json(\"data/sample_data.json\")\n",
        "\n",
        "print(\"JSON file read successfully!\")\n",
        "df_json.show()\n",
        "df_json.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading Text Files\n",
        "\n",
        "Text files can be read as DataFrames where each line becomes a row with a single column. Useful for unstructured data or when you need to process raw text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text file created!\n"
          ]
        }
      ],
      "source": [
        "# Create a sample text file\n",
        "text_content = \"\"\"This is line 1\n",
        "This is line 2\n",
        "This is line 3\n",
        "This is line 4\n",
        "This is line 5\"\"\"\n",
        "\n",
        "with open(\"data/sample_data.txt\", \"w\") as f:\n",
        "    f.write(text_content)\n",
        "\n",
        "print(\"Sample text file created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text file read as DataFrame:\n",
            "Number of lines: 5\n",
            "\n",
            "Content:\n",
            "+--------------+\n",
            "|value         |\n",
            "+--------------+\n",
            "|This is line 1|\n",
            "|This is line 2|\n",
            "|This is line 3|\n",
            "|This is line 4|\n",
            "|This is line 5|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read text file as DataFrame (each line becomes a row)\n",
        "df_text = spark.read.text(\"data/sample_data.txt\")\n",
        "\n",
        "print(\"Text file read as DataFrame:\")\n",
        "print(f\"Number of lines: {df_text.count()}\")\n",
        "print(\"\\nContent:\")\n",
        "df_text.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading Multiple Files\n",
        "\n",
        "PySpark can read multiple files at once. Just provide a directory path or a pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multiple CSV files created!\n"
          ]
        }
      ],
      "source": [
        "# Create multiple CSV files\n",
        "csv1 = \"\"\"Name,Age\n",
        "Alice,25\n",
        "Bob,30\"\"\"\n",
        "\n",
        "csv2 = \"\"\"Name,Age\n",
        "Charlie,35\n",
        "Diana,28\"\"\"\n",
        "\n",
        "with open(\"data/data_part1.csv\", \"w\") as f:\n",
        "    f.write(csv1)\n",
        "    \n",
        "with open(\"data/data_part2.csv\", \"w\") as f:\n",
        "    f.write(csv2)\n",
        "\n",
        "print(\"Multiple CSV files created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read multiple files using wildcard pattern\n",
        "df_multiple = spark.read.csv(\"data/data_part*.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(\"Reading multiple files:\")\n",
        "df_multiple.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing DataFrames to Files\n",
        "\n",
        "After processing data, you often need to write it back to files. PySpark supports writing to various formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame written to data/output_data.csv\n",
            "Note: Spark creates a directory with part files for parallel writing\n"
          ]
        }
      ],
      "source": [
        "# Write DataFrame to CSV\n",
        "output_path = \"data/output_data.csv\"\n",
        "\n",
        "# Note: This creates a directory with part files (Spark writes in parallel)\n",
        "df_csv.write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(f\"DataFrame written to {output_path}\")\n",
        "print(\"Note: Spark creates a directory with part files for parallel writing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame written to data/output_data.json\n"
          ]
        }
      ],
      "source": [
        "# Write DataFrame to JSON\n",
        "output_json_path = \"data/output_data.json\"\n",
        "\n",
        "df_json.write.json(output_json_path, mode=\"overwrite\")\n",
        "\n",
        "print(f\"DataFrame written to {output_json_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write Modes\n",
        "\n",
        "When writing files, you can specify the mode to control how Spark handles existing data:\n",
        "\n",
        "### 1. `overwrite`\n",
        "\n",
        "If the folder where the results need to be written already exists, then it will be **overwritten**.\n",
        "\n",
        "**Use case**: When you want to replace existing data completely.\n",
        "\n",
        "```python\n",
        "df.write.mode(\"overwrite\").csv(\"path/to/output\")\n",
        "```\n",
        "\n",
        "### 2. `ignore`\n",
        "\n",
        "If the folder already exists, writing files will be **ignored** (no error, no write).\n",
        "\n",
        "**Use case**: When you want to skip writing if data already exists, without throwing an error.\n",
        "\n",
        "```python\n",
        "df.write.mode(\"ignore\").csv(\"path/to/output\")\n",
        "```\n",
        "\n",
        "### 3. `append`\n",
        "\n",
        "If the folder already exists, **new files will be appended** to the existing folder.\n",
        "\n",
        "**Use case**: When you want to add new data to existing data (e.g., incremental loads).\n",
        "\n",
        "```python\n",
        "df.write.mode(\"append\").csv(\"path/to/output\")\n",
        "```\n",
        "\n",
        "### 4. `errorIfExists` (default)\n",
        "\n",
        "If the folder already exists, the write operation will **throw an error**.\n",
        "\n",
        "**Use case**: When you want to ensure you don't accidentally overwrite existing data.\n",
        "\n",
        "```python\n",
        "df.write.mode(\"errorIfExists\").csv(\"path/to/output\")\n",
        "# or simply\n",
        "df.write.csv(\"path/to/output\")  # errorIfExists is the default\n",
        "```\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Mode | Behavior if folder exists |\n",
        "|------|---------------------------|\n",
        "| `overwrite` | Overwrites existing data |\n",
        "| `ignore` | Ignores write (no error) |\n",
        "| `append` | Appends new files to existing folder |\n",
        "| `errorIfExists` | Throws an error (default behavior) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written with 'overwrite' mode\n",
            "Appended with 'append' mode\n"
          ]
        }
      ],
      "source": [
        "# Example: Write with different modes\n",
        "df_csv.write.csv(\"data/output_mode_example.csv\", header=True, mode=\"overwrite\")\n",
        "print(\"Written with 'overwrite' mode\")\n",
        "\n",
        "# Append mode\n",
        "df_csv.write.csv(\"data/output_mode_example.csv\", header=True, mode=\"append\")\n",
        "print(\"Appended with 'append' mode\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing Single File (Coalesce)\n",
        "\n",
        "By default, Spark writes multiple part files (one per partition). To write a single file, use `coalesce(1)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written as single file using coalesce(1)\n",
            "Note: Use coalesce(1) only for small datasets. For large data, multiple files are better for parallel processing.\n"
          ]
        }
      ],
      "source": [
        "# Write as single file using coalesce\n",
        "df_csv.coalesce(1).write.csv(\"data/output_single_file.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "print(\"Written as single file using coalesce(1)\")\n",
        "print(\"Note: Use coalesce(1) only for small datasets. For large data, multiple files are better for parallel processing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "1. **Define Schema Explicitly**: For production, always define schema instead of using `inferSchema=True`\n",
        "2. **Use Parquet for Large Data**: Parquet is columnar and compressed (we'll learn this later)\n",
        "3. **Multiple Files are OK**: Spark writes multiple part files - this is normal and efficient\n",
        "4. **Avoid coalesce(1) for Large Data**: Only use for small datasets that need to be single files\n",
        "5. **Use Appropriate Write Modes**: Choose the right mode based on your use case\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you learned:\n",
        "\n",
        "1. **Reading CSV Files**: Using `spark.read.csv()` with options like `header`, `inferSchema`, and explicit schema\n",
        "2. **Reading JSON Files**: Using `spark.read.json()` for JSON data\n",
        "3. **Reading Text Files**: Using `spark.read.text()` for unstructured text\n",
        "4. **Reading Multiple Files**: Using wildcard patterns to read multiple files\n",
        "5. **Writing DataFrames**: Using `write.csv()`, `write.json()` with different modes\n",
        "6. **Write Modes**: `overwrite`, `append`, `ignore`, `error`\n",
        "7. **Single File Output**: Using `coalesce(1)` for small datasets\n",
        "\n",
        "**Key Takeaway**: PySpark can read and write various file formats. Always define schemas explicitly for production code, and understand that Spark writes multiple files by default for parallel processing.\n",
        "\n",
        "**Next Steps**: In Module 3, we'll learn about basic DataFrame operations like filtering, selecting columns, and sorting data.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
