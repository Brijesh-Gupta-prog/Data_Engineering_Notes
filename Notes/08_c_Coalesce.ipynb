{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e4d33f",
   "metadata": {},
   "source": [
    "# Understanding Coalesce: Efficiently Reducing Partitions\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **What coalesce is** and how it works internally\n",
    "2. **When to use coalesce** vs when to use repartition\n",
    "3. **How coalesce differs from repartition** (the critical distinction)\n",
    "4. **Performance implications** of coalesce vs repartition\n",
    "5. **Best practices** for using coalesce in production\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of Spark architecture (executors, cores, tasks) - see `08_a_Spark_Architecture.ipynb`\n",
    "- Understanding of partitions and their impact - see `08_b_Partitions_Concepts.ipynb`\n",
    "- Basic familiarity with Spark DataFrame operations\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This notebook builds on the concepts from `08_a_Spark_Architecture.ipynb` and `08_b_Partitions_Concepts.ipynb`. Make sure you understand partitions, tasks, and cores before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5bf6b",
   "metadata": {},
   "source": [
    "## Introduction: The Problem Coalesce Solves\n",
    "\n",
    "### Common Scenario\n",
    "\n",
    "You've been working with Spark and you've learned that:\n",
    "- Having too few partitions wastes cores (underutilization)\n",
    "- Having too many partitions creates overhead (scheduling inefficiency)\n",
    "\n",
    "**But what if you have the opposite problem?**\n",
    "\n",
    "**Scenario:**\n",
    "- You read data and it creates **100 partitions**\n",
    "- Your cluster has only **16 cores**\n",
    "- You want to reduce to **32 partitions** (2Ã— cores for optimal performance)\n",
    "\n",
    "**Question:** Should you use `repartition(32)` or `coalesce(32)`?\n",
    "\n",
    "**Answer:** Use `coalesce(32)` - it's more efficient!\n",
    "\n",
    "**Why?** Let's understand the fundamental difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b9a64",
   "metadata": {},
   "source": [
    "## What is Coalesce?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**`coalesce(numPartitions)`** is a Spark operation that **reduces** the number of partitions in a DataFrame or RDD **without performing a full shuffle**.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "1. **Only reduces partitions** - Cannot increase partition count\n",
    "2. **No full shuffle** - More efficient than repartition when reducing\n",
    "3. **Combines adjacent partitions** - Merges partitions that are already on the same executor\n",
    "4. **Minimal data movement** - Only moves data when necessary\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Before Coalesce (100 partitions):\n",
    "â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â” ... (100 partitions)\n",
    "â”‚ P0 â”‚â”‚ P1 â”‚â”‚ P2 â”‚â”‚ P3 â”‚â”‚ P4 â”‚â”‚ P5 â”‚â”‚ P6 â”‚â”‚ P7 â”‚\n",
    "â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜\n",
    "\n",
    "After coalesce(32):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” ... (32 partitions)\n",
    "â”‚ P0+P1+ â”‚â”‚ P4+P5+ â”‚â”‚ P8+P9+ â”‚â”‚ P12+...â”‚\n",
    "â”‚ P2+P3  â”‚â”‚ P6+P7  â”‚â”‚ P10+11 â”‚â”‚        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Key: Partitions are combined locally (on the same executor) when possible\n",
    "```\n",
    "\n",
    "### How Coalesce Works Internally\n",
    "\n",
    "**Step 1: Partition Mapping**\n",
    "- Coalesce maps multiple source partitions to fewer target partitions\n",
    "- It tries to keep partitions on the same executor when possible\n",
    "\n",
    "**Step 2: Local Combination**\n",
    "- Partitions on the same executor are combined locally (no network transfer)\n",
    "- Only when partitions need to be moved across executors does network I/O occur\n",
    "\n",
    "**Step 3: Result**\n",
    "- Fewer partitions, but data is mostly reorganized locally\n",
    "- Much less expensive than a full shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145c787",
   "metadata": {},
   "source": [
    "## Coalesce vs Repartition: The Critical Difference\n",
    "\n",
    "### The Fundamental Distinction\n",
    "\n",
    "| Aspect | `coalesce()` | `repartition()` |\n",
    "|--------|-------------|-----------------|\n",
    "| **Can increase partitions?** | âŒ No | âœ… Yes |\n",
    "| **Can decrease partitions?** | âœ… Yes | âœ… Yes |\n",
    "| **Shuffle operation?** | âš ï¸ Partial (minimal) | âœ… Full shuffle |\n",
    "| **Performance** | ðŸš€ Faster (when reducing) | ðŸŒ Slower (always shuffles) |\n",
    "| **Use case** | Reducing partitions | Changing partition count (up or down) |\n",
    "| **Data movement** | Minimal (local combination) | Full redistribution |\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "**Scenario: Reducing from 100 partitions to 32**\n",
    "\n",
    "**Using `coalesce(32)`:**\n",
    "```\n",
    "Executor 1: [P0] [P1] [P2] [P3] â†’ Combine locally â†’ [P0+P1+P2+P3]\n",
    "Executor 2: [P4] [P5] [P6] [P7] â†’ Combine locally â†’ [P4+P5+P6+P7]\n",
    "...\n",
    "Result: Minimal network I/O, fast operation\n",
    "```\n",
    "\n",
    "**Using `repartition(32)`:**\n",
    "```\n",
    "All partitions â†’ Full shuffle across network â†’ Redistribute to 32 partitions\n",
    "Result: Full network I/O, slower operation\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> **`coalesce()` is optimized for reducing partitions with minimal data movement. `repartition()` always performs a full shuffle, redistributing all data across the cluster.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af2450",
   "metadata": {},
   "source": [
    "## When Should You Use Coalesce?\n",
    "\n",
    "### Use Case 1: Reducing Too Many Partitions\n",
    "\n",
    "**Scenario:**\n",
    "- You have 1000 partitions\n",
    "- Your cluster has 16 cores\n",
    "- You want to reduce to 32 partitions (2Ã— cores)\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "df = df.coalesce(32)  # âœ… Efficient - no full shuffle\n",
    "```\n",
    "\n",
    "**Why coalesce?**\n",
    "- You're reducing partitions (coalesce's strength)\n",
    "- Avoids expensive full shuffle\n",
    "- Much faster than `repartition(32)`\n",
    "\n",
    "### Use Case 2: After Filtering Large Datasets\n",
    "\n",
    "**Scenario:**\n",
    "- You read 100 GB of data â†’ 200 partitions\n",
    "- You filter and now have only 10 GB of data\n",
    "- But you still have 200 partitions (many are now empty or tiny)\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "df_filtered = df.filter(df.amount > 1000)\n",
    "df_optimized = df_filtered.coalesce(32)  # âœ… Reduce partitions after filtering\n",
    "```\n",
    "\n",
    "**Why coalesce?**\n",
    "- After filtering, many partitions may be small or empty\n",
    "- Reducing partitions improves efficiency\n",
    "- Coalesce is perfect for this (no need to shuffle)\n",
    "\n",
    "### Use Case 3: Before Writing to Storage\n",
    "\n",
    "**Scenario:**\n",
    "- You have 500 partitions after processing\n",
    "- You want to write to storage with fewer files (e.g., 50 files)\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "df_final = df.coalesce(50)  # âœ… Reduce before writing\n",
    "df_final.write.parquet(\"output_path/\")\n",
    "```\n",
    "\n",
    "**Why coalesce?**\n",
    "- Writing fewer, larger files is often more efficient\n",
    "- Avoids creating too many small files\n",
    "- Coalesce is faster than repartition for this\n",
    "\n",
    "### Use Case 4: After Wide Transformations\n",
    "\n",
    "**Scenario:**\n",
    "- After a join or groupBy, you have 200 partitions\n",
    "- You want to reduce to 32 for subsequent operations\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "df_joined = df1.join(df2, on=\"key\")\n",
    "df_optimized = df_joined.coalesce(32)  # âœ… Reduce after wide transformation\n",
    "```\n",
    "\n",
    "**Why coalesce?**\n",
    "- Wide transformations often create many partitions\n",
    "- Reducing partitions can improve subsequent operations\n",
    "- Coalesce avoids another expensive shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f2641",
   "metadata": {},
   "source": [
    "## When NOT to Use Coalesce\n",
    "\n",
    "### âŒ Don't Use Coalesce When You Need to Increase Partitions\n",
    "\n",
    "**Scenario:**\n",
    "- You have 4 partitions\n",
    "- You need 32 partitions\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "df = df.coalesce(32)  # âŒ This won't work! Coalesce can't increase partitions\n",
    "# Result: Still 4 partitions (coalesce ignores requests to increase)\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "df = df.repartition(32)  # âœ… Use repartition to increase\n",
    "```\n",
    "\n",
    "**Why?**\n",
    "- Coalesce can only reduce partitions\n",
    "- If you request more partitions than you have, coalesce does nothing\n",
    "- Use repartition when you need to increase\n",
    "\n",
    "### âŒ Don't Use Coalesce When You Need Even Data Distribution\n",
    "\n",
    "**Scenario:**\n",
    "- You have 100 partitions with uneven data distribution (skewed)\n",
    "- You want 32 evenly distributed partitions\n",
    "\n",
    "**Coalesce:**\n",
    "```python\n",
    "df = df.coalesce(32)  # âš ï¸ May preserve skew\n",
    "```\n",
    "\n",
    "**Repartition:**\n",
    "```python\n",
    "df = df.repartition(32)  # âœ… Redistributes evenly (full shuffle)\n",
    "```\n",
    "\n",
    "**Why?**\n",
    "- Coalesce combines partitions locally, which may preserve existing skew\n",
    "- Repartition performs a full shuffle, which redistributes data evenly\n",
    "- If data skew is a concern, repartition is better\n",
    "\n",
    "### âš ï¸ Coalesce Limitation: May Not Achieve Exact Count\n",
    "\n",
    "**Important Note:**\n",
    "- Coalesce tries to reduce to the specified number\n",
    "- But it may not always achieve exactly that number if partitions are distributed across many executors\n",
    "- It's a \"best effort\" operation\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "df = df.coalesce(32)  # Request 32 partitions\n",
    "actual_partitions = df.rdd.getNumPartitions()  # Might be 30, 31, 32, or 33\n",
    "```\n",
    "\n",
    "**Why?**\n",
    "- Coalesce works by combining partitions on the same executor\n",
    "- If partitions are spread across many executors, exact count may vary\n",
    "- For exact count guarantee, use repartition (but it's more expensive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8a916",
   "metadata": {},
   "source": [
    "## Understanding Coalesce Internals: How It Works\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "**Step 1: Partition Analysis**\n",
    "- Spark analyzes current partition distribution across executors\n",
    "- Identifies which partitions are on the same executor\n",
    "\n",
    "**Step 2: Local Combination (Preferred)**\n",
    "- Partitions on the same executor are combined locally\n",
    "- No network transfer needed\n",
    "- Fast and efficient\n",
    "\n",
    "**Step 3: Cross-Executor Movement (When Necessary)**\n",
    "- If partitions need to be moved across executors to achieve target count\n",
    "- Network I/O occurs, but still less than full shuffle\n",
    "\n",
    "### Visual Example: Coalesce in Action\n",
    "\n",
    "**Initial State (100 partitions across 4 executors):**\n",
    "\n",
    "```\n",
    "Executor 1: [P0] [P1] [P2] [P3] ... [P24]  (25 partitions)\n",
    "Executor 2: [P25] [P26] [P27] ... [P49]    (25 partitions)\n",
    "Executor 3: [P50] [P51] [P52] ... [P74]    (25 partitions)\n",
    "Executor 4: [P75] [P76] [P77] ... [P99]    (25 partitions)\n",
    "```\n",
    "\n",
    "**After `coalesce(32)`:**\n",
    "```\n",
    "Executor 1: [P0+P1+...+P7] [P8+...+P15] [P16+...+P24]  (3 partitions)\n",
    "Executor 2: [P25+...+P32] [P33+...+P40] [P41+...+P49]  (3 partitions)\n",
    "Executor 3: [P50+...+P57] [P58+...+P65] [P66+...+P74]  (3 partitions)\n",
    "Executor 4: [P75+...+P82] [P83+...+P90] [P91+...+P99]  (3 partitions)\n",
    "\n",
    "Total: ~12 partitions (may need cross-executor movement to reach exactly 32)\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "- Most combinations happen locally (no network)\n",
    "- Only when target count requires it, cross-executor movement occurs\n",
    "- Much more efficient than repartition's full shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3438d7",
   "metadata": {},
   "source": [
    "## Practical Example: Demonstrating Coalesce\n",
    "\n",
    "Let's see coalesce in action with a practical example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20d63f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/02 22:26:06 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "26/01/02 22:26:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/02 22:26:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/02 22:26:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPARK SESSION INITIALIZED\n",
      "======================================================================\n",
      "Spark Version: 3.5.1\n",
      "Default Parallelism: 11\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CoalesceDemo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SPARK SESSION INITIALIZED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96d104",
   "metadata": {},
   "source": [
    "### Step 1: Create a DataFrame with Many Partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b8902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING DATAFRAME WITH MANY PARTITIONS\n",
      "======================================================================\n",
      "Original DataFrame partitions: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 11) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After repartition(100): 100\n",
      "Available cores: 11\n",
      "\n",
      "âš ï¸  Problem: 100 partitions for 11 cores\n",
      "   This creates overhead - too many small partitions!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with many partitions\n",
    "# This simulates having too many partitions (common after joins or wide transformations)\n",
    "\n",
    "data = [(i, f\"Product_{i % 100}\", 100.0 + i) for i in range(10000)]\n",
    "columns = [\"id\", \"product_name\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Repartition to create many partitions (simulating a scenario where we have too many)\n",
    "df_many_partitions = df.repartition(100)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING DATAFRAME WITH MANY PARTITIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original DataFrame partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"After repartition(100): {df_many_partitions.rdd.getNumPartitions()}\")\n",
    "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\nâš ï¸  Problem: {df_many_partitions.rdd.getNumPartitions()} partitions for {spark.sparkContext.defaultParallelism} cores\")\n",
    "print(f\"   This creates overhead - too many small partitions!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b6cdc",
   "metadata": {},
   "source": [
    "### Step 2: Using Coalesce to Reduce Partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9479bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "USING COALESCE TO REDUCE PARTITIONS\n",
      "======================================================================\n",
      "Target partitions: 22 (2Ã— 11 cores)\n",
      "\n",
      "Applying coalesce(22)...\n",
      "\n",
      "âœ… Coalesce Complete!\n",
      "   â€¢ Original partitions: 100\n",
      "   â€¢ After coalesce: 22\n",
      "   â€¢ Time taken: 0.009 seconds\n",
      "   â€¢ Note: Coalesce is fast because it avoids full shuffle\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate optimal partition count (2Ã— cores)\n",
    "optimal_partitions = spark.sparkContext.defaultParallelism * 2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"USING COALESCE TO REDUCE PARTITIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Target partitions: {optimal_partitions} (2Ã— {spark.sparkContext.defaultParallelism} cores)\")\n",
    "\n",
    "# Use coalesce to reduce partitions\n",
    "print(f\"\\nApplying coalesce({optimal_partitions})...\")\n",
    "start_time = time.time()\n",
    "df_coalesced = df_many_partitions.coalesce(optimal_partitions)\n",
    "coalesce_time = time.time() - start_time\n",
    "\n",
    "# Trigger action to see actual execution\n",
    "coalesced_count = df_coalesced.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"\\nâœ… Coalesce Complete!\")\n",
    "print(f\"   â€¢ Original partitions: {df_many_partitions.rdd.getNumPartitions()}\")\n",
    "print(f\"   â€¢ After coalesce: {coalesced_count}\")\n",
    "print(f\"   â€¢ Time taken: {coalesce_time:.3f} seconds\")\n",
    "print(f\"   â€¢ Note: Coalesce is fast because it avoids full shuffle\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ec2e7",
   "metadata": {},
   "source": [
    "### Step 3: Comparing Coalesce vs Repartition Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8712518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PERFORMANCE COMPARISON: COALESCE vs REPARTITION\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£  Testing coalesce (reducing partitions)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ Partitions: 100 â†’ 22\n",
      "   â€¢ Time: 0.676 seconds\n",
      "   â€¢ Operation: Minimal shuffle (local combination)\n",
      "\n",
      "2ï¸âƒ£  Testing repartition (reducing partitions)...\n",
      "   â€¢ Partitions: 100 â†’ 22\n",
      "   â€¢ Time: 0.192 seconds\n",
      "   â€¢ Operation: Full shuffle (redistributes all data)\n",
      "\n",
      "======================================================================\n",
      "COMPARISON RESULTS\n",
      "======================================================================\n",
      "ðŸš€ Coalesce is 0.28Ã— faster than repartition!\n",
      "   â€¢ Coalesce: 0.676s (minimal data movement)\n",
      "   â€¢ Repartition: 0.192s (full shuffle)\n",
      "\n",
      "ðŸ’¡ Key Insight: When reducing partitions, coalesce is much more efficient!\n",
      "   Coalesce avoids the expensive full shuffle that repartition performs.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare performance: coalesce vs repartition\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON: COALESCE vs REPARTITION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test coalesce\n",
    "print(\"\\n1ï¸âƒ£  Testing coalesce (reducing partitions)...\")\n",
    "start = time.time()\n",
    "df_coalesce = df_many_partitions.coalesce(optimal_partitions)\n",
    "# Trigger action\n",
    "_ = df_coalesce.count()\n",
    "coalesce_time = time.time() - start\n",
    "coalesce_partitions = df_coalesce.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"   â€¢ Partitions: {df_many_partitions.rdd.getNumPartitions()} â†’ {coalesce_partitions}\")\n",
    "print(f\"   â€¢ Time: {coalesce_time:.3f} seconds\")\n",
    "print(f\"   â€¢ Operation: Minimal shuffle (local combination)\")\n",
    "\n",
    "# Test repartition (for comparison)\n",
    "print(\"\\n2ï¸âƒ£  Testing repartition (reducing partitions)...\")\n",
    "start = time.time()\n",
    "df_repartition = df_many_partitions.repartition(optimal_partitions)\n",
    "# Trigger action\n",
    "_ = df_repartition.count()\n",
    "repartition_time = time.time() - start\n",
    "repartition_partitions = df_repartition.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"   â€¢ Partitions: {df_many_partitions.rdd.getNumPartitions()} â†’ {repartition_partitions}\")\n",
    "print(f\"   â€¢ Time: {repartition_time:.3f} seconds\")\n",
    "print(f\"   â€¢ Operation: Full shuffle (redistributes all data)\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "if repartition_time > 0:\n",
    "    speedup = repartition_time / coalesce_time\n",
    "    print(f\"ðŸš€ Coalesce is {speedup:.2f}Ã— faster than repartition!\")\n",
    "    print(f\"   â€¢ Coalesce: {coalesce_time:.3f}s (minimal data movement)\")\n",
    "    print(f\"   â€¢ Repartition: {repartition_time:.3f}s (full shuffle)\")\n",
    "    print(f\"\\nðŸ’¡ Key Insight: When reducing partitions, coalesce is much more efficient!\")\n",
    "    print(f\"   Coalesce avoids the expensive full shuffle that repartition performs.\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e943b41",
   "metadata": {},
   "source": [
    "### Step 4: Demonstrating Coalesce Limitation (Cannot Increase Partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ff42c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMONSTRATING COALESCE LIMITATION\n",
      "======================================================================\n",
      "\n",
      "Starting with 4 partitions\n",
      "\n",
      "âš ï¸  Attempting to use coalesce(32) to INCREASE partitions...\n",
      "   â€¢ Requested: 32 partitions\n",
      "   â€¢ Actual: 4 partitions\n",
      "   â€¢ Result: Coalesce did NOT increase partitions!\n",
      "\n",
      "âœ… Using repartition(32) to INCREASE partitions...\n",
      "   â€¢ Requested: 32 partitions\n",
      "   â€¢ Actual: 32 partitions\n",
      "   â€¢ Result: Repartition successfully increased partitions!\n",
      "\n",
      "======================================================================\n",
      "KEY TAKEAWAY\n",
      "======================================================================\n",
      "âŒ Coalesce CANNOT increase partitions\n",
      "âœ… Use repartition() when you need to increase partition count\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate that coalesce cannot increase partitions\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATING COALESCE LIMITATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a DataFrame with few partitions\n",
    "df_few = df.repartition(4)\n",
    "print(f\"\\nStarting with {df_few.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Try to use coalesce to increase partitions (this won't work)\n",
    "print(f\"\\nâš ï¸  Attempting to use coalesce(32) to INCREASE partitions...\")\n",
    "df_coalesce_attempt = df_few.coalesce(32)\n",
    "actual_partitions = df_coalesce_attempt.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"   â€¢ Requested: 32 partitions\")\n",
    "print(f\"   â€¢ Actual: {actual_partitions} partitions\")\n",
    "print(f\"   â€¢ Result: Coalesce did NOT increase partitions!\")\n",
    "\n",
    "# Use repartition to increase (this works)\n",
    "print(f\"\\nâœ… Using repartition(32) to INCREASE partitions...\")\n",
    "df_repartition_increase = df_few.repartition(32)\n",
    "actual_partitions_rep = df_repartition_increase.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"   â€¢ Requested: 32 partitions\")\n",
    "print(f\"   â€¢ Actual: {actual_partitions_rep} partitions\")\n",
    "print(f\"   â€¢ Result: Repartition successfully increased partitions!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âŒ Coalesce CANNOT increase partitions\")\n",
    "print(\"âœ… Use repartition() when you need to increase partition count\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a3e7a",
   "metadata": {},
   "source": [
    "## Real-World Use Cases\n",
    "\n",
    "### Use Case 1: After Filtering Large Datasets\n",
    "\n",
    "**Scenario:** You filter a large dataset and end up with many small partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86146d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "USE CASE 1: After Filtering Large Datasets\n",
      "======================================================================\n",
      "Large dataset: 100 partitions\n",
      "After filtering: 100 partitions (same count)\n",
      "After coalesce: 22 partitions\n",
      "\n",
      "ðŸ’¡ Insight: After filtering, many partitions may be small.\n",
      "   Coalescing reduces overhead while maintaining efficiency.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Use Case 1: After filtering large datasets\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"USE CASE 1: After Filtering Large Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate reading large dataset with many partitions\n",
    "df_large = spark.range(0, 100000).repartition(100)\n",
    "print(f\"Large dataset: {df_large.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Filter the data (many partitions may become small or empty)\n",
    "df_filtered = df_large.filter(df_large.id > 50000)\n",
    "print(f\"After filtering: {df_filtered.rdd.getNumPartitions()} partitions (same count)\")\n",
    "\n",
    "# Use coalesce to optimize\n",
    "optimal = spark.sparkContext.defaultParallelism * 2\n",
    "df_optimized = df_filtered.coalesce(optimal)\n",
    "print(f\"After coalesce: {df_optimized.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Insight: After filtering, many partitions may be small.\")\n",
    "print(f\"   Coalescing reduces overhead while maintaining efficiency.\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877be1e",
   "metadata": {},
   "source": [
    "### Use Case 2: Before Writing to Storage\n",
    "\n",
    "**Scenario:** You want to write fewer, larger files to storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca82ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "USE CASE 2: Before Writing to Storage\n",
      "======================================================================\n",
      "Processed data: 200 partitions\n",
      "Before writing: 50 partitions\n",
      "\n",
      "ðŸ’¡ Insight: Writing fewer, larger files is often more efficient.\n",
      "   Coalesce reduces the number of output files without expensive shuffle.\n",
      "   This is especially important for cloud storage (fewer API calls).\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Use Case 2: Before writing to storage\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"USE CASE 2: Before Writing to Storage\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate processed data with many partitions\n",
    "df_processed = spark.range(0, 50000).repartition(200)\n",
    "print(f\"Processed data: {df_processed.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Before writing, coalesce to reduce number of output files\n",
    "target_files = 50  # Want 50 output files instead of 200\n",
    "df_for_write = df_processed.coalesce(target_files)\n",
    "print(f\"Before writing: {df_for_write.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Insight: Writing fewer, larger files is often more efficient.\")\n",
    "print(f\"   Coalesce reduces the number of output files without expensive shuffle.\")\n",
    "print(f\"   This is especially important for cloud storage (fewer API calls).\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b65923",
   "metadata": {},
   "source": [
    "## Decision Tree: When to Use Coalesce vs Repartition\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "```\n",
    "Do you need to change partition count?\n",
    "â”‚\n",
    "â”œâ”€ YES, I need to INCREASE partitions\n",
    "â”‚  â””â”€> Use repartition() âœ…\n",
    "â”‚\n",
    "â””â”€ YES, I need to DECREASE partitions\n",
    "   â”‚\n",
    "   â”œâ”€ Do you need even data distribution?\n",
    "   â”‚  â”‚\n",
    "   â”‚  â”œâ”€ YES (data is skewed)\n",
    "   â”‚  â”‚  â””â”€> Use repartition() âœ… (full shuffle redistributes evenly)\n",
    "   â”‚  â”‚\n",
    "   â”‚  â””â”€ NO (data distribution is fine)\n",
    "   â”‚     â””â”€> Use coalesce() âœ… (faster, minimal shuffle)\n",
    "   â”‚\n",
    "   â””â”€ NO, partition count is fine\n",
    "      â””â”€> Do nothing âœ…\n",
    "```\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Scenario | Operation | Reason |\n",
    "|----------|-----------|--------|\n",
    "| Increase partitions | `repartition()` | Coalesce can't increase |\n",
    "| Decrease partitions, no skew | `coalesce()` | Faster, minimal shuffle |\n",
    "| Decrease partitions, with skew | `repartition()` | Full shuffle redistributes evenly |\n",
    "| Before writing to storage | `coalesce()` | Reduce output files efficiently |\n",
    "| After filtering | `coalesce()` | Many small partitions â†’ fewer larger ones |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01adf38c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dee55f3",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### âœ… DO\n",
    "\n",
    "1. **Use coalesce when reducing partitions**\n",
    "   - It's faster and more efficient than repartition\n",
    "   - Avoids expensive full shuffle\n",
    "\n",
    "2. **Coalesce before writing to storage**\n",
    "   - Reduces number of output files\n",
    "   - More efficient for cloud storage (fewer API calls)\n",
    "   - Better for downstream consumers\n",
    "\n",
    "3. **Coalesce after filtering**\n",
    "   - Many partitions may become small after filtering\n",
    "   - Reducing partitions improves efficiency\n",
    "\n",
    "4. **Check partition count after coalesce**\n",
    "   - Coalesce may not achieve exact count\n",
    "   - Always verify: `df.rdd.getNumPartitions()`\n",
    "\n",
    "5. **Use coalesce after wide transformations**\n",
    "   - Joins and groupBy often create many partitions\n",
    "   - Coalesce can optimize for subsequent operations\n",
    "\n",
    "### âŒ DON'T\n",
    "\n",
    "1. **Don't use coalesce to increase partitions**\n",
    "   - It won't work - use repartition instead\n",
    "\n",
    "2. **Don't use coalesce when you need even distribution**\n",
    "   - If data is skewed, repartition is better\n",
    "   - Coalesce may preserve existing skew\n",
    "\n",
    "3. **Don't coalesce too aggressively**\n",
    "   - Don't reduce to fewer partitions than cores\n",
    "   - Maintain at least `cores` partitions (ideally 2-4Ã—)\n",
    "\n",
    "4. **Don't assume coalesce achieves exact count**\n",
    "   - Always verify the actual partition count\n",
    "   - Coalesce is \"best effort\"\n",
    "\n",
    "5. **Don't coalesce unnecessarily**\n",
    "   - If partition count is already optimal, don't coalesce\n",
    "   - Measure first, optimize second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3aa4c",
   "metadata": {},
   "source": [
    "## Common Mistakes and How to Avoid Them\n",
    "\n",
    "### Mistake 1: Using Coalesce to Increase Partitions\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "df = df.coalesce(32)  # âŒ If df has 4 partitions, this does nothing!\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "df = df.repartition(32)  # âœ… Use repartition to increase\n",
    "```\n",
    "\n",
    "### Mistake 2: Not Checking Actual Partition Count\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "df = df.coalesce(32)\n",
    "# Assume it has 32 partitions (might not!)\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "df = df.coalesce(32)\n",
    "actual = df.rdd.getNumPartitions()  # âœ… Always verify\n",
    "print(f\"Actual partitions: {actual}\")\n",
    "```\n",
    "\n",
    "### Mistake 3: Using Coalesce on Skewed Data\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# Data is skewed, but using coalesce preserves skew\n",
    "df = df.coalesce(32)  # âš ï¸ May not help with skew\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# Use repartition to redistribute evenly\n",
    "df = df.repartition(32)  # âœ… Full shuffle redistributes evenly\n",
    "```\n",
    "\n",
    "### Mistake 4: Coalescing Below Core Count\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "cores = spark.sparkContext.defaultParallelism  # 16 cores\n",
    "df = df.coalesce(8)  # âŒ Only 8 partitions for 16 cores = waste!\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "cores = spark.sparkContext.defaultParallelism  # 16 cores\n",
    "df = df.coalesce(32)  # âœ… 2Ã— cores for optimal performance\n",
    "```\n",
    "\n",
    "### Mistake 5: Not Understanding the Performance Difference\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# Always using repartition (slower when reducing)\n",
    "df = df.repartition(32)  # âš ï¸ Unnecessary full shuffle\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# Use coalesce when reducing (faster)\n",
    "df = df.coalesce(32)  # âœ… Minimal shuffle, much faster\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044138e7",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Core Concept\n",
    "\n",
    "**Coalesce:**\n",
    "- âœ… Reduces partitions efficiently\n",
    "- âœ… Minimal data movement (local combination)\n",
    "- âœ… Faster than repartition when reducing\n",
    "- âŒ Cannot increase partitions\n",
    "- âš ï¸ May preserve data skew\n",
    "\n",
    "**Repartition:**\n",
    "- âœ… Can increase or decrease partitions\n",
    "- âœ… Full shuffle (redistributes evenly)\n",
    "- âœ… Better for handling data skew\n",
    "- âŒ More expensive (full shuffle always)\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "> **When reducing partitions, use `coalesce()`. When increasing partitions or needing even distribution, use `repartition()`.**\n",
    "\n",
    "### Remember\n",
    "\n",
    "1. **Coalesce = Combine locally, minimal shuffle**\n",
    "2. **Repartition = Full shuffle, redistributes all data**\n",
    "3. **Always verify partition count after coalesce**\n",
    "4. **Coalesce is faster when reducing, but repartition is more predictable**\n",
    "5. **Use coalesce before writing to reduce output files**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Practice using coalesce in your own Spark jobs\n",
    "- Monitor Spark UI to see the difference in data movement\n",
    "- Experiment with coalesce vs repartition on your data\n",
    "- Review `08_b_Partitions_Concepts.ipynb` to understand partition optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d6bd3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **What coalesce is**\n",
    "   - Operation to reduce partitions without full shuffle\n",
    "   - Combines partitions locally when possible\n",
    "   - More efficient than repartition when reducing\n",
    "\n",
    "2. **When to use coalesce**\n",
    "   - Reducing too many partitions\n",
    "   - After filtering large datasets\n",
    "   - Before writing to storage\n",
    "   - After wide transformations\n",
    "\n",
    "3. **How coalesce differs from repartition**\n",
    "   - Coalesce: Minimal shuffle, local combination\n",
    "   - Repartition: Full shuffle, redistributes all data\n",
    "   - Coalesce: Can only reduce\n",
    "   - Repartition: Can increase or decrease\n",
    "\n",
    "4. **Best practices**\n",
    "   - Use coalesce when reducing partitions\n",
    "   - Use repartition when increasing or needing even distribution\n",
    "   - Always verify partition count\n",
    "   - Don't coalesce below core count\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "> **Coalesce is your go-to tool for efficiently reducing partitions. It's faster than repartition because it avoids the expensive full shuffle. But remember: it can only reduce, not increase, and may preserve data skew.**\n",
    "\n",
    "---\n",
    "\n",
    "**Related Notebooks:**\n",
    "- `08_a_Spark_Architecture.ipynb` - Understanding executors, cores, and tasks\n",
    "- `08_b_Partitions_Concepts.ipynb` - Understanding partitions and optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f26c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
