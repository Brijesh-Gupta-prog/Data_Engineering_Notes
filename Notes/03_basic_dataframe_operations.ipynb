{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Basic DataFrame Operations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "DataFrame operations are the core of PySpark data processing. This module covers basic operations like selecting columns, filtering data, sorting, and basic transformations. These are the fundamental building blocks for working with PySpark DataFrames.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Filtering data (WHERE clause equivalent)\n",
    "- Selecting columns (SELECT clause equivalent)\n",
    "- Sorting data (ORDER BY equivalent)\n",
    "- Handling duplicates\n",
    "- Understanding different ways to access columns\n",
    "- Basic column operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 21:31:38 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/12/28 21:31:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/28 21:31:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 21:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/28 21:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/28 21:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Grace| 29|      NULL| 58000|  London|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, when, sum, avg, count, max, min\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Transformations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"New York\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"London\"),\n",
    "    (\"Charlie\", 35, \"Sales\", 70000, \"Tokyo\"),\n",
    "    (\"Diana\", 28, \"IT\", 55000, \"Paris\"),\n",
    "    (\"Eve\", 32, \"HR\", 65000, \"Sydney\"),\n",
    "    (\"Frank\", 27, \"Sales\", 52000, \"New York\"),\n",
    "    (\"Grace\", 29, None, 58000, \"London\")  # Department is null\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "Filtering allows you to select rows based on conditions. Similar to SQL's WHERE clause.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees older than 28:\n",
      "+-------+---+----------+------+------+\n",
      "|   Name|Age|Department|Salary|  City|\n",
      "+-------+---+----------+------+------+\n",
      "|    Bob| 30|        IT| 60000|London|\n",
      "|Charlie| 35|     Sales| 70000| Tokyo|\n",
      "|    Eve| 32|        HR| 65000|Sydney|\n",
      "|  Grace| 29|      NULL| 58000|London|\n",
      "+-------+---+----------+------+------+\n",
      "\n",
      "\n",
      "Same result using col():\n",
      "+-------+---+----------+------+------+\n",
      "|   Name|Age|Department|Salary|  City|\n",
      "+-------+---+----------+------+------+\n",
      "|    Bob| 30|        IT| 60000|London|\n",
      "|Charlie| 35|     Sales| 70000| Tokyo|\n",
      "|    Eve| 32|        HR| 65000|Sydney|\n",
      "|  Grace| 29|      NULL| 58000|London|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using column expression\n",
    "df_filtered = df.filter(df.Age > 28)\n",
    "print(\"Employees older than 28:\")\n",
    "df_filtered.show()\n",
    "\n",
    "# Alternative syntax using col()\n",
    "df_filtered2 = df.filter(col(\"Age\") > 28)\n",
    "print(\"\\nSame result using col():\")\n",
    "df_filtered2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age > 28 AND Salary > 60000:\n",
      "+-------+---+----------+------+------+\n",
      "|   Name|Age|Department|Salary|  City|\n",
      "+-------+---+----------+------+------+\n",
      "|Charlie| 35|     Sales| 70000| Tokyo|\n",
      "|    Eve| 32|        HR| 65000|Sydney|\n",
      "+-------+---+----------+------+------+\n",
      "\n",
      "\n",
      "Department is Sales OR IT:\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple conditions using & (and) and | (or)\n",
    "df_complex_filter = df.filter((df.Age > 28) & (df.Salary > 60000))\n",
    "print(\"Age > 28 AND Salary > 60000:\")\n",
    "df_complex_filter.show()\n",
    "\n",
    "# Using OR\n",
    "df_or_filter = df.filter((df.Department == \"Sales\") | (df.Department == \"IT\"))\n",
    "print(\"\\nDepartment is Sales OR IT:\")\n",
    "df_or_filter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees in New York:\n",
      "+-----+---+----------+------+--------+\n",
      "| Name|Age|Department|Salary|    City|\n",
      "+-----+---+----------+------+--------+\n",
      "|Alice| 25|     Sales| 50000|New York|\n",
      "|Frank| 27|     Sales| 52000|New York|\n",
      "+-----+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using where() - same as filter()\n",
    "df_where = df.where(df.City == \"New York\")\n",
    "print(\"Employees in New York:\")\n",
    "df_where.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Columns\n",
    "\n",
    "Select specific columns from a DataFrame. Similar to SQL's SELECT clause.\n",
    "\n",
    "### Accessing Columns in PySpark\n",
    "\n",
    "PySpark provides multiple ways to access columns in a DataFrame. Understanding these methods is crucial for writing effective PySpark code.\n",
    "\n",
    "#### 1. String Notation\n",
    "\n",
    "The simplest way to reference a column is by using a string with the column name.\n",
    "\n",
    "```python\n",
    "df.select(\"Name\", \"Age\", \"Salary\")\n",
    "df.filter(\"Age > 30\")\n",
    "```\n",
    "\n",
    "**When to use**: Simple column references in `select()`, `filter()`, `groupBy()`, etc.\n",
    "\n",
    "#### 2. Prefixing Column Name with DataFrame\n",
    "\n",
    "You can access columns using dot notation by prefixing the column name with the DataFrame name.\n",
    "\n",
    "```python\n",
    "df.select(df.Name, df.Age)\n",
    "df.filter(df.Age > 30)\n",
    "df.filter(df.Department == \"IT\")\n",
    "```\n",
    "\n",
    "**When to use**: When you need to reference columns in a more object-oriented style.\n",
    "\n",
    "#### 3. Array Notation\n",
    "\n",
    "You can access columns using bracket notation (similar to dictionary access).\n",
    "\n",
    "```python\n",
    "df.select(df[\"Name\"], df[\"Age\"])\n",
    "df.filter(df[\"Age\"] > 30)\n",
    "```\n",
    "\n",
    "**When to use**: Alternative syntax that works similarly to dot notation.\n",
    "\n",
    "#### 4. Column Object Notation\n",
    "\n",
    "Using the `col()` function from `pyspark.sql.functions` to create a Column object.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"Name\"), col(\"Age\"))\n",
    "df.filter(col(\"Age\") > 30)\n",
    "```\n",
    "\n",
    "**When to use**: When you need to use column methods and functions programmatically.\n",
    "\n",
    "#### 5. Column Expression\n",
    "\n",
    "Using `expr()` function to write SQL-like expressions as strings.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.select(expr(\"Name\"), expr(\"Age\"))\n",
    "df.select(expr(\"Salary * 1.1 as NewSalary\"))\n",
    "df.filter(expr(\"Age > 30\"))\n",
    "```\n",
    "\n",
    "**When to use**: When you need to write complex SQL-like expressions or when working with calculated columns.\n",
    "\n",
    "### Why Are There So Many Ways of Accessing Columns?\n",
    "\n",
    "Each method serves different purposes and has specific use cases:\n",
    "\n",
    "#### Prefixing Column Name with DataFrame\n",
    "\n",
    "**Purpose**: Resolve ambiguity when multiple DataFrames have columns with the same name.\n",
    "\n",
    "**Example**: If two different DataFrames have columns with the same name, prefixing helps the system identify which DataFrame's column to use.\n",
    "\n",
    "```python\n",
    "# If both orders_df and customer_df have a 'cust_id' column\n",
    "# Without prefixing - AMBIGUOUS:\n",
    "# df.select(\"cust_id\")  # Which DataFrame's cust_id?\n",
    "\n",
    "# With prefixing - CLEAR:\n",
    "orders_df.select(orders_df.cust_id)\n",
    "customer_df.select(customer_df.cust_id)\n",
    "```\n",
    "\n",
    "**When to use**: When joining DataFrames with common column names, or when working with multiple DataFrames in the same context.\n",
    "\n",
    "#### Column Expression\n",
    "\n",
    "**Purpose**: Required when evaluation needs to be performed in a SQL way.\n",
    "\n",
    "**Example**: When you need to perform calculations or transformations that are easier to express in SQL syntax.\n",
    "\n",
    "```python\n",
    "# Increment customer ID and create a new customer ID\n",
    "df.select(expr(\"cust_id + 1 as new_cust_id\"))\n",
    "```\n",
    "\n",
    "**When to use**: \n",
    "- Complex calculations that are easier to write in SQL syntax\n",
    "- When you want to leverage SQL's expression capabilities\n",
    "- Creating aliased columns with calculations\n",
    "\n",
    "#### Column Object\n",
    "\n",
    "**Purpose**: Provides various predefined functions to achieve desired results in a programmatic approach.\n",
    "\n",
    "**Example**: Using column methods for filtering and transformations.\n",
    "\n",
    "```python\n",
    "# Using Column Object methods\n",
    "orders_df.select(\"*\").where(col('order_status').like('PENDING%')).show()\n",
    "\n",
    "# Equivalent using string expression\n",
    "orders_df.select(\"*\").where(\"order_status like 'PENDING%'\").show()\n",
    "```\n",
    "\n",
    "**When to use**:\n",
    "- When you need to use column methods (`.like()`, `.isNull()`, `.isNotNull()`, etc.)\n",
    "- Programmatic column manipulation\n",
    "- Type-safe column operations\n",
    "- Chaining column transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `select()` vs `selectExpr()`\n",
    "\n",
    "Both `select()` and `selectExpr()` are used to select columns, but they handle expressions differently:\n",
    "\n",
    "**Key Difference:**\n",
    "\n",
    "- **`select()`**: You must **explicitly segregate** column names and expressions. For expressions, you need to use `expr()` or column operations.\n",
    "- **`selectExpr()`**: **Automatically identifies** whether the value passed is a column name or an expression and handles it accordingly. You can write SQL-like expressions directly as strings.\n",
    "\n",
    "**When to Use:**\n",
    "- Use `select()` when you want explicit control and type safety\n",
    "- Use `selectExpr()` when you prefer SQL-like syntax and want Spark to automatically parse expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Using select() - Must explicitly use expr() for expressions\n",
      "============================================================\n",
      "Using select() with expr():\n",
      "+-------+---+---------+\n",
      "|   Name|Age|NewSalary|\n",
      "+-------+---+---------+\n",
      "|  Alice| 25|  55000.0|\n",
      "|    Bob| 30|  66000.0|\n",
      "|Charlie| 35|  77000.0|\n",
      "|  Diana| 28|  60500.0|\n",
      "|    Eve| 32|  71500.0|\n",
      "|  Frank| 27|  57200.0|\n",
      "|  Grace| 29|  63800.0|\n",
      "+-------+---+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Using selectExpr() - Automatically handles expressions\n",
      "============================================================\n",
      "Using selectExpr() - no need for expr():\n",
      "+-------+---+---------+\n",
      "|   Name|Age|NewSalary|\n",
      "+-------+---+---------+\n",
      "|  Alice| 25|  55000.0|\n",
      "|    Bob| 30|  66000.0|\n",
      "|Charlie| 35|  77000.0|\n",
      "|  Diana| 28|  60500.0|\n",
      "|    Eve| 32|  71500.0|\n",
      "|  Frank| 27|  57200.0|\n",
      "|  Grace| 29|  63800.0|\n",
      "+-------+---+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Key Takeaway:\n",
      "============================================================\n",
      "select(): Must explicitly use expr() for SQL expressions\n",
      "selectExpr(): Automatically parses SQL expressions from strings\n",
      "\n",
      "Both produce the same result, but selectExpr() is more convenient\n",
      "for SQL-like expressions!\n"
     ]
    }
   ],
   "source": [
    "# Example: select() vs selectExpr()\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Using select() - Must explicitly use expr() for expressions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# With select(), you need to explicitly use expr() for SQL expressions\n",
    "df_select = df.select(\n",
    "    \"Name\",                    # Column name - direct\n",
    "    \"Age\",                     # Column name - direct\n",
    "    expr(\"Salary * 1.1 as NewSalary\")  # Expression - must use expr()\n",
    ")\n",
    "\n",
    "print(\"Using select() with expr():\")\n",
    "df_select.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Using selectExpr() - Automatically handles expressions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# With selectExpr(), you can write SQL-like expressions directly\n",
    "df_selectExpr = df.selectExpr(\n",
    "    \"Name\",                    # Column name - works\n",
    "    \"Age\",                     # Column name - works\n",
    "    \"Salary * 1.1 as NewSalary\"  # Expression - automatically parsed!\n",
    ")\n",
    "\n",
    "print(\"Using selectExpr() - no need for expr():\")\n",
    "df_selectExpr.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Takeaway:\")\n",
    "print(\"=\"*60)\n",
    "print(\"select(): Must explicitly use expr() for SQL expressions\")\n",
    "print(\"selectExpr(): Automatically parses SQL expressions from strings\")\n",
    "print(\"\\nBoth produce the same result, but selectExpr() is more convenient\")\n",
    "print(\"for SQL-like expressions!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicates in DataFrames\n",
    "\n",
    "Duplicate rows can occur in DataFrames for various reasons. PySpark provides two methods to handle duplicates:\n",
    "\n",
    "### 1. `distinct()` - Remove All Duplicate Rows\n",
    "\n",
    "**Usage**: `df.distinct()` or `df.dropDuplicates()`\n",
    "\n",
    "**Behavior**: Removes duplicate rows when **all columns** are considered. Two rows are considered duplicates only if all their column values are identical.\n",
    "\n",
    "**When to Use**: When you want to remove rows that are completely identical across all columns.\n",
    "\n",
    "### 2. `dropDuplicates()` - Remove Duplicates Based on Subset of Columns\n",
    "\n",
    "**Usage**: `df.dropDuplicates([column1, column2, ...])`\n",
    "\n",
    "**Behavior**: Removes duplicate rows when only a **subset of columns** are considered. You can specify which columns to use for duplicate detection.\n",
    "\n",
    "**When to Use**: When you want to remove duplicates based on specific columns (e.g., keep only one row per customer_id, even if other columns differ).\n",
    "\n",
    "**Key Difference:**\n",
    "- `distinct()`: Considers all columns\n",
    "- `dropDuplicates()`: Can consider a subset of columns (or all columns if no subset specified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with duplicates:\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|  Alice| 28|     Sales| 55000|  Boston|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 1: distinct() - Removes duplicates when ALL columns match\n",
      "============================================================\n",
      "After distinct() - only complete duplicates removed:\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Alice| 28|     Sales| 55000|  Boston|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 2: dropDuplicates() - Remove duplicates based on ALL columns\n",
      "============================================================\n",
      "After dropDuplicates() (all columns):\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Alice| 28|     Sales| 55000|  Boston|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 3: dropDuplicates([columns]) - Remove duplicates based on SUBSET\n",
      "============================================================\n",
      "After dropDuplicates(['Name']) - keeps one row per Name:\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Summary:\n",
      "============================================================\n",
      "1. df.distinct() → Removes duplicates when ALL columns match\n",
      "2. df.dropDuplicates() → Same as distinct() (all columns)\n",
      "3. df.dropDuplicates(['col1', 'col2']) → Removes duplicates based on specified columns\n",
      "\n",
      "Note: When using dropDuplicates() with subset, Spark keeps the FIRST occurrence\n",
      "and removes subsequent duplicates based on the specified columns.\n"
     ]
    }
   ],
   "source": [
    "# Example: Handling Duplicates\n",
    "\n",
    "# Create a DataFrame with duplicate rows\n",
    "data_with_duplicates = [\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"New York\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"London\"),\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"New York\"),  # Complete duplicate\n",
    "    (\"Charlie\", 35, \"Sales\", 70000, \"Tokyo\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"London\"),  # Complete duplicate\n",
    "    (\"Alice\", 28, \"Sales\", 55000, \"Boston\"),  # Same name, different other values\n",
    "]\n",
    "\n",
    "df_duplicates = spark.createDataFrame(data_with_duplicates, [\"Name\", \"Age\", \"Department\", \"Salary\", \"City\"])\n",
    "\n",
    "print(\"Original DataFrame with duplicates:\")\n",
    "df_duplicates.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Method 1: distinct() - Removes duplicates when ALL columns match\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# distinct() removes rows where ALL columns are identical\n",
    "df_distinct = df_duplicates.distinct()\n",
    "\n",
    "print(\"After distinct() - only complete duplicates removed:\")\n",
    "df_distinct.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Method 2: dropDuplicates() - Remove duplicates based on ALL columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# dropDuplicates() without arguments works like distinct()\n",
    "df_drop_all = df_duplicates.dropDuplicates()\n",
    "\n",
    "print(\"After dropDuplicates() (all columns):\")\n",
    "df_drop_all.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Method 3: dropDuplicates([columns]) - Remove duplicates based on SUBSET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# dropDuplicates() with column subset - removes duplicates based on Name only\n",
    "# Keeps the first occurrence when multiple rows have the same Name\n",
    "df_drop_subset = df_duplicates.dropDuplicates([\"Name\"])\n",
    "\n",
    "print(\"After dropDuplicates(['Name']) - keeps one row per Name:\")\n",
    "df_drop_subset.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. df.distinct() → Removes duplicates when ALL columns match\")\n",
    "print(\"2. df.dropDuplicates() → Same as distinct() (all columns)\")\n",
    "print(\"3. df.dropDuplicates(['col1', 'col2']) → Removes duplicates based on specified columns\")\n",
    "print(\"\\nNote: When using dropDuplicates() with subset, Spark keeps the FIRST occurrence\")\n",
    "print(\"and removes subsequent duplicates based on the specified columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns:\n",
      "+-------+---+------+\n",
      "|   Name|Age|Salary|\n",
      "+-------+---+------+\n",
      "|  Alice| 25| 50000|\n",
      "|    Bob| 30| 60000|\n",
      "|Charlie| 35| 70000|\n",
      "|  Diana| 28| 55000|\n",
      "|    Eve| 32| 65000|\n",
      "|  Frank| 27| 52000|\n",
      "|  Grace| 29| 58000|\n",
      "+-------+---+------+\n",
      "\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "|  Diana| 28|\n",
      "|    Eve| 32|\n",
      "|  Frank| 27|\n",
      "|  Grace| 29|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "df_selected = df.select(\"Name\", \"Age\", \"Salary\")\n",
    "print(\"Selected columns:\")\n",
    "df_selected.show()\n",
    "\n",
    "# Select using col()\n",
    "df_selected2 = df.select(col(\"Name\"), col(\"Age\"))\n",
    "df_selected2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select with calculated column:\n",
      "+-------+---+-----------------+\n",
      "|   Name|Age|        NewSalary|\n",
      "+-------+---+-----------------+\n",
      "|  Alice| 25|55000.00000000001|\n",
      "|    Bob| 30|          66000.0|\n",
      "|Charlie| 35|          77000.0|\n",
      "|  Diana| 28|60500.00000000001|\n",
      "|    Eve| 32|          71500.0|\n",
      "|  Frank| 27|57200.00000000001|\n",
      "|  Grace| 29|63800.00000000001|\n",
      "+-------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select with expressions\n",
    "df_with_expr = df.select(\"Name\", \"Age\", (col(\"Salary\") * 1.1).alias(\"NewSalary\"))\n",
    "print(\"Select with calculated column:\")\n",
    "df_with_expr.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Data\n",
    "\n",
    "Sort data by one or more columns. Similar to SQL's ORDER BY clause.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Salary (ascending):\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|  Grace| 29|      NULL| 58000|  London|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "Sorted by Salary (descending):\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|  Grace| 29|      NULL| 58000|  London|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by single column (ascending)\n",
    "df_sorted = df.orderBy(\"Salary\")\n",
    "print(\"Sorted by Salary (ascending):\")\n",
    "df_sorted.show()\n",
    "\n",
    "# Sort descending\n",
    "df_sorted_desc = df.orderBy(col(\"Salary\").desc())\n",
    "print(\"\\nSorted by Salary (descending):\")\n",
    "df_sorted_desc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Department, then Salary (descending):\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Grace| 29|      NULL| 58000|  London|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by multiple columns\n",
    "df_multi_sort = df.orderBy(\"Department\", col(\"Salary\").desc())\n",
    "print(\"Sorted by Department, then Salary (descending):\")\n",
    "df_multi_sort.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "1. **Filtering Data**: Using `filter()` and `where()` to select rows based on conditions (SQL WHERE equivalent)\n",
    "2. **Selecting Columns**: Using `select()` to choose specific columns (SQL SELECT equivalent)\n",
    "3. **Sorting Data**: Using `orderBy()` to sort data (SQL ORDER BY equivalent)\n",
    "4. **Handling Duplicates**: Using `distinct()` and `dropDuplicates()`\n",
    "5. **Column Access Methods**: Understanding different ways to reference columns (string, dot notation, col(), expr())\n",
    "\n",
    "**Key Takeaway**: These are the fundamental operations for working with PySpark DataFrames. They are lazy transformations that create a plan but don't execute until an action is called.\n",
    "\n",
    "**Next Steps**: In Module 4, we'll learn about data transformations including grouping, aggregations, adding/renaming columns, and handling null values.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
