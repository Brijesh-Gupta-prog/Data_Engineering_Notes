{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10393707",
   "metadata": {},
   "source": [
    "# Spark Architecture: Understanding Executors, Cores, and Tasks\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **What Spark architecture is** and why it matters\n",
    "2. **Executors** - what they are and their role in distributed computing\n",
    "3. **Cores** - how CPU cores enable parallelism\n",
    "4. **Tasks** - the unit of work in Spark\n",
    "5. **How these components work together** to process data in parallel\n",
    "6. **Basic configuration concepts** for optimizing Spark jobs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python\n",
    "- Familiarity with data processing concepts\n",
    "- No prior Spark experience required - this is a foundational module!\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This notebook provides the foundational understanding needed before diving into advanced topics like partitions, optimization, and performance tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab0706",
   "metadata": {},
   "source": [
    "## Introduction: Why Understanding Architecture Matters\n",
    "\n",
    "As a fresh graduate entering the world of big data, you'll often hear terms like:\n",
    "- \"We need more executors\"\n",
    "- \"The job is using only 25% of available cores\"\n",
    "- \"Tasks are not being distributed evenly\"\n",
    "\n",
    "**Understanding Spark architecture is crucial because:**\n",
    "- It helps you write efficient code\n",
    "- It enables you to debug performance issues\n",
    "- It allows you to optimize resource usage (which saves money!)\n",
    "- It makes you a better data engineer\n",
    "\n",
    "**Think of it this way:**\n",
    "- Without understanding architecture â†’ You're driving blindfolded\n",
    "- With understanding architecture â†’ You can see the road and make informed decisions\n",
    "\n",
    "Let's start with the big picture!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e1cdc",
   "metadata": {},
   "source": [
    "## The Big Picture: Spark Cluster Architecture\n",
    "\n",
    "### High-Level Overview\n",
    "\n",
    "When you run a Spark application, it runs on a **cluster** - a collection of machines working together. Here's the basic structure:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    SPARK CLUSTER                             â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
    "â”‚  â”‚   Driver Node    â”‚  â† Your code runs here               â”‚\n",
    "â”‚  â”‚  (Master/Client) â”‚     (coordinates everything)         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚\n",
    "â”‚           â”‚                                                  â”‚\n",
    "â”‚           â”‚ Coordinates                                      â”‚\n",
    "â”‚           â”‚                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚         Worker Nodes (Executors)              â”‚          â”‚\n",
    "â”‚  â”‚                                                â”‚          â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚Executor 1â”‚  â”‚Executor 2â”‚  â”‚Executor 3â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚ 4 Cores  â”‚  â”‚ 4 Cores  â”‚  â”‚ 4 Cores  â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â”‚\n",
    "â”‚  â”‚                                                â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Driver Node**: \n",
    "   - Runs your main program\n",
    "   - Coordinates the entire job\n",
    "   - Manages task scheduling\n",
    "\n",
    "2. **Worker Nodes**: \n",
    "   - Physical machines in the cluster\n",
    "   - Run executors (JVM processes)\n",
    "   - Do the actual data processing\n",
    "\n",
    "3. **Executors**: \n",
    "   - JVM processes running on worker nodes\n",
    "   - Execute tasks assigned by the driver\n",
    "   - Store data in memory/disk\n",
    "\n",
    "**Simple Analogy:**\n",
    "- **Driver** = Project manager (coordinates work)\n",
    "- **Worker Nodes** = Office buildings\n",
    "- **Executors** = Teams working in those buildings\n",
    "- **Cores** = Individual workers in each team\n",
    "- **Tasks** = Specific jobs assigned to workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62510230",
   "metadata": {},
   "source": [
    "## Component 1: Executors\n",
    "\n",
    "### What is an Executor?\n",
    "\n",
    "An **executor** is a JVM (Java Virtual Machine) process that runs on a worker node. Think of it as a \"worker process\" that does the actual data processing.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "**1. Process Isolation**\n",
    "- Each executor is a separate JVM process\n",
    "- If one executor crashes, others continue working\n",
    "- Provides fault tolerance\n",
    "\n",
    "**2. Memory Management**\n",
    "- Each executor has its own memory space\n",
    "- Memory is divided into:\n",
    "  - **Execution memory**: For computations (shuffles, joins, aggregations)\n",
    "  - **Storage memory**: For caching DataFrames\n",
    "\n",
    "**3. Task Execution**\n",
    "- Executors receive tasks from the driver\n",
    "- Execute tasks in parallel using multiple cores\n",
    "- Return results back to the driver\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "Worker Node (Physical Machine)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Operating System                    â”‚\n",
    "â”‚                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚  Executor (JVM Process)        â”‚ â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  Core 1  â”‚  â”‚  Core 2  â”‚   â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  [Task]  â”‚  â”‚  [Task]  â”‚   â”‚ â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  Core 3  â”‚  â”‚  Core 4  â”‚   â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  [Task]  â”‚  â”‚  [Task]  â”‚   â”‚ â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚\n",
    "â”‚  â”‚                                â”‚ â”‚\n",
    "â”‚  â”‚  Memory: 8 GB                  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Common Configuration\n",
    "\n",
    "```python\n",
    "# Typical executor configuration\n",
    "num_executors = 4          # 4 executor processes\n",
    "executor_cores = 4         # 4 CPU cores per executor\n",
    "executor_memory = \"8g\"     # 8 GB RAM per executor\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "- You have 4 separate JVM processes\n",
    "- Each process can use 4 CPU cores simultaneously\n",
    "- Each process has 8 GB of memory\n",
    "- Total: 16 cores and 32 GB RAM across all executors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2248c6",
   "metadata": {},
   "source": [
    "## Component 2: Cores\n",
    "\n",
    "### What is a Core?\n",
    "\n",
    "A **core** is a CPU core - a physical processing unit that can execute instructions. Modern CPUs have multiple cores, allowing parallel execution.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**1. Parallelism = Number of Cores**\n",
    "- Each core can execute one task at a time\n",
    "- More cores = more tasks can run simultaneously\n",
    "- This is your **parallelism capacity**\n",
    "\n",
    "**2. Core vs Thread**\n",
    "- **Core**: Physical hardware unit\n",
    "- **Thread**: Software execution unit\n",
    "- In Spark: 1 core = 1 thread = 1 task at a time\n",
    "\n",
    "**3. Total Parallelism**\n",
    "```\n",
    "Total Parallelism = Number of Executors Ã— Cores per Executor\n",
    "\n",
    "Example:\n",
    "- 4 executors Ã— 4 cores = 16 total cores\n",
    "- Maximum 16 tasks can run simultaneously\n",
    "```\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Executor with 4 Cores:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Executor                       â”‚\n",
    "â”‚                                 â”‚\n",
    "â”‚  Core 1: [Task A] â† Processing â”‚\n",
    "â”‚  Core 2: [Task B] â† Processing â”‚\n",
    "â”‚  Core 3: [Task C] â† Processing â”‚\n",
    "â”‚  Core 4: [Task D] â† Processing â”‚\n",
    "â”‚                                 â”‚\n",
    "â”‚  All 4 cores working in parallelâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Important Rule: One Task Per Core\n",
    "\n",
    "**Critical Understanding:**\n",
    "- âŒ One core cannot process multiple tasks simultaneously\n",
    "- âŒ Multiple cores cannot share one task\n",
    "- âœ… One core processes one task at a time\n",
    "- âœ… Multiple cores can process multiple tasks in parallel\n",
    "\n",
    "**Why this matters:**\n",
    "If you have 16 cores but only 4 tasks, you're wasting 12 cores (75% idle)!\n",
    "\n",
    "```\n",
    "16 Cores Available:\n",
    "[â—] [â—] [â—] [â—] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹] [â—‹]\n",
    " â†‘   â†‘   â†‘   â†‘\n",
    "Used Used Used Used\n",
    "(4 tasks running, 12 cores idle - BAD!)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111dc8a",
   "metadata": {},
   "source": [
    "## Component 3: Tasks\n",
    "\n",
    "### What is a Task?\n",
    "\n",
    "A **task** is the smallest unit of work in Spark. It represents processing one partition of data on one executor using one core.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "**1. Task = Partition Processing**\n",
    "- Each task processes exactly one partition\n",
    "- One partition = one task = one core\n",
    "- Tasks are independent and can run in parallel\n",
    "\n",
    "**2. Task Lifecycle**\n",
    "\n",
    "```\n",
    "Task Creation â†’ Task Scheduling â†’ Task Execution â†’ Result Return\n",
    "     â†“              â†“                  â†“              â†“\n",
    "  Driver        Driver assigns    Executor runs   Results sent\n",
    "  creates       to executor       on one core     back to driver\n",
    "```\n",
    "\n",
    "**3. Task Distribution**\n",
    "\n",
    "The driver schedules tasks across executors and cores:\n",
    "\n",
    "```\n",
    "Driver's Task Queue:\n",
    "[Task 1] [Task 2] [Task 3] [Task 4] [Task 5] [Task 6] ... [Task 16]\n",
    "\n",
    "Distribution:\n",
    "Executor 1 (4 cores): Task 1, Task 2, Task 3, Task 4\n",
    "Executor 2 (4 cores): Task 5, Task 6, Task 7, Task 8\n",
    "Executor 3 (4 cores): Task 9, Task 10, Task 11, Task 12\n",
    "Executor 4 (4 cores): Task 13, Task 14, Task 15, Task 16\n",
    "```\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Cluster with 4 Executors, 4 Cores Each (16 total cores):\n",
    "\n",
    "Executor 1:          Executor 2:          Executor 3:          Executor 4:\n",
    "[Task 1]            [Task 5]            [Task 9]            [Task 13]\n",
    "[Task 2]            [Task 6]            [Task 10]           [Task 14]\n",
    "[Task 3]            [Task 7]            [Task 11]           [Task 15]\n",
    "[Task 4]            [Task 8]            [Task 12]           [Task 16]\n",
    "\n",
    "All 16 tasks running in parallel = All 16 cores utilized = OPTIMAL!\n",
    "```\n",
    "\n",
    "### Task vs Partition\n",
    "\n",
    "**Important Distinction:**\n",
    "- **Partition**: A logical division of data\n",
    "- **Task**: The work unit that processes a partition\n",
    "\n",
    "**Relationship:**\n",
    "- 1 partition â†’ 1 task (in most cases)\n",
    "- If you have 100 partitions, you'll have 100 tasks\n",
    "- If you have 16 cores, you can process 16 tasks simultaneously\n",
    "- Remaining 84 tasks will wait in queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f60e2",
   "metadata": {},
   "source": [
    "## How They Work Together: A Complete Example\n",
    "\n",
    "### Scenario Setup\n",
    "\n",
    "Let's say you have:\n",
    "- **4 executors**\n",
    "- **4 cores per executor** (16 total cores)\n",
    "- **100 partitions** of data to process\n",
    "\n",
    "### Step-by-Step Execution\n",
    "\n",
    "**Step 1: Driver Creates Tasks**\n",
    "```\n",
    "100 partitions â†’ 100 tasks created\n",
    "```\n",
    "\n",
    "**Step 2: Driver Schedules Tasks**\n",
    "```\n",
    "First batch: 16 tasks (one per core)\n",
    "Remaining: 84 tasks wait in queue\n",
    "```\n",
    "\n",
    "**Step 3: Executors Execute Tasks**\n",
    "```\n",
    "Executor 1: [Task 1] [Task 2] [Task 3] [Task 4] â†’ 4 cores busy\n",
    "Executor 2: [Task 5] [Task 6] [Task 7] [Task 8] â†’ 4 cores busy\n",
    "Executor 3: [Task 9] [Task 10] [Task 11] [Task 12] â†’ 4 cores busy\n",
    "Executor 4: [Task 13] [Task 14] [Task 15] [Task 16] â†’ 4 cores busy\n",
    "\n",
    "Queue: [Task 17] ... [Task 100] (84 tasks waiting)\n",
    "```\n",
    "\n",
    "**Step 4: Tasks Complete, New Tasks Start**\n",
    "```\n",
    "When Task 1 finishes â†’ Task 17 starts immediately\n",
    "When Task 2 finishes â†’ Task 18 starts immediately\n",
    "... and so on until all 100 tasks complete\n",
    "```\n",
    "\n",
    "### Timeline Visualization\n",
    "\n",
    "```\n",
    "Time â†’\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Batch 1: Tasks 1-16 running (16 cores utilized)            â”‚\n",
    "â”‚ Batch 2: Tasks 17-32 running (16 cores utilized)            â”‚\n",
    "â”‚ Batch 3: Tasks 33-48 running (16 cores utilized)           â”‚\n",
    "â”‚ ...                                                          â”‚\n",
    "â”‚ Batch 7: Tasks 97-100 running (4 cores utilized, 12 idle)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "- With 100 partitions and 16 cores, you need ~7 batches\n",
    "- Each batch processes 16 tasks in parallel\n",
    "- Last batch has only 4 tasks (12 cores idle, but that's okay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92e1dd",
   "metadata": {},
   "source": [
    "## Common Scenarios and What They Mean\n",
    "\n",
    "### Scenario 1: Too Few Partitions (Underutilization)\n",
    "\n",
    "**Configuration:**\n",
    "- 4 executors Ã— 4 cores = 16 cores available\n",
    "- Data has only 4 partitions\n",
    "\n",
    "**What Happens:**\n",
    "```\n",
    "Executor 1: [Task 1] [â—‹] [â—‹] [â—‹]  â†’ 1 core used, 3 idle\n",
    "Executor 2: [Task 2] [â—‹] [â—‹] [â—‹]  â†’ 1 core used, 3 idle\n",
    "Executor 3: [Task 3] [â—‹] [â—‹] [â—‹]  â†’ 1 core used, 3 idle\n",
    "Executor 4: [Task 4] [â—‹] [â—‹] [â—‹]  â†’ 1 core used, 3 idle\n",
    "\n",
    "Utilization: 4/16 = 25% (75% waste!)\n",
    "```\n",
    "\n",
    "**Problem:** You're paying for 16 cores but only using 4!\n",
    "\n",
    "**Solution:** Increase partitions (e.g., use `repartition(16)` or more)\n",
    "\n",
    "### Scenario 2: Too Many Partitions (Overhead)\n",
    "\n",
    "**Configuration:**\n",
    "- 4 executors Ã— 4 cores = 16 cores available\n",
    "- Data has 10,000 partitions\n",
    "\n",
    "**What Happens:**\n",
    "```\n",
    "- 16 tasks run simultaneously (good!)\n",
    "- But 9,984 tasks wait in queue\n",
    "- Each task is very small (overhead dominates)\n",
    "- Task scheduling overhead becomes significant\n",
    "```\n",
    "\n",
    "**Problem:** Too much overhead, small tasks are inefficient\n",
    "\n",
    "**Solution:** Reduce partitions to a reasonable number (e.g., 2-4Ã— number of cores)\n",
    "\n",
    "### Scenario 3: Optimal Configuration\n",
    "\n",
    "**Configuration:**\n",
    "- 4 executors Ã— 4 cores = 16 cores available\n",
    "- Data has 32-64 partitions (2-4Ã— number of cores)\n",
    "\n",
    "**What Happens:**\n",
    "```\n",
    "- All 16 cores stay busy\n",
    "- Good load balancing\n",
    "- Reasonable task sizes\n",
    "- Efficient resource utilization\n",
    "```\n",
    "\n",
    "**Result:** Optimal performance! âœ…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c0cf9",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Golden Rules\n",
    "\n",
    "1. **One Task = One Partition = One Core**\n",
    "   - A task processes exactly one partition\n",
    "   - A task uses exactly one core\n",
    "   - Multiple tasks can run in parallel on multiple cores\n",
    "\n",
    "2. **Parallelism = Number of Cores**\n",
    "   - Maximum parallelism = Total cores in cluster\n",
    "   - To utilize all cores, you need at least that many partitions/tasks\n",
    "\n",
    "3. **Optimal Partition Count**\n",
    "   - Too few partitions â†’ Underutilization (cores sit idle)\n",
    "   - Too many partitions â†’ Overhead (scheduling overhead dominates)\n",
    "   - Sweet spot: 2-4Ã— the number of cores\n",
    "\n",
    "### The Formula\n",
    "\n",
    "```\n",
    "Total Cores = Number of Executors Ã— Cores per Executor\n",
    "\n",
    "Optimal Partitions = 2-4 Ã— Total Cores\n",
    "\n",
    "Example:\n",
    "- 4 executors Ã— 4 cores = 16 cores\n",
    "- Optimal partitions: 32-64 partitions\n",
    "```\n",
    "\n",
    "### What This Means for You\n",
    "\n",
    "**As a data engineer, you should:**\n",
    "- âœ… Understand your cluster configuration (executors, cores)\n",
    "- âœ… Monitor task distribution in Spark UI\n",
    "- âœ… Ensure partitions â‰¥ cores (ideally 2-4Ã—)\n",
    "- âœ… Avoid creating too few or too many partitions\n",
    "\n",
    "**Common Mistakes to Avoid:**\n",
    "- âŒ Assuming Spark will automatically use all cores\n",
    "- âŒ Not checking partition count before processing\n",
    "- âŒ Creating millions of tiny partitions\n",
    "- âŒ Having fewer partitions than cores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346ee07",
   "metadata": {},
   "source": [
    "## Practical Example: Checking Your Configuration\n",
    "\n",
    "Let's see how to check your Spark configuration and understand what you're working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a358b79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/02 22:11:44 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "26/01/02 22:11:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/02 22:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/02 22:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPARK SESSION INITIALIZED\n",
      "======================================================================\n",
      "Spark Version: 3.5.1\n",
      "App Name: SparkArchitectureDemo\n",
      "Master: local[*]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkArchitectureDemo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SPARK SESSION INITIALIZED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7c950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPARK CLUSTER CONFIGURATION\n",
      "======================================================================\n",
      "Default Parallelism: 11\n",
      "  â†’ This represents the number of cores available\n",
      "\n",
      "Executor Configuration:\n",
      "  Executor Memory: Not set\n",
      "  Cores per Executor: Not set\n",
      "  Number of Executors: Not set\n",
      "\n",
      "======================================================================\n",
      "NOTE: In local mode, you're running on a single machine.\n",
      "In a real cluster, you'd see multiple executors across worker nodes.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check Spark Configuration\n",
    "# This shows you the actual configuration of your Spark cluster\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Get default parallelism (usually equals number of cores)\n",
    "default_parallelism = sc.defaultParallelism\n",
    "\n",
    "# Get Spark configuration\n",
    "conf = sc.getConf()\n",
    "\n",
    "# Try to get executor information (may not be available in local mode)\n",
    "try:\n",
    "    executor_memory = conf.get(\"spark.executor.memory\", \"Not set\")\n",
    "    executor_cores = conf.get(\"spark.executor.cores\", \"Not set\")\n",
    "    num_executors = conf.get(\"spark.executor.instances\", \"Not set\")\n",
    "except:\n",
    "    executor_memory = \"Not available in local mode\"\n",
    "    executor_cores = \"Not available in local mode\"\n",
    "    num_executors = \"Not available in local mode\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SPARK CLUSTER CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Default Parallelism: {default_parallelism}\")\n",
    "print(f\"  â†’ This represents the number of cores available\")\n",
    "print(f\"\\nExecutor Configuration:\")\n",
    "print(f\"  Executor Memory: {executor_memory}\")\n",
    "print(f\"  Cores per Executor: {executor_cores}\")\n",
    "print(f\"  Number of Executors: {num_executors}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NOTE: In local mode, you're running on a single machine.\")\n",
    "print(\"In a real cluster, you'd see multiple executors across worker nodes.\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "UNDERSTANDING PARTITIONS AND TASKS\n",
      "======================================================================\n",
      "DataFrame created with 1000 rows\n",
      "Number of partitions: 11\n",
      "Default parallelism: 11\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHT:\n",
      "  - This DataFrame has 11 partitions\n",
      "  - When you perform an operation, Spark will create 11 tasks\n",
      "  - With 11 cores, 11 tasks can run simultaneously\n",
      "  - âœ… Good: 11 tasks can utilize all 11 cores\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: Understanding Partitions and Tasks\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(i, f\"Product_{i}\", 100.0 + i) for i in range(1000)]\n",
    "columns = [\"id\", \"product_name\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Check number of partitions\n",
    "# Note: While RDDs are legacy APIs, .rdd.getNumPartitions() is still the standard\n",
    "# way to get partition count from DataFrames (no direct DataFrame API exists for this)\n",
    "num_partitions = df.rdd.getNumPartitions()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UNDERSTANDING PARTITIONS AND TASKS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"DataFrame created with {len(data)} rows\")\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "print(f\"Default parallelism: {default_parallelism}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(f\"  - This DataFrame has {num_partitions} partitions\")\n",
    "print(f\"  - When you perform an operation, Spark will create {num_partitions} tasks\")\n",
    "print(f\"  - With {default_parallelism} cores, {min(default_parallelism, num_partitions)} tasks can run simultaneously\")\n",
    "if num_partitions < default_parallelism:\n",
    "    print(f\"  - âš ï¸  WARNING: Only {num_partitions} tasks, but {default_parallelism} cores available!\")\n",
    "    print(f\"     This means {default_parallelism - num_partitions} cores will be idle.\")\n",
    "elif num_partitions >= default_parallelism:\n",
    "    print(f\"  - âœ… Good: {num_partitions} tasks can utilize all {default_parallelism} cores\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f542b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REPARTITIONING EXAMPLE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 11) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original partitions: 11\n",
      "Repartitioned to: 22 partitions\n",
      "Default parallelism (cores): 11\n",
      "\n",
      "âœ… Now you have 22 partitions\n",
      "   This allows better utilization of 11 cores\n",
      "   Multiple batches of tasks can run efficiently\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: Repartitioning to Optimize\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"REPARTITIONING EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Repartition to match or exceed number of cores\n",
    "optimal_partitions = default_parallelism * 2  # 2Ã— cores for good load balancing\n",
    "\n",
    "df_repartitioned = df.repartition(optimal_partitions)\n",
    "# Note: Using .rdd.getNumPartitions() as it's the standard way to check partition count\n",
    "new_num_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"Original partitions: {num_partitions}\")\n",
    "print(f\"Repartitioned to: {new_num_partitions} partitions\")\n",
    "print(f\"Default parallelism (cores): {default_parallelism}\")\n",
    "print(f\"\\nâœ… Now you have {new_num_partitions} partitions\")\n",
    "print(f\"   This allows better utilization of {default_parallelism} cores\")\n",
    "print(f\"   Multiple batches of tasks can run efficiently\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e2d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OBSERVING TASK EXECUTION\n",
      "======================================================================\n",
      "Performing a simple operation to demonstrate task execution...\n",
      "(In Spark UI, you would see tasks being distributed across executors)\n",
      "\n",
      "Operation completed!\n",
      "  - Processed 22 partitions\n",
      "  - Created 22 tasks\n",
      "  - Tasks executed across available cores\n",
      "  - Result: 1000 unique products\n",
      "\n",
      "ğŸ’¡ TIP: Open Spark UI (usually at http://localhost:4040) to see\n",
      "   task distribution, executor utilization, and execution timeline\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example: Observing Task Execution\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OBSERVING TASK EXECUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Performing a simple operation to demonstrate task execution...\")\n",
    "print(\"(In Spark UI, you would see tasks being distributed across executors)\")\n",
    "print()\n",
    "\n",
    "# Perform an operation that triggers task execution\n",
    "result = df_repartitioned.groupBy(\"product_name\").count()\n",
    "\n",
    "# Count the results (this triggers execution)\n",
    "count = result.count()\n",
    "\n",
    "print(f\"Operation completed!\")\n",
    "print(f\"  - Processed {new_num_partitions} partitions\")\n",
    "print(f\"  - Created {new_num_partitions} tasks\")\n",
    "print(f\"  - Tasks executed across available cores\")\n",
    "print(f\"  - Result: {count} unique products\")\n",
    "print(\"\\nğŸ’¡ TIP: Open Spark UI (usually at http://localhost:4040) to see\")\n",
    "print(\"   task distribution, executor utilization, and execution timeline\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03323013",
   "metadata": {},
   "source": [
    "## Summary: The Complete Picture\n",
    "\n",
    "### Architecture Components Recap\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    SPARK CLUSTER                             â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
    "â”‚  â”‚   Driver Node    â”‚  â† Creates tasks, coordinates work    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚\n",
    "â”‚           â”‚                                                  â”‚\n",
    "â”‚           â”‚ Schedules tasks                                  â”‚\n",
    "â”‚           â”‚                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚         Worker Nodes                          â”‚          â”‚\n",
    "â”‚  â”‚                                                â”‚          â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚Executor 1â”‚  â”‚Executor 2â”‚  â”‚Executor 3â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚ Core 1   â”‚  â”‚ Core 1   â”‚  â”‚ Core 1   â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚ Core 2   â”‚  â”‚ Core 2   â”‚  â”‚ Core 2   â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚ Core 3   â”‚  â”‚ Core 3   â”‚  â”‚ Core 3   â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚ Core 4   â”‚  â”‚ Core 4   â”‚  â”‚ Core 4   â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚  â”‚[Task]    â”‚   â”‚          â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â”‚\n",
    "â”‚  â”‚                                                â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Relationships\n",
    "\n",
    "1. **Executors** run on worker nodes (JVM processes)\n",
    "2. **Cores** are CPU cores within each executor\n",
    "3. **Tasks** are work units that process partitions\n",
    "4. **Partitions** are logical divisions of data\n",
    "\n",
    "**The Flow:**\n",
    "```\n",
    "Data â†’ Partitions â†’ Tasks â†’ Scheduled to Executors â†’ Executed on Cores\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "âœ… **DO:**\n",
    "- Understand your cluster configuration\n",
    "- Ensure partitions â‰¥ cores (ideally 2-4Ã—)\n",
    "- Monitor Spark UI for task distribution\n",
    "- Repartition data when needed\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Assume Spark automatically uses all cores\n",
    "- Create too few partitions (underutilization)\n",
    "- Create too many partitions (overhead)\n",
    "- Ignore partition counts\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand the basics:\n",
    "1. Practice checking partition counts in your DataFrames\n",
    "2. Learn about `repartition()` and `coalesce()` operations\n",
    "3. Explore Spark UI to see tasks in action\n",
    "4. Move on to advanced topics like partition optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54873399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
