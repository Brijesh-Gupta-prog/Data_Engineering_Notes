{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apache Spark and Modern Data Architectures\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook covers the evolution beyond Hadoop MapReduce, including Apache Spark and the modern data architecture evolution from Data Warehouse to Data Lakehouse.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Challenges with MapReduce\n",
        "- Apache Spark and its advantages\n",
        "- Data Architecture Evolution: Data Warehouse → Data Lake → Lakehouse\n",
        "- Summary and key takeaways\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenges with MapReduce\n",
        "\n",
        "Despite its success, Hadoop MapReduce faced several challenges:\n",
        "\n",
        "### 1. Storage Capacity\n",
        "- **Issue**: Limited by disk I/O performance\n",
        "- **Impact**: Slow data access and processing\n",
        "- **Reason**: MapReduce writes intermediate results to disk\n",
        "\n",
        "### 2. Processing Time\n",
        "- **Issue**: High latency for iterative and interactive workloads\n",
        "- **Impact**: Not suitable for real-time or near-real-time processing\n",
        "- **Reason**: \n",
        "  - Disk-based processing (slow I/O)\n",
        "  - Overhead of job setup and teardown\n",
        "  - Not optimized for iterative algorithms (machine learning)\n",
        "\n",
        "### 3. Programming Complexity\n",
        "- **Issue**: Writing MapReduce programs is complex\n",
        "- **Impact**: Requires deep understanding of distributed systems\n",
        "- **Reason**: Low-level programming model\n",
        "\n",
        "### 4. Limited Language Support\n",
        "- **Issue**: MapReduce primarily available in Java\n",
        "- **Impact**: Limited accessibility for developers using other languages\n",
        "- **Reason**: Native Java implementation\n",
        "\n",
        "### 5. Hive Performance\n",
        "- **Issue**: Hive SQL queries performed slower than database SQL queries\n",
        "- **Impact**: Not suitable for interactive SQL workloads\n",
        "- **Reason**: \n",
        "  - Hive translates SQL to MapReduce jobs\n",
        "  - MapReduce overhead adds latency\n",
        "  - Not optimized for SQL workloads\n",
        "\n",
        "### Need for a Better Solution\n",
        "\n",
        "These challenges led to the development of **Apache Spark**, which addressed many of these limitations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enter Apache Spark\n",
        "\n",
        "**Apache Spark** was developed to address the limitations of Hadoop MapReduce.\n",
        "\n",
        "Apache Spark is a general purpose, inmemory, compute engine.\n",
        "\n",
        "### Advantages of Apache Spark over Hadoop\n",
        "\n",
        "**1. Performance**\n",
        "- **10 to 100 times faster** than Hadoop MapReduce\n",
        "- **In-memory processing**: Keeps data in memory instead of disk\n",
        "- **Optimized execution engine**: Advanced query optimization\n",
        "- **Reduced overhead**: Less job setup/teardown overhead\n",
        "\n",
        "**2. Ease of Development**\n",
        "- **Spark SQL**: High-performance SQL engine\n",
        "- **Composable Function API**: Easy to build complex data pipelines\n",
        "- **DataFrame API**: Similar to Pandas, intuitive for data engineers\n",
        "- **High-level APIs**: Less boilerplate code compared to MapReduce\n",
        "\n",
        "**3. Language Support**\n",
        "- **Java**: Full support\n",
        "- **Scala**: Native language (Spark is written in Scala)\n",
        "- **Python**: PySpark API\n",
        "- **R**: SparkR API\n",
        "- **SQL**: Spark SQL\n",
        "\n",
        "**4. Storage**\n",
        "- **HDFS**: Can read from and write to HDFS\n",
        "- **Multiple formats**: CSV, JSON, Parquet, ORC, Avro, etc.\n",
        "- **Multiple sources**: Databases, cloud storage, streaming sources\n",
        "\n",
        "**5. Resource Management**\n",
        "- **YARN**: Can run on Hadoop YARN\n",
        "- **Mesos**: Apache Mesos support\n",
        "- **Kubernetes**: Native Kubernetes support\n",
        "- **Standalone**: Can run in standalone mode\n",
        "\n",
        "### Apache Spark Deployment Options\n",
        "\n",
        "**1. With Hadoop (Data Lake)**\n",
        "- Spark runs on top of Hadoop cluster\n",
        "- Uses HDFS for storage\n",
        "- Uses YARN for resource management\n",
        "- Traditional Data Lake architecture\n",
        "\n",
        "**2. Without Hadoop (Lakehouse)**\n",
        "- Spark runs independently\n",
        "- **Cloud platforms**: AWS, Azure, GCP\n",
        "- **Databricks Spark Platform**: Managed Spark service\n",
        "- Modern Lakehouse architecture\n",
        "- Can use cloud storage (S3, ADLS, GCS) instead of HDFS\n",
        "\n",
        "Note - To turn your \"Without Hadoop\" setup into a Lakehouse, you usually add a storage format like Delta Lake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Spark and PySpark***\n",
        "\n",
        "Apache Spark (The Engine)\n",
        "Spark is the \"muscle.\" It is written in Scala and runs on the Java Virtual Machine (JVM). Its job is to handle the heavy lifting: breaking data into partitions, distributing tasks across a cluster of computers, and managing memory/fault tolerance.\n",
        "\n",
        "PySpark (The Interface)\n",
        "Since Spark is built in Scala, Python cannot talk to it directly. PySpark acts as a bridge. When you write Python code using PySpark, a library called Py4J translates those commands into calls that the Spark engine (the JVM) can understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Architecture Evolution\n",
        "\n",
        "Data architectures have evolved significantly to meet changing business needs and technological capabilities.\n",
        "\n",
        "### 1. Data Warehouse Architecture\n",
        "\n",
        "**Era**: 1980s - 2000s\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Structured Data → ETL → Data Warehouse → BI / Reports\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- **Input**: Only structured data\n",
        "- **Process**: ETL (Extract, Transform, Load)\n",
        "- **Storage**: Data Warehouse (optimized for analytics)\n",
        "- **Output**: BI tools and reports\n",
        "- **Schema**: Schema-on-write (structured before storage)\n",
        "- **Use Case**: Business intelligence, reporting, analytics\n",
        "\n",
        "**Limitations:**\n",
        "- Only handles structured data\n",
        "- Expensive to scale\n",
        "- Long ETL processes\n",
        "- Limited flexibility\n",
        "\n",
        "### 2. Data Lake Architecture\n",
        "\n",
        "**Era**: 2010s\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Structured, Semi-structured, Unstructured Data (Data Lake)\n",
        "    │\n",
        "    ├─→ ETL → Data Warehouse → BI / Reports\n",
        "    │\n",
        "    └─→ Data Science / ML\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- **Input**: All data types (structured, semi-structured, unstructured)\n",
        "- **Storage**: Data Lake (raw data storage)\n",
        "- **Processing**: \n",
        "  - ETL to Data Warehouse for BI/Reports\n",
        "  - Direct access for Data Science/ML\n",
        "- **Schema**: Schema-on-read (flexible schema)\n",
        "- **Use Case**: \n",
        "  - Traditional BI (via Data Warehouse)\n",
        "  - Advanced analytics and machine learning (direct from Data Lake)\n",
        "\n",
        "**Advantages:**\n",
        "- Handles all data types\n",
        "- Cost-effective storage\n",
        "- Flexible schema\n",
        "- Supports both BI and ML use cases\n",
        "\n",
        "**Challenges:**\n",
        "- Data quality issues (raw data)\n",
        "- Governance challenges\n",
        "- Performance issues for BI workloads\n",
        "- Still requires Data Warehouse for some use cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Data Lakehouse Architecture\n",
        "\n",
        "**Era**: 2020s (Modern Approach)\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Structured, Semi-structured, Unstructured Data (Data Lake)\n",
        "    │\n",
        "    └─→ Metadata and Governance Layer\n",
        "            │\n",
        "            ├─→ BI / Reports\n",
        "            └─→ Data Science / ML\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- **Input**: All data types (structured, semi-structured, unstructured)\n",
        "- **Storage**: Data Lake (single storage layer)\n",
        "- **Enhancement**: Metadata and governance layer on top\n",
        "- **Output**: \n",
        "  - Direct BI/Reports (no separate Data Warehouse needed)\n",
        "  - Direct Data Science/ML\n",
        "- **Schema**: Schema-on-read with metadata management\n",
        "- **Key Innovation**: Data Lake performs at par with Data Warehouse\n",
        "\n",
        "**Key Features:**\n",
        "- **ACID Transactions**: Ensures data consistency\n",
        "- **Schema Enforcement**: Data quality and governance\n",
        "- **Performance Optimization**: Query performance similar to Data Warehouse\n",
        "- **Unified Storage**: Single source of truth for all data\n",
        "- **Cost-Effective**: Eliminates need for separate Data Warehouse\n",
        "\n",
        "**Advantages:**\n",
        "- **Simplified Architecture**: One storage layer instead of two\n",
        "- **Cost Reduction**: No separate Data Warehouse needed\n",
        "- **Better Governance**: Metadata layer provides data quality\n",
        "- **Performance**: Optimized for both BI and ML workloads\n",
        "- **Flexibility**: Supports all data types and use cases\n",
        "\n",
        "**Technology Stack:**\n",
        "- **Storage**: Cloud storage (S3, ADLS, GCS) or HDFS\n",
        "- **Processing**: Apache Spark, Delta Lake\n",
        "- **Governance**: Apache Hive Metastore, Unity Catalog (Databricks)\n",
        "- **Query Engine**: Spark SQL, Presto, Trino\n",
        "\n",
        "### Architecture Comparison\n",
        "\n",
        "| Aspect | Data Warehouse | Data Lake | Data Lakehouse |\n",
        "|--------|---------------|-----------|----------------|\n",
        "| **Data Types** | Structured only | All types | All types |\n",
        "| **Schema** | Schema-on-write | Schema-on-read | Schema-on-read + governance |\n",
        "| **Storage** | Data Warehouse | Data Lake | Data Lake |\n",
        "| **BI Performance** | Optimized | Via Data Warehouse | Optimized |\n",
        "| **ML Support** | Limited | Direct access | Direct access |\n",
        "| **Cost** | High | Low | Low |\n",
        "| **Governance** | Strong | Weak | Strong |\n",
        "| **Complexity** | Medium | High | Medium |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Big Data Evolution Timeline\n",
        "\n",
        "### Key Milestones\n",
        "\n",
        "| Year | Milestone | Impact |\n",
        "|------|-----------|--------|\n",
        "| **1959** | COBOL developed | Beginning of structured business data processing |\n",
        "| **1970s-1980s** | RDBMS revolution (Oracle, SQL Server) | SQL-based data management becomes standard |\n",
        "| **1990s-2000s** | Internet and Mobile Revolution | Explosion of data variety, volume, and velocity |\n",
        "| **2003** | Google File System (GFS) paper | Foundation for distributed file systems |\n",
        "| **2004** | MapReduce paper | Foundation for distributed computing |\n",
        "| **2006** | Apache Hadoop project started | Open-source big data platform |\n",
        "| **2008** | Hadoop becomes top-level Apache project | Enterprise adoption begins |\n",
        "| **2010s** | Data Lake architecture emerges | Support for all data types |\n",
        "| **2014** | Apache Spark 1.0 released | Faster alternative to MapReduce |\n",
        "| **2020s** | Data Lakehouse architecture | Unified architecture for BI and ML |\n",
        "\n",
        "### Evolution Path\n",
        "\n",
        "```\n",
        "COBOL (1959)\n",
        "    ↓\n",
        "RDBMS (1970s-1980s)\n",
        "    ↓\n",
        "Internet/Mobile Revolution (1990s-2000s)\n",
        "    ↓\n",
        "Big Data Problem (3Vs)\n",
        "    ↓\n",
        "Hadoop (2006)\n",
        "    ↓\n",
        "Apache Spark (2014)\n",
        "    ↓\n",
        "Data Lakehouse (2020s)\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Data processing evolved** from single-machine to distributed systems\n",
        "2. **Data types expanded** from structured to include semi-structured and unstructured\n",
        "3. **Scalability shifted** from vertical (monolithic) to horizontal (distributed)\n",
        "4. **Performance improved** with in-memory processing (Spark vs MapReduce)\n",
        "5. **Architecture simplified** with Lakehouse (unified storage for BI and ML)\n",
        "6. **Cost reduced** through commodity hardware and cloud computing\n",
        "\n",
        "### Modern Big Data Stack\n",
        "\n",
        "**Storage**: Data Lake (S3, ADLS, GCS, HDFS)\n",
        "\n",
        "**Processing**: Apache Spark\n",
        "\n",
        "**Governance**: Metadata layer (Hive Metastore, Unity Catalog)\n",
        "\n",
        "**Query**: Spark SQL, Presto, Trino\n",
        "\n",
        "**Platforms**: Databricks, AWS EMR, Azure Synapse, GCP Dataproc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection Questions\n",
        "\n",
        "1. **Why did RDBMS systems fail to handle Big Data?**\n",
        "   - Think about the 3Vs (Variety, Volume, Velocity)\n",
        "   - Consider schema-on-write vs schema-on-read\n",
        "\n",
        "2. **What are the key advantages of distributed systems over monolithic systems?**\n",
        "   - Scalability, fault tolerance, cost-effectiveness\n",
        "\n",
        "3. **How does Hadoop address the Big Data problem?**\n",
        "   - YARN for resource management\n",
        "   - HDFS for distributed storage\n",
        "   - MapReduce for distributed computing\n",
        "\n",
        "4. **Why is Apache Spark faster than Hadoop MapReduce?**\n",
        "   - In-memory processing\n",
        "   - Optimized execution engine\n",
        "   - Reduced overhead\n",
        "\n",
        "5. **What is the key difference between Data Lake and Data Lakehouse?**\n",
        "   - Governance and metadata layer\n",
        "   - Performance optimization\n",
        "   - Unified architecture\n",
        "\n",
        "6. **How has data architecture evolved over time?**\n",
        "   - From Data Warehouse to Data Lake to Data Lakehouse\n",
        "   - From structured-only to all data types\n",
        "   - From separate systems to unified architecture\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
