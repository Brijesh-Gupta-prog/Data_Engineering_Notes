{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practical Demonstration: Executors and Partitions\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a **hands-on demonstration** of the partition concepts covered in `08_a_Partitions_Concepts.ipynb`.\n",
        "\n",
        "## What You'll Do\n",
        "\n",
        "1. **Create 4 Parquet files** to simulate real-world storage (like ADLS)\n",
        "2. **Read them and observe** partition behavior\n",
        "3. **View Spark UI** to see parallelism **before repartitioning**\n",
        "4. **Repartition** based on your machine's actual cores\n",
        "5. **View Spark UI again** to see improved parallelism **after repartitioning**\n",
        "6. **Compare performance** before and after optimization\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Complete `08_a_Partitions_Concepts.ipynb` first to understand the concepts\n",
        "- Spark installed and configured\n",
        "- Jupyter notebook environment ready\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:** This is the **practice notebook**. For concepts and theory, see `08_a_Partitions_Concepts.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initializing Spark Session\n",
        "\n",
        "Before we start, let's create a Spark session with UI enabled so we can monitor job execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è  Detected 11 CPU cores on your system\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/03 12:03:21 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.20 instead (on interface en0)\n",
            "26/01/03 12:03:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "26/01/03 12:03:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/01/03 12:03:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SPARK SESSION INITIALIZED\n",
            "======================================================================\n",
            "Spark Version: 3.5.1\n",
            "Spark App Name: PartitionOptimizationDemo\n",
            "Master: local[*]\n",
            "Default Parallelism: 11\n",
            "\n",
            "üåê Spark UI URL: http://192.168.1.20:4041\n",
            "\n",
            "üí° TIP: Open this URL in your browser to monitor job execution!\n",
            "   You'll see task distribution, parallelism, and resource utilization\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session with UI enabled\n",
        "from pyspark.sql import SparkSession\n",
        "import multiprocessing\n",
        "\n",
        "# Stop any existing Spark session\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Get actual CPU cores on your system\n",
        "physical_cores = multiprocessing.cpu_count()\n",
        "print(f\"üñ•Ô∏è  Detected {physical_cores} CPU cores on your system\")\n",
        "\n",
        "# Create Spark session with UI enabled\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PartitionOptimizationDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Display Spark version and configuration\n",
        "print(\"=\" * 70)\n",
        "print(\"SPARK SESSION INITIALIZED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Spark App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")\n",
        "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
        "\n",
        "# Get Spark UI URL\n",
        "ui_url = spark.sparkContext.uiWebUrl\n",
        "print(f\"\\nüåê Spark UI URL: {ui_url}\")\n",
        "print(\"\\nüí° TIP: Open this URL in your browser to monitor job execution!\")\n",
        "print(\"   You'll see task distribution, parallelism, and resource utilization\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Creating Sample Data Files\n",
        "\n",
        "Let's create 4 Parquet files to simulate the real-world scenario of having 4 large files in storage (like ADLS).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CREATING 4 PARQUET FILES TO DEMONSTRATE PARTITION CONCEPT\n",
            "======================================================================\n",
            "Cleaned up existing directory: data/sales_demo\n",
            "\n",
            "Creating data for North region (File 1/4)...\n",
            "  ‚úì Created DataFrame for North region\n",
            "  ‚úì Records: 50,000\n",
            "\n",
            "Creating data for South region (File 2/4)...\n",
            "  ‚úì Created DataFrame for South region\n",
            "  ‚úì Records: 50,000\n",
            "\n",
            "Creating data for East region (File 3/4)...\n",
            "  ‚úì Created DataFrame for East region\n",
            "  ‚úì Records: 50,000\n",
            "\n",
            "Creating data for West region (File 4/4)...\n",
            "  ‚úì Created DataFrame for West region\n",
            "  ‚úì Records: 50,000\n",
            "\n",
            "Combining all regions into a single dataset...\n",
            "\n",
            "Writing to parquet with 4 partitions (4 files)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/03 12:03:33 WARN TaskSetManager: Stage 0 contains a task of very large size (2304 KiB). The maximum recommended task size is 1000 KiB.\n",
            "[Stage 0:>                                                          (0 + 4) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "‚úÖ All 4 Parquet files created successfully!\n",
            "üìÅ Location: data/sales_demo/\n",
            "üìä Number of parquet files: 4\n",
            "üíæ Total size: 1.30 MB\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Step 1: Create 4 Parquet Files to Simulate Real-World Scenario\n",
        "# This mimics having 4 large files in ADLS (like part-00000.parquet, part-00001.parquet, etc.)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from pyspark.sql import Row\n",
        "from datetime import date, timedelta\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CREATING 4 PARQUET FILES TO DEMONSTRATE PARTITION CONCEPT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = \"data/sales_demo\"\n",
        "\n",
        "# Clean up existing directory if it exists\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "    print(f\"Cleaned up existing directory: {output_dir}\")\n",
        "\n",
        "# Create sample sales data\n",
        "# Each file will represent a different region with substantial data\n",
        "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
        "records_per_file = 50000  # Enough data to see the effect, but manageable\n",
        "\n",
        "# Collect all data first\n",
        "all_dataframes = []\n",
        "\n",
        "for i, region in enumerate(regions):\n",
        "    print(f\"\\nCreating data for {region} region (File {i+1}/4)...\")\n",
        "    \n",
        "    # Generate sample sales data\n",
        "    data = []\n",
        "    base_date = date(2023, 1, 1)\n",
        "    \n",
        "    for j in range(records_per_file):\n",
        "        data.append(Row(\n",
        "            sale_id=i * records_per_file + j,\n",
        "            region=region,\n",
        "            product_id=f\"PROD_{j % 1000:04d}\",\n",
        "            customer_id=f\"CUST_{j % 5000:05d}\",\n",
        "            sale_amount=round(100.0 + (j % 1000) * 0.5, 2),\n",
        "            sale_date=base_date + timedelta(days=j % 365),\n",
        "            quantity=(j % 10) + 1\n",
        "        ))\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = spark.createDataFrame(data)\n",
        "    all_dataframes.append(df)\n",
        "    \n",
        "    print(f\"  ‚úì Created DataFrame for {region} region\")\n",
        "    print(f\"  ‚úì Records: {records_per_file:,}\")\n",
        "\n",
        "# Combine all dataframes\n",
        "print(f\"\\nCombining all regions into a single dataset...\")\n",
        "combined_df = all_dataframes[0]\n",
        "for df in all_dataframes[1:]:\n",
        "    combined_df = combined_df.union(df)\n",
        "\n",
        "# Write to parquet with exactly 4 partitions (4 files)\n",
        "# This simulates having exactly 4 files in storage\n",
        "print(f\"\\nWriting to parquet with 4 partitions (4 files)...\")\n",
        "combined_df.coalesce(4).write.mode(\"overwrite\").parquet(output_dir)\n",
        "\n",
        "# Verify the files were created\n",
        "parquet_files = glob.glob(f\"{output_dir}/*.parquet\")\n",
        "if not parquet_files:\n",
        "    # Sometimes files are in subdirectories\n",
        "    parquet_files = glob.glob(f\"{output_dir}/**/*.parquet\", recursive=True)\n",
        "\n",
        "total_size = sum(os.path.getsize(f) for f in parquet_files) if parquet_files else 0\n",
        "file_count = len(parquet_files)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ All 4 Parquet files created successfully!\")\n",
        "print(f\"üìÅ Location: {output_dir}/\")\n",
        "print(f\"üìä Number of parquet files: {file_count}\")\n",
        "if total_size > 0:\n",
        "    print(f\"üíæ Total size: {total_size / (1024*1024):.2f} MB\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Reading Data and Observing the Problem\n",
        "\n",
        "Now let's read the data and see how Spark creates partitions. **‚ö†Ô∏è IMPORTANT: Check Spark UI now!**\n",
        "\n",
        "You should see only 4 tasks running (one per partition), and most of your cores will be idle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 1: Understanding Your System Configuration\n",
            "======================================================================\n",
            "\n",
            "üñ•Ô∏è  Physical CPU Cores on Your System: 11\n",
            "‚öôÔ∏è  Spark Default Parallelism: 11\n",
            "\n",
            "üåê Spark UI URL: http://192.168.1.20:4041\n",
            "   üëâ OPEN THIS URL NOW to monitor the job execution!\n",
            "\n",
            "‚ÑπÔ∏è  Executor info not available: getExecutorInfos not available\n",
            "‚ÑπÔ∏è  Using default parallelism (11) as total available cores\n",
            "‚ÑπÔ∏è  Assuming 1 executor with 11 cores (local mode)\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Reading 4 Parquet Files (The Problem)\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT: Check Spark UI now!\n",
            "   Go to: http://192.168.1.20:4041\n",
            "   Navigate to 'Jobs' or 'Stages' tab\n",
            "   You'll see only 4 tasks running (one per partition)\n",
            "   Most of your cores will be idle!\n",
            "\n",
            "üìÅ Files Read: 4 parquet files from data/sales_demo/\n",
            "üìä Total Records: 200,000\n",
            "üî¢ Spark Partitions Created: 4\n",
            "‚öôÔ∏è  Available Cores: 11\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "DIAGNOSIS:\n",
            "----------------------------------------------------------------------\n",
            "‚ö†Ô∏è  PROBLEM DETECTED!\n",
            "   ‚Ä¢ You have 4 partitions but 11 cores\n",
            "   ‚Ä¢ 7 cores will be IDLE (doing nothing)\n",
            "   ‚Ä¢ Resource utilization: 36.4%\n",
            "   ‚Ä¢ Waste: 63.6% of your compute resources\n",
            "\n",
            "   This means:\n",
            "   ‚Ä¢ Only 4 tasks will run in parallel\n",
            "   ‚Ä¢ 7 cores will sit idle, wasting resources\n",
            "   ‚Ä¢ Your job will run much slower than it could\n",
            "\n",
            "üìä Check Spark UI:\n",
            "   ‚Ä¢ Go to: http://192.168.1.20:4041\n",
            "   ‚Ä¢ Look at the 'Stages' tab\n",
            "   ‚Ä¢ You should see only 4 tasks\n",
            "   ‚Ä¢ Notice how many cores are idle!\n",
            "\n",
            "======================================================================\n",
            "STEP 3: Visualizing the Problem\n",
            "======================================================================\n",
            "\n",
            "Current Situation:\n",
            "  Files in storage: 4\n",
            "  Spark partitions: 4\n",
            "  Available cores: 11\n",
            "  Number of executors: 1\n",
            "  Cores per executor: 11\n",
            "\n",
            "  Task Distribution (Local Mode - 1 executor with 11 cores):\n",
            "    Executor 1: [Task 1] [Task 2] [Task 3] [Task 4] [‚óã] [‚óã] [‚óã] [‚óã]\n",
            "                ... (4 tasks total, 7 cores idle)\n",
            "    Total: 4 cores busy, 7 cores idle\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Read the 4 Parquet Files and Diagnose the Problem\n",
        "\n",
        "import multiprocessing\n",
        "import time\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 1: Understanding Your System Configuration\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get actual CPU cores on your system\n",
        "physical_cores = multiprocessing.cpu_count()\n",
        "print(f\"\\nüñ•Ô∏è  Physical CPU Cores on Your System: {physical_cores}\")\n",
        "\n",
        "# Get Spark's default parallelism (usually equals total cores available to Spark)\n",
        "default_parallelism = spark.sparkContext.defaultParallelism\n",
        "print(f\"‚öôÔ∏è  Spark Default Parallelism: {default_parallelism}\")\n",
        "\n",
        "# Get Spark UI URL\n",
        "ui_url = spark.sparkContext.uiWebUrl\n",
        "print(f\"\\nüåê Spark UI URL: {ui_url}\")\n",
        "print(\"   üëâ OPEN THIS URL NOW to monitor the job execution!\")\n",
        "\n",
        "# Try to get executor info\n",
        "num_executors = 1\n",
        "cores_per_executor = default_parallelism\n",
        "try:\n",
        "    status_tracker = spark.sparkContext.statusTracker()\n",
        "    if hasattr(status_tracker, 'getExecutorInfos'):\n",
        "        executors = status_tracker.getExecutorInfos()\n",
        "        num_executors = len(executors)\n",
        "        print(f\"\\nüì¶ Number of Executors: {num_executors}\")\n",
        "        if executors:\n",
        "            cores_per_executor = executors[0].totalCores\n",
        "            print(f\"üîß Cores per Executor: {cores_per_executor}\")\n",
        "            total_spark_cores = num_executors * cores_per_executor\n",
        "            print(f\"üìä Total Spark Cores: {total_spark_cores}\")\n",
        "    else:\n",
        "        raise AttributeError(\"getExecutorInfos not available\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ÑπÔ∏è  Executor info not available: {e}\")\n",
        "    print(f\"‚ÑπÔ∏è  Using default parallelism ({default_parallelism}) as total available cores\")\n",
        "    print(f\"‚ÑπÔ∏è  Assuming 1 executor with {default_parallelism} cores (local mode)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 2: Reading 4 Parquet Files (The Problem)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Check Spark UI now!\")\n",
        "print(f\"   Go to: {ui_url}\")\n",
        "print(\"   Navigate to 'Jobs' or 'Stages' tab\")\n",
        "print(\"   You'll see only 4 tasks running (one per partition)\")\n",
        "print(\"   Most of your cores will be idle!\\n\")\n",
        "\n",
        "# Read the 4 parquet files we created\n",
        "# This simulates reading from ADLS: spark.read.parquet(\"abfss://.../sales/\")\n",
        "sales_df = spark.read.parquet(\"data/sales_demo/\")\n",
        "\n",
        "# Check how many partitions Spark created\n",
        "num_partitions = sales_df.rdd.getNumPartitions()\n",
        "total_records = sales_df.count()\n",
        "\n",
        "print(f\"üìÅ Files Read: 4 parquet files from data/sales_demo/\")\n",
        "print(f\"üìä Total Records: {total_records:,}\")\n",
        "print(f\"üî¢ Spark Partitions Created: {num_partitions}\")\n",
        "print(f\"‚öôÔ∏è  Available Cores: {default_parallelism}\")\n",
        "\n",
        "# Diagnosis\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"DIAGNOSIS:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "if num_partitions < default_parallelism:\n",
        "    waste_percentage = (1 - num_partitions / default_parallelism) * 100\n",
        "    idle_cores = default_parallelism - num_partitions\n",
        "    utilization = (num_partitions / default_parallelism) * 100\n",
        "    \n",
        "    print(f\"‚ö†Ô∏è  PROBLEM DETECTED!\")\n",
        "    print(f\"   ‚Ä¢ You have {num_partitions} partitions but {default_parallelism} cores\")\n",
        "    print(f\"   ‚Ä¢ {idle_cores} cores will be IDLE (doing nothing)\")\n",
        "    print(f\"   ‚Ä¢ Resource utilization: {utilization:.1f}%\")\n",
        "    print(f\"   ‚Ä¢ Waste: {waste_percentage:.1f}% of your compute resources\")\n",
        "    print(f\"\\n   This means:\")\n",
        "    print(f\"   ‚Ä¢ Only {num_partitions} tasks will run in parallel\")\n",
        "    print(f\"   ‚Ä¢ {idle_cores} cores will sit idle, wasting resources\")\n",
        "    print(f\"   ‚Ä¢ Your job will run much slower than it could\")\n",
        "    \n",
        "    print(f\"\\nüìä Check Spark UI:\")\n",
        "    print(f\"   ‚Ä¢ Go to: {ui_url}\")\n",
        "    print(f\"   ‚Ä¢ Look at the 'Stages' tab\")\n",
        "    print(f\"   ‚Ä¢ You should see only {num_partitions} tasks\")\n",
        "    print(f\"   ‚Ä¢ Notice how many cores are idle!\")\n",
        "else:\n",
        "    print(\"‚úÖ Partition count looks good!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 3: Visualizing the Problem\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nCurrent Situation:\")\n",
        "print(f\"  Files in storage: 4\")\n",
        "print(f\"  Spark partitions: {num_partitions}\")\n",
        "print(f\"  Available cores: {default_parallelism}\")\n",
        "print(f\"  Number of executors: {num_executors}\")\n",
        "print(f\"  Cores per executor: {cores_per_executor}\")\n",
        "\n",
        "# Visualize task distribution based on actual configuration\n",
        "if num_executors == 1:\n",
        "    # Local mode - single executor\n",
        "    print(f\"\\n  Task Distribution (Local Mode - 1 executor with {cores_per_executor} cores):\")\n",
        "    task_visual = \" \".join([f\"[Task {i+1}]\" for i in range(min(num_partitions, 4))])\n",
        "    idle_visual = \" \".join([\"[‚óã]\"] * max(0, min(cores_per_executor - num_partitions, 4)))\n",
        "    print(f\"    Executor 1: {task_visual} {idle_visual}\")\n",
        "    if cores_per_executor > 4:\n",
        "        print(f\"                ... ({num_partitions} tasks total, {default_parallelism - num_partitions} cores idle)\")\n",
        "    else:\n",
        "        print(f\"                ‚Üë Only {num_partitions} tasks, {default_parallelism - num_partitions} cores idle\")\n",
        "else:\n",
        "    # Cluster mode - multiple executors\n",
        "    tasks_per_executor_visual = max(1, num_partitions // num_executors)\n",
        "    print(f\"\\n  Task Distribution ({num_executors} executors with {cores_per_executor} cores each):\")\n",
        "    for i in range(min(num_executors, num_partitions)):\n",
        "        idle_cores_vis = max(0, cores_per_executor - 1)\n",
        "        idle_dots = \"[‚óã] \" * min(idle_cores_vis, 3)\n",
        "        if idle_cores_vis > 3:\n",
        "            idle_dots += f\"... ({idle_cores_vis} total idle)\"\n",
        "        print(f\"    Executor {i+1}: [Task {i+1}] {idle_dots}‚Üí 1 core used, {idle_cores_vis} idle\")\n",
        "    if num_partitions < num_executors:\n",
        "        print(f\"    (Only {num_partitions} tasks for {num_executors} executors)\")\n",
        "print(f\"    Total: {num_partitions} cores busy, {default_parallelism - num_partitions} cores idle\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Optimizing with Repartition Based on Your Machine's Cores\n",
        "\n",
        "Now we'll repartition the data based on the actual number of cores detected on your machine. **‚ö†Ô∏è Check Spark UI again after this!**\n",
        "\n",
        "You should now see many more tasks (equal to 2√ó your core count), and all cores should be busy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 4: The Solution - Optimizing with Repartition\n",
            "======================================================================\n",
            "\n",
            "üéØ Target Partitions: 22 (2√ó your 11 cores)\n",
            "   This is calculated based on YOUR machine's actual core count: 11\n",
            "\n",
            "Repartitioning from 4 to 22 partitions...\n",
            "   This will trigger a shuffle operation...\n",
            "\n",
            "‚úÖ Repartitioning Complete!\n",
            "   ‚Ä¢ New partition count: 22\n",
            "   ‚Ä¢ Records preserved: 200,000 (same as before)\n",
            "   ‚Ä¢ Available cores: 11\n",
            "\n",
            "üìà Improvement:\n",
            "   ‚Ä¢ Resource utilization: 200.0%\n",
            "   ‚Ä¢ All 11 cores can now be utilized\n",
            "   ‚Ä¢ Tasks will be queued for smooth execution\n",
            "\n",
            "  Optimized Task Distribution:\n",
            "    Executor 1: [22 tasks] ‚Üí All 11 cores busy + queued tasks\n",
            "    Total: 22 tasks ‚Üí All 11 cores utilized!\n",
            "\n",
            "üìä Check Spark UI NOW to see the difference!\n",
            "   ‚Ä¢ Go to: http://192.168.1.20:4041\n",
            "   ‚Ä¢ Navigate to 'Stages' tab\n",
            "   ‚Ä¢ You should now see 22 tasks instead of 4\n",
            "   ‚Ä¢ Notice how all cores are now being utilized!\n",
            "   ‚Ä¢ Compare this with what you saw before repartitioning!\n"
          ]
        }
      ],
      "source": [
        "# Step 3: The Solution - Optimizing with Repartition Based on Your Machine's Cores\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 4: The Solution - Optimizing with Repartition\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate optimal partition count based on YOUR machine's cores\n",
        "# Using 2√ó core count for optimal load balancing\n",
        "optimal_partitions = default_parallelism * 2\n",
        "print(f\"\\nüéØ Target Partitions: {optimal_partitions} (2√ó your {default_parallelism} cores)\")\n",
        "print(f\"   This is calculated based on YOUR machine's actual core count: {physical_cores}\")\n",
        "\n",
        "# Repartition the data\n",
        "print(f\"\\nRepartitioning from {num_partitions} to {optimal_partitions} partitions...\")\n",
        "print(\"   This will trigger a shuffle operation...\")\n",
        "sales_df_optimized = sales_df.repartition(optimal_partitions)\n",
        "\n",
        "# Verify\n",
        "optimized_partitions = sales_df_optimized.rdd.getNumPartitions()\n",
        "optimized_records = sales_df_optimized.count()\n",
        "\n",
        "print(f\"\\n‚úÖ Repartitioning Complete!\")\n",
        "print(f\"   ‚Ä¢ New partition count: {optimized_partitions}\")\n",
        "print(f\"   ‚Ä¢ Records preserved: {optimized_records:,} (same as before)\")\n",
        "print(f\"   ‚Ä¢ Available cores: {default_parallelism}\")\n",
        "\n",
        "# Show improvement\n",
        "utilization_after = (optimized_partitions / default_parallelism) * 100\n",
        "print(f\"\\nüìà Improvement:\")\n",
        "print(f\"   ‚Ä¢ Resource utilization: {utilization_after:.1f}%\")\n",
        "print(f\"   ‚Ä¢ All {default_parallelism} cores can now be utilized\")\n",
        "print(f\"   ‚Ä¢ Tasks will be queued for smooth execution\")\n",
        "\n",
        "print(f\"\\n  Optimized Task Distribution:\")\n",
        "if num_executors == 1:\n",
        "    # Local mode\n",
        "    tasks_per_executor = optimized_partitions\n",
        "    print(f\"    Executor 1: [{tasks_per_executor} tasks] ‚Üí All {cores_per_executor} cores busy + queued tasks\")\n",
        "    print(f\"    Total: {optimized_partitions} tasks ‚Üí All {default_parallelism} cores utilized!\")\n",
        "else:\n",
        "    # Cluster mode\n",
        "    tasks_per_executor = optimized_partitions // num_executors\n",
        "    for i in range(num_executors):\n",
        "        print(f\"    Executor {i+1}: [{tasks_per_executor} tasks] ‚Üí All {cores_per_executor} cores busy + queued tasks\")\n",
        "    print(f\"    Total: {optimized_partitions} tasks ‚Üí All {default_parallelism} cores utilized!\")\n",
        "\n",
        "print(f\"\\nüìä Check Spark UI NOW to see the difference!\")\n",
        "print(f\"   ‚Ä¢ Go to: {ui_url}\")\n",
        "print(f\"   ‚Ä¢ Navigate to 'Stages' tab\")\n",
        "print(f\"   ‚Ä¢ You should now see {optimized_partitions} tasks instead of {num_partitions}\")\n",
        "print(f\"   ‚Ä¢ Notice how all cores are now being utilized!\")\n",
        "print(f\"   ‚Ä¢ Compare this with what you saw before repartitioning!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Performance Comparison\n",
        "\n",
        "Let's run a simple operation to see the performance difference. **Watch Spark UI during execution!**\n",
        "\n",
        "You'll see the difference in parallelism - before: only 4 tasks, after: many more tasks utilizing all cores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 5: Performance Comparison\n",
            "======================================================================\n",
            "\n",
            "üåê Keep Spark UI open: http://192.168.1.20:4041\n",
            "   Watch the 'Stages' tab to see task distribution in real-time!\n",
            "\n",
            "Running a simple aggregation to show the difference...\n",
            "   üëÄ Watch Spark UI to see the difference in parallelism!\n",
            "\n",
            "‚è±Ô∏è  Without optimization (4 partitions):\n",
            "   üëâ Check Spark UI - you should see only 4 tasks running\n",
            "   Time taken: 0.47 seconds\n",
            "   Tasks: 4 (only 4 cores utilized)\n",
            "\n",
            "‚è±Ô∏è  With optimization (22 partitions):\n",
            "   üëâ Check Spark UI - you should see 22 tasks running\n",
            "   üëâ Notice how all 11 cores are now busy!\n",
            "   Time taken: 0.56 seconds\n",
            "   Tasks: 22 (all 11 cores utilized)\n",
            "\n",
            "üìä Speedup: 0.83√ó faster with optimized partitions\n",
            "   (Note: Speedup may vary based on data size and cluster configuration)\n",
            "   ‚ÑπÔ∏è  For small datasets, overhead of repartitioning may outweigh benefits\n",
            "   ‚ÑπÔ∏è  Benefits are more pronounced with larger datasets and more cores\n",
            "   ‚ÑπÔ∏è  The key benefit is better resource utilization, not always speed\n",
            "\n",
            "======================================================================\n",
            "‚úÖ DEMONSTRATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Key Takeaways:\n",
            "  ‚Ä¢ Started with 4 partitions (from 4 files)\n",
            "  ‚Ä¢ Optimized to 22 partitions (based on 11 cores)\n",
            "  ‚Ä¢ Now utilizing all 11 cores efficiently!\n",
            "\n",
            "üìä Spark UI Observations:\n",
            "  ‚Ä¢ Before: 4 tasks ‚Üí 4 cores busy, 7 idle\n",
            "  ‚Ä¢ After: 22 tasks ‚Üí All 11 cores busy\n",
            "  ‚Ä¢ Check Spark UI at: http://192.168.1.20:4041\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Performance Comparison\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 5: Performance Comparison\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüåê Keep Spark UI open: {ui_url}\")\n",
        "print(\"   Watch the 'Stages' tab to see task distribution in real-time!\")\n",
        "\n",
        "# Show a simple operation to demonstrate the difference\n",
        "print(\"\\nRunning a simple aggregation to show the difference...\")\n",
        "print(\"   üëÄ Watch Spark UI to see the difference in parallelism!\")\n",
        "\n",
        "# Without optimization\n",
        "print(f\"\\n‚è±Ô∏è  Without optimization ({num_partitions} partitions):\")\n",
        "print(\"   üëâ Check Spark UI - you should see only 4 tasks running\")\n",
        "start = time.time()\n",
        "result_bad = sales_df.groupBy(\"region\").agg({\"sale_amount\": \"sum\"}).collect()\n",
        "time_bad = time.time() - start\n",
        "print(f\"   Time taken: {time_bad:.2f} seconds\")\n",
        "print(f\"   Tasks: {num_partitions} (only {num_partitions} cores utilized)\")\n",
        "\n",
        "# With optimization\n",
        "print(f\"\\n‚è±Ô∏è  With optimization ({optimal_partitions} partitions):\")\n",
        "print(f\"   üëâ Check Spark UI - you should see {optimal_partitions} tasks running\")\n",
        "print(f\"   üëâ Notice how all {default_parallelism} cores are now busy!\")\n",
        "start = time.time()\n",
        "result_good = sales_df_optimized.groupBy(\"region\").agg({\"sale_amount\": \"sum\"}).collect()\n",
        "time_good = time.time() - start\n",
        "print(f\"   Time taken: {time_good:.2f} seconds\")\n",
        "print(f\"   Tasks: {optimized_partitions} (all {default_parallelism} cores utilized)\")\n",
        "\n",
        "if time_bad > 0 and time_good > 0:\n",
        "    speedup = time_bad / time_good if time_good > 0 else 1\n",
        "    print(f\"\\nüìä Speedup: {speedup:.2f}√ó faster with optimized partitions\")\n",
        "    print(f\"   (Note: Speedup may vary based on data size and cluster configuration)\")\n",
        "    if speedup < 1:\n",
        "        print(f\"   ‚ÑπÔ∏è  For small datasets, overhead of repartitioning may outweigh benefits\")\n",
        "        print(f\"   ‚ÑπÔ∏è  Benefits are more pronounced with larger datasets and more cores\")\n",
        "        print(f\"   ‚ÑπÔ∏è  The key benefit is better resource utilization, not always speed\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ DEMONSTRATION COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nKey Takeaways:\")\n",
        "print(f\"  ‚Ä¢ Started with {num_partitions} partitions (from 4 files)\")\n",
        "print(f\"  ‚Ä¢ Optimized to {optimized_partitions} partitions (based on {default_parallelism} cores)\")\n",
        "print(f\"  ‚Ä¢ Now utilizing all {default_parallelism} cores efficiently!\")\n",
        "print(f\"\\nüìä Spark UI Observations:\")\n",
        "print(f\"  ‚Ä¢ Before: {num_partitions} tasks ‚Üí {num_partitions} cores busy, {default_parallelism - num_partitions} idle\")\n",
        "print(f\"  ‚Ä¢ After: {optimized_partitions} tasks ‚Üí All {default_parallelism} cores busy\")\n",
        "print(f\"  ‚Ä¢ Check Spark UI at: {ui_url}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What You Demonstrated\n",
        "\n",
        "1. **Created 4 Parquet files** - Simulated real-world storage scenario\n",
        "2. **Read data naively** - Observed only 4 partitions created\n",
        "3. **Checked Spark UI** - Saw only 4 tasks, most cores idle\n",
        "4. **Repartitioned** - Based on your machine's actual cores (2√ó core count)\n",
        "5. **Checked Spark UI again** - Saw many more tasks, all cores busy\n",
        "6. **Compared performance** - Observed the difference in parallelism\n",
        "\n",
        "### Key Observations from Spark UI\n",
        "\n",
        "**Before Repartitioning:**\n",
        "- Only 4 tasks running\n",
        "- Most cores idle\n",
        "- Poor resource utilization\n",
        "\n",
        "**After Repartitioning:**\n",
        "- Many tasks running (2√ó your core count)\n",
        "- All cores busy\n",
        "- Optimal resource utilization\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Apply this knowledge to your own data\n",
        "- Always check partition count before expensive operations\n",
        "- Use Spark UI to monitor and optimize your jobs\n",
        "- Review the concepts notebook (`08_a_Partitions_Concepts.ipynb`) for deeper understanding\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
