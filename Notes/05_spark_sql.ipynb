{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 - Spark SQL\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module focuses on Spark SQL - using SQL syntax to work with PySpark DataFrames. DataFrames and Spark SQL tables are interconvertible, allowing you to work with either DataFrame API or SQL syntax. This is especially useful for those coming from SQL backgrounds.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- DataFrames and Spark SQL tables are interconvertible\n",
    "- Creating Spark SQL table views from DataFrames\n",
    "- Converting Spark SQL tables back to DataFrames\n",
    "- All view creation methods (createTempView, createOrReplaceTempView, createGlobalTempView, createOrReplaceGlobalTempView)\n",
    "- Using SQL queries with PySpark\n",
    "- Best practices for interoperability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and Spark SQL Tables - Interconvertible\n",
    "\n",
    "**Key Concept**: DataFrames and Spark SQL tables are **interconvertible**. This means you can:\n",
    "- Create a Spark SQL table view from a DataFrame (to use SQL syntax)\n",
    "- Convert a Spark SQL table back to a DataFrame (to use DataFrame API)\n",
    "\n",
    "This interoperability is very convenient, especially for SQL developers who prefer SQL syntax over DataFrame API.\n",
    "\n",
    "### Why Create Spark SQL Table Views from DataFrames?\n",
    "\n",
    "When you create a Spark SQL table view from a DataFrame, you can execute normal SQL queries on it. This is much more convenient for SQL developers who are familiar with SQL syntax rather than the DataFrame API. The Spark SQL table view acts as a bridge between the DataFrame and SQL worlds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 21:38:39 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/12/28 21:38:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/28 21:38:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 21:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/28 21:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/28 21:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/12/28 21:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/12/28 21:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/12/28 21:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Grace| 29|        HR| 58000|  London|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "Created temporary view 'employees' from DataFrame\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"New York\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"London\"),\n",
    "    (\"Charlie\", 35, \"Sales\", 70000, \"Tokyo\"),\n",
    "    (\"Diana\", 28, \"IT\", 55000, \"Paris\"),\n",
    "    (\"Eve\", 32, \"HR\", 65000, \"Sydney\"),\n",
    "    (\"Frank\", 27, \"Sales\", 52000, \"New York\"),\n",
    "    (\"Grace\", 29, \"HR\", 58000, \"London\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Create a temporary view from the DataFrame (this is what we'll use in SQL queries)\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "print(\"\\nCreated temporary view 'employees' from DataFrame\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting DataFrame to Spark SQL Table View\n",
    "\n",
    "### Method 1: createOrReplaceTempView()\n",
    "\n",
    "Creates a temporary view or replaces it if it already exists. The view is session-scoped (only available in the current SparkSession).\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"view_name\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the temporary view:\n",
      "+----+---+----------+\n",
      "|Name|Age|Department|\n",
      "+----+---+----------+\n",
      "|John| 30|        IT|\n",
      "|Jane| 25|        HR|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a temporary view from DataFrame\n",
    "# Note: We already created the 'employees' view in the setup cell above\n",
    "# This is just to demonstrate the method\n",
    "\n",
    "# Create another DataFrame for demonstration\n",
    "sample_df = spark.createDataFrame(\n",
    "    [(\"John\", 30, \"IT\"), (\"Jane\", 25, \"HR\")],\n",
    "    [\"Name\", \"Age\", \"Department\"]\n",
    ")\n",
    "\n",
    "# Create a temporary view\n",
    "sample_df.createOrReplaceTempView(\"sample_employees\")\n",
    "\n",
    "# Now we can query it using SQL\n",
    "print(\"Querying the temporary view:\")\n",
    "spark.sql(\"SELECT * FROM sample_employees\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Query Result:\n",
      "+----------+------------------+-------------+\n",
      "|Department|         AvgSalary|EmployeeCount|\n",
      "+----------+------------------+-------------+\n",
      "|        HR|           61500.0|            2|\n",
      "|        IT|           57500.0|            2|\n",
      "|     Sales|57333.333333333336|            3|\n",
      "+----------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query using SQL\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT Department, AVG(Salary) as AvgSalary, COUNT(*) as EmployeeCount\n",
    "    FROM employees\n",
    "    GROUP BY Department\n",
    "    ORDER BY AvgSalary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Query Result:\")\n",
    "result_sql.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex SQL Query:\n",
      "+-------+----------+------+------------------+\n",
      "|   Name|Department|Salary|       DiffFromAvg|\n",
      "+-------+----------+------+------------------+\n",
      "|Charlie|     Sales| 70000|12666.666666666664|\n",
      "|    Eve|        HR| 65000|            3500.0|\n",
      "|    Bob|        IT| 60000|            2500.0|\n",
      "|  Grace|        HR| 58000|           -3500.0|\n",
      "+-------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More complex SQL query\n",
    "result_complex = spark.sql(\"\"\"\n",
    "    SELECT Name, Department, Salary,\n",
    "           Salary - (SELECT AVG(Salary) FROM employees e2 WHERE e2.Department = employees.Department) as DiffFromAvg\n",
    "    FROM employees\n",
    "    WHERE Salary > 55000\n",
    "    ORDER BY Salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Complex SQL Query:\")\n",
    "result_complex.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Spark SQL Table View Back to DataFrame\n",
    "\n",
    "Once you have a Spark SQL table view, you can convert it back to a DataFrame using `spark.table()` or by querying it with `spark.sql()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Spark SQL table view back to DataFrame:\n",
      "Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Grace| 29|        HR| 58000|  London|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "Converted using spark.sql() - also returns a DataFrame:\n",
      "Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Grace| 29|        HR| 58000|  London|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "Using DataFrame API on the converted DataFrame:\n",
      "+-------+---+----------+------+------+\n",
      "|   Name|Age|Department|Salary|  City|\n",
      "+-------+---+----------+------+------+\n",
      "|Charlie| 35|     Sales| 70000| Tokyo|\n",
      "|    Eve| 32|        HR| 65000|Sydney|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Convert Spark SQL table view to DataFrame using spark.table()\n",
    "df_from_view = spark.table(\"employees\")\n",
    "\n",
    "print(\"Converted Spark SQL table view back to DataFrame:\")\n",
    "print(f\"Type: {type(df_from_view)}\")\n",
    "df_from_view.show()\n",
    "\n",
    "# Method 2: Convert using spark.sql() - returns a DataFrame\n",
    "df_from_sql = spark.sql(\"SELECT * FROM employees\")\n",
    "\n",
    "print(\"\\nConverted using spark.sql() - also returns a DataFrame:\")\n",
    "print(f\"Type: {type(df_from_sql)}\")\n",
    "df_from_sql.show()\n",
    "\n",
    "# Now you can use DataFrame API operations\n",
    "print(\"\\nUsing DataFrame API on the converted DataFrame:\")\n",
    "df_from_view.filter(df_from_view.Salary > 60000).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative View Creation Methods\n",
    "\n",
    "Spark provides several methods to create views from DataFrames, each with different scoping and behavior:\n",
    "\n",
    "1. **createTempView()**: Creates a temporary view (fails if view already exists)\n",
    "2. **createOrReplaceTempView()**: Creates or replaces a temporary view (session-scoped)\n",
    "3. **createGlobalTempView()**: Creates a global temporary view (fails if view already exists)\n",
    "4. **createOrReplaceGlobalTempView()**: Creates or replaces a global temporary view (application-scoped)\n",
    "\n",
    "**Key Differences:**\n",
    "- **Temp views**: Session-scoped (only available in the current SparkSession)\n",
    "- **Global temp views**: Application-scoped (available across all SparkSessions in the same Spark application)\n",
    "- **create vs createOrReplace**: `create` fails if view exists, `createOrReplace` overwrites existing view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary view 'employees_temp' using createTempView()\n",
      "Created/replaced temporary view 'employees_replace' using createOrReplaceTempView()\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: createTempView() - fails if view already exists\n",
    "try:\n",
    "    df.createTempView(\"employees_temp\")\n",
    "    print(\"Created temporary view 'employees_temp' using createTempView()\")\n",
    "except Exception as e:\n",
    "    print(f\"Error (view might already exist): {e}\")\n",
    "\n",
    "# Example: createOrReplaceTempView() - replaces if exists\n",
    "df.createOrReplaceTempView(\"employees_replace\")\n",
    "print(\"Created/replaced temporary view 'employees_replace' using createOrReplaceTempView()\")\n",
    "\n",
    "# Query the temp view\n",
    "spark.sql(\"SELECT * FROM employees_replace LIMIT 3\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created global temporary view using createGlobalTempView()\n",
      "+-----+---+----------+------+--------+\n",
      "| Name|Age|Department|Salary|    City|\n",
      "+-----+---+----------+------+--------+\n",
      "|Alice| 25|     Sales| 50000|New York|\n",
      "|  Bob| 30|        IT| 60000|  London|\n",
      "+-----+---+----------+------+--------+\n",
      "\n",
      "\n",
      "Created/replaced global temporary view using createOrReplaceGlobalTempView()\n",
      "+-----+---+----------+------+--------+\n",
      "| Name|Age|Department|Salary|    City|\n",
      "+-----+---+----------+------+--------+\n",
      "|Alice| 25|     Sales| 50000|New York|\n",
      "|  Bob| 30|        IT| 60000|  London|\n",
      "+-----+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: createGlobalTempView() - creates global view (fails if exists)\n",
    "try:\n",
    "    df.createGlobalTempView(\"global_employees_create\")\n",
    "    print(\"Created global temporary view using createGlobalTempView()\")\n",
    "    # Query global view (note: must use global_temp database prefix)\n",
    "    spark.sql(\"SELECT * FROM global_temp.global_employees_create LIMIT 2\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Error (view might already exist): {e}\")\n",
    "\n",
    "# Example: createOrReplaceGlobalTempView() - replaces if exists\n",
    "df.createOrReplaceGlobalTempView(\"global_employees_replace\")\n",
    "print(\"\\nCreated/replaced global temporary view using createOrReplaceGlobalTempView()\")\n",
    "spark.sql(\"SELECT * FROM global_temp.global_employees_replace LIMIT 2\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: View Creation Methods Comparison\n",
    "\n",
    "| Method | Scope | Behavior if Exists |\n",
    "|--------|-------|-------------------|\n",
    "| `createTempView()` | Session | Fails with error |\n",
    "| `createOrReplaceTempView()` | Session | Replaces existing view |\n",
    "| `createGlobalTempView()` | Application | Fails with error |\n",
    "| `createOrReplaceGlobalTempView()` | Application | Replaces existing view |\n",
    "\n",
    "**When to use each:**\n",
    "- **createOrReplaceTempView()**: Most common - use when you want to ensure the view exists\n",
    "- **createTempView()**: Use when you want to ensure the view doesn't already exist\n",
    "- **Global views**: Use when you need to share views across multiple SparkSessions in the same application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Temporary Views\n",
    "\n",
    "Global temporary views are accessible across Spark sessions (within the same Spark application).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying global temporary view:\n",
      "+-------+---+----------+------+------+\n",
      "|   Name|Age|Department|Salary|  City|\n",
      "+-------+---+----------+------+------+\n",
      "|Charlie| 35|     Sales| 70000| Tokyo|\n",
      "|    Eve| 32|        HR| 65000|Sydney|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create global temporary view\n",
    "df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "\n",
    "# Query global view (note the global_temp prefix)\n",
    "result_global = spark.sql(\"SELECT * FROM global_temp.global_employees WHERE Age > 30\")\n",
    "print(\"Querying global temporary view:\")\n",
    "result_global.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "1. **DataFrames and Spark SQL Tables are Interconvertible**: You can create views from DataFrames and query them with SQL\n",
    "2. **Creating Views**: Using `createOrReplaceTempView()`, `createTempView()`, `createGlobalTempView()`, and `createOrReplaceGlobalTempView()`\n",
    "3. **SQL Queries**: Using `spark.sql()` to execute SQL queries on DataFrames\n",
    "4. **Converting Views to DataFrames**: Using `spark.table()` or `spark.sql()` to convert views back to DataFrames\n",
    "\n",
    "**Key Takeaways**: \n",
    "- Spark SQL allows SQL developers to use familiar SQL syntax with PySpark DataFrames\n",
    "- DataFrames and SQL tables are interconvertible - use the syntax you prefer\n",
    "- Creating views from DataFrames enables powerful SQL-based data processing\n",
    "- Use DataFrame API for most operations, SQL for complex queries\n",
    "\n",
    "**Next Steps**: In Module 6, we'll learn about joins - combining data from multiple DataFrames.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
