{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Introduction & SparkSession\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module covers Apache Spark fundamentals and PySpark, the Python API for Apache Spark. We'll start with understanding what Apache Spark is, its architecture, and then dive into PySpark for practical implementation. This module focuses on DataFrames and Spark SQL - the primary APIs for data engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single node machines or clusters.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Multi-language**: Supports Python, Scala, Java, R, and SQL\n",
    "- **Flexible Deployment**: Works on single machines or distributed clusters\n",
    "- **Unified Platform**: Handles batch processing, streaming, SQL, and machine learning\n",
    "- **High Performance**: In-memory computing for faster processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities\n",
    "\n",
    "Apache Spark provides a comprehensive set of capabilities:\n",
    "\n",
    "- **ANSI SQL**: Full SQL support for querying structured data\n",
    "- **Batch Processing API**: Process large volumes of data in batches\n",
    "- **Stream Processing API**: Real-time data processing with Spark Streaming\n",
    "- **Machine Learning API**: MLlib library for scalable machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Apache Spark?\n",
    "\n",
    "Apache Spark has become the industry standard for big data processing due to several key advantages:\n",
    "\n",
    "- **Abstraction**: High-level APIs that hide the complexity of distributed computing\n",
    "- **Ease of Use**: Simple APIs similar to familiar tools (like Pandas for DataFrames)\n",
    "- **Unified**: Single platform for batch, streaming, SQL, and ML workloads\n",
    "- **Open Source**: Free, community-driven, and continuously improved\n",
    "- **Ecosystem**: Rich ecosystem with integrations for various data sources and tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark System Architecture\n",
    "\n",
    "### Unified Architecture\n",
    "\n",
    "Apache Spark follows a layered architecture where everything funnels into Spark Core:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                        SPARK CONNECT                          │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                    HIGH-LEVEL SPARK APIs                      │\n",
    "│                                                              │\n",
    "│  ┌──────────────┐  ┌──────────────────┐  ┌──────────────┐  │\n",
    "│  │ SQL / DF API │  │ Structured        │  │ Pandas API  │  │\n",
    "│  │              │  │ Streaming         │  │             │  │\n",
    "│  └──────────────┘  └──────────────────┘  └──────────────┘  │\n",
    "│                                                              │\n",
    "│                    ┌──────────────┐                          │\n",
    "│                    │   MLlib      │                          │\n",
    "│                    └──────────────┘                          │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                   LANGUAGE BINDINGS                           │\n",
    "│                                                              │\n",
    "│      Python        Scala         Java           R             │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                 SPARK CORE (RDD API)                          │\n",
    "│                                                              │\n",
    "│   - DAG Scheduler                                             │\n",
    "│   - Task Scheduler                                            │\n",
    "│   - Memory Management                                         │\n",
    "│   - Fault Tolerance                                           │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                  RESOURCE MANAGER LAYER                       │\n",
    "│                                                              │\n",
    "│   Spark Standalone   |   YARN   |   Kubernetes                │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                     COMPUTE CLUSTER                          │\n",
    "│                                                              │\n",
    "│     Hadoop        AWS EC2        Azure VM        GCP VM        │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                       STORAGE LAYER                           │\n",
    "│                                                              │\n",
    "│      HDFS        |        S3        |     ADLS     |   GCS     │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Architecture Principles\n",
    "\n",
    "**Everything funnels into Spark Core**: All APIs (SQL, DataFrame, Streaming, MLlib) ultimately execute through Spark Core.\n",
    "\n",
    "**Language doesn't matter**: Python, Scala, Java, and R all compile to the same JVM-executed plans.\n",
    "\n",
    "**DataFrame API is the primary interface**: DataFrames and Spark SQL are the recommended APIs for data engineering tasks.\n",
    "\n",
    "**Resource Managers don't compute**: They only allocate resources (CPU, memory) to Spark applications.\n",
    "\n",
    "**Storage is passive**: Spark pulls data from storage; storage never pushes data to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Platform\n",
    "\n",
    "### Understanding the Complete Spark Ecosystem\n",
    "\n",
    "**Apache Spark is a framework and library**, but to build, test, deploy, and operate Spark applications in production, you need more than just Spark:\n",
    "\n",
    "- **Resource Manager**: Allocates compute resources (YARN, Kubernetes, Spark Standalone)\n",
    "- **Compute Cluster**: Physical or virtual machines that execute Spark jobs\n",
    "- **Storage**: Data storage systems (HDFS, S3, ADLS, GCS)\n",
    "- **Security & Governance**: Access control, data lineage, compliance\n",
    "- **Monitoring & Operations**: Job monitoring, alerting, performance tuning\n",
    "\n",
    "### Spark Platforms\n",
    "\n",
    "Different vendors package Spark with these components to create complete platforms:\n",
    "\n",
    "- **Databricks**: Unified analytics platform with managed Spark, Delta Lake, MLflow, and Unity Catalog\n",
    "- **AWS EMR**: Amazon's managed Spark service on AWS infrastructure\n",
    "- **Azure HDInsight**: Microsoft's managed Spark service on Azure\n",
    "- **Cloudera Hadoop**: On-premise Hadoop distribution with Spark\n",
    "- **Google Cloud Dataproc**: Google's managed Spark service on GCP\n",
    "\n",
    "**Key Point**: Spark alone is not enough for production. You need a complete platform that combines Spark with resource management, compute infrastructure, storage, security, and governance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Databricks Cloud?\n",
    "\n",
    "**Databricks** is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Managed Open Source Integration**: Seamlessly integrates Apache Spark, MLflow, Delta Lake, Unity Catalog, and PostgreSQL\n",
    "- **Unified Platform**: Combines data engineering, data science, and machine learning in one platform\n",
    "- **Enterprise-Grade**: Built-in security, governance, and compliance features\n",
    "- **Scalable**: Automatically scales compute resources based on workload\n",
    "- **Collaborative**: Team collaboration features for data teams\n",
    "\n",
    "### Databricks Components\n",
    "\n",
    "- **Apache Spark**: Core processing engine\n",
    "- **Delta Lake**: Open-source storage layer with ACID transactions\n",
    "- **MLflow**: Machine learning lifecycle management\n",
    "- **Unity Catalog**: Unified governance for data and AI assets\n",
    "- **Databricks SQL**: Serverless SQL warehouse for analytics\n",
    "\n",
    "**Note**: While Databricks is a popular Spark platform, Spark applications can run on any compatible platform (AWS EMR, Azure HDInsight, on-premise clusters, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is PySpark?\n",
    "\n",
    "**PySpark** is the Python API for Apache Spark. It allows Python developers to leverage Spark's distributed computing capabilities using familiar Python syntax.\n",
    "\n",
    "**Key Features:**\n",
    "- **Distributed Computing**: Process data across multiple machines (cluster)\n",
    "- **In-Memory Processing**: Fast processing by keeping data in memory\n",
    "- **Fault Tolerance**: Automatically recovers from failures\n",
    "- **Lazy Evaluation**: Operations are optimized before execution\n",
    "- **DataFrame API**: Similar to Pandas, optimized for structured data processing\n",
    "\n",
    "**Why PySpark for Data Engineering?**\n",
    "- **Handles Big Data**: Process terabytes/petabytes of data\n",
    "- **Faster than Pandas**: For large datasets, Spark is much faster\n",
    "- **Scalable**: Can scale from single machine to thousands of machines\n",
    "- **Multiple Data Sources**: Read from CSV, JSON, Parquet, Hive, databases, etc.\n",
    "- **Industry Standard**: Used by major companies (Netflix, Uber, Airbnb, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You'll Learn in This Notebook\n",
    "\n",
    "- What is Apache Spark and its architecture?\n",
    "- Why Apache Spark for data engineering?\n",
    "- Understanding Spark platforms and ecosystem\n",
    "- Basic PySpark setup and installation\n",
    "- Creating your first SparkSession\n",
    "- Working with PySpark DataFrames\n",
    "- Creating DataFrames from various file formats (CSV, JSON, Parquet)\n",
    "- Understanding schema inference vs explicit schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark vs Pandas\n",
    "\n",
    "| Feature | Pandas | PySpark |\n",
    "|---------|--------|---------|\n",
    "| **Data Size** | Best for data that fits in memory (single machine) | Handles data larger than memory (distributed) |\n",
    "| **Speed** | Fast for small/medium data | Faster for large data (distributed processing) |\n",
    "| **Scalability** | Single machine | Multiple machines (cluster) |\n",
    "| **Use Case** | Data analysis, ETL on small datasets | Big data processing, ETL on large datasets |\n",
    "| **Syntax** | Python-like | Similar to Pandas (DataFrame API) |\n",
    "\n",
    "**Rule of Thumb**: \n",
    "- Use **Pandas** when data fits in your machine's memory\n",
    "- Use **PySpark** when data is too large or you need distributed processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PySpark\n",
    "\n",
    "PySpark can be installed using pip:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "**Note**: For this course, we'll use PySpark in local mode (single machine). In production, Spark runs on clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.1\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /opt/homebrew/lib/python3.11/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.1 in /opt/homebrew/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/homebrew/lib/python3.11/site-packages (from pyspark==3.5.1) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.1\n",
      "PySpark imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Check if PySpark is installed\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"PySpark version: {pyspark.__version__}\")\n",
    "    print(\"PySpark imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"PySpark not installed. Please run: pip install pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a SparkSession\n",
    "\n",
    "**SparkSession** is the entry point to PySpark. It's similar to a database connection - you need it to work with Spark.\n",
    "\n",
    "### What is SparkSession?\n",
    "\n",
    "**SparkSession** acts as an entry point to the Spark Cluster. To run code on a Spark Cluster, a SparkSession must be created.\n",
    "\n",
    "**Key Points:**\n",
    "- **For Higher-Level APIs**: To work with DataFrames and Spark SQL, SparkSession is required\n",
    "- **For RDD Level**: SparkContext is required (but SparkSession wraps SparkContext)\n",
    "- **Unified Entry Point**: SparkSession acts as an umbrella that encapsulates and unifies different contexts like SparkContext, HiveContext, SQLContext\n",
    "- **One Session Per Application**: Only one SparkSession object is typically created per application\n",
    "\n",
    "### Using Builder Pattern\n",
    "\n",
    "SparkSession uses the **Builder Pattern** for creation, which allows you to configure various settings in a fluent, readable way:\n",
    "\n",
    "**Key Points:**\n",
    "- Use `getOrCreate()` to reuse existing session or create new one\n",
    "- Always stop the session when done (in notebooks, this is usually automatic)\n",
    "- The builder pattern allows chaining configuration options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/30 06:40:44 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/12/30 06:40:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/30 06:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created: <pyspark.sql.session.SparkSession object at 0x112801350>\n",
      "Spark version: 3.5.1\n",
      "Spark context: <SparkContext master=local[*] appName=PySpark Introduction>\n",
      "\n",
      "============================================================\n",
      "Understanding Builder Pattern:\n",
      "============================================================\n",
      "- .builder: Starts the builder\n",
      "- .appName(): Sets application name\n",
      "- .master(): Sets the master URL (local[*] for local mode)\n",
      "- .getOrCreate(): Creates or retrieves existing session\n",
      "\n",
      "Note: Only one SparkSession object is created per application.\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession using Builder Pattern\n",
    "# appName: Name of your application (appears in Spark UI)\n",
    "# master: \"local[*]\" means use all available cores on local machine\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Introduction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check SparkSession\n",
    "print(f\"SparkSession created: {spark}\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark context: {spark.sparkContext}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Understanding Builder Pattern:\")\n",
    "print(\"=\"*60)\n",
    "print(\"- .builder: Starts the builder\")\n",
    "print(\"- .appName(): Sets application name\")\n",
    "print(\"- .master(): Sets the master URL (local[*] for local mode)\")\n",
    "print(\"- .getOrCreate(): Creates or retrieves existing session\")\n",
    "print(\"\\nNote: Only one SparkSession object is created per application.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Spark, the .master() parameter tells Spark where the cluster is and how many resources it should use.\n",
    "\n",
    "When you pass the string \"local[*]\", you are giving Spark two specific instructions:\n",
    "\n",
    "local: This tells Spark not to look for an external cluster (like YARN or Kubernetes). Instead, it tells Spark to run the Driver, the Master, and the Executor all inside a single process on your own machine.\n",
    "\n",
    "[*]: This tells Spark how many CPU cores to use.\n",
    "\n",
    "- local[1]: Use only one core (serial processing).\n",
    "- local[2]: Use two cores.\n",
    "- local[*]: Use all available cores on your machine. This is the most common setting for development because it maximizes your laptop's power.\n",
    "\n",
    "\n",
    "- If Master is local: The \"Cluster Manager\" is just a thread on your laptop.\n",
    "- If Master is yarn or spark://: The SparkSession sends a request over the network to a remote Cluster Manager to ask for \"Workers\" on different servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the exact number of cores your Spark application is using in local[*] mode, you need to access the Spark Web UI. Since Spark runs only as long as your script is active, you must keep your program running (e.g., using a time.sleep or an input prompt) to view the UI.\n",
    "\n",
    "**Step-by-Step: Accessing the Core Count**\n",
    "\n",
    "- Open the Web UI: While your Spark script is running, open your web browser and go to: http://localhost:4040 (If port 4040 is taken, Spark will automatically try 4041, 4042, etc.)\n",
    "- Navigate to the \"Executors\" Tab: In the top navigation menu, click on the Executors tab. This is the \"Resource Dashboard\" for your Spark session.\n",
    "- Find the \"Cores\" Column: In the table below the summary, look for a row labeled driver. In local[*] mode, the driver and the executor are the same process.\n",
    "- Look at the Cores column.\n",
    "- The number displayed there is the total count of threads Spark has successfully claimed from your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SparkContext\n",
    "\n",
    "**SparkContext** is the entry point to Spark's low-level API. SparkSession wraps SparkContext and provides higher-level APIs.\n",
    "\n",
    "- **SparkContext**: Low-level API (RDD operations)\n",
    "- **SparkSession**: High-level API (DataFrame operations) - **We'll use this mostly**\n",
    "\n",
    "For most data engineering tasks, you'll work with SparkSession and DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Multiple Spark Sessions\n",
    "\n",
    "While typically you create one SparkSession per application, Spark allows you to create multiple Spark Sessions when needed.\n",
    "\n",
    "**Important Points:**\n",
    "- **Same SparkContext**: Multiple Spark Sessions created in the same application will share the same underlying SparkContext\n",
    "- **Isolated Environments**: Each SparkSession can have its own isolated environment (different configurations, temporary views, etc.)\n",
    "- **Use Cases**: Useful when you need different configurations for different parts of your application\n",
    "\n",
    "**Example Use Case**: You might want one SparkSession for reading from one data source with specific configurations, and another for writing to a different data source with different settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First SparkSession:\n",
      "App Name: PySpark Introduction\n",
      "SparkContext ID: 4530630736\n",
      "\n",
      "Second SparkSession:\n",
      "App Name: PySpark Introduction\n",
      "SparkContext ID: 4530630736\n",
      "\n",
      "============================================================\n",
      "Key Observation:\n",
      "============================================================\n",
      "Notice that both SparkSessions share the same SparkContext!\n",
      "This is because SparkContext is created once per JVM/application.\n",
      "\n",
      "Every Spark Application has:\n",
      "- One Driver (Master) - where your code runs\n",
      "- Multiple Executors (Workers) - where tasks execute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 21:25:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating Multiple Spark Sessions\n",
    "# Note: In practice, you usually only need one SparkSession\n",
    "\n",
    "# First SparkSession (already created above)\n",
    "print(\"First SparkSession:\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"SparkContext ID: {id(spark.sparkContext)}\")\n",
    "\n",
    "# Create a second SparkSession with different app name\n",
    "spark2 = SparkSession.builder \\\n",
    "    .appName(\"PySpark Second Session\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"\\nSecond SparkSession:\")\n",
    "print(f\"App Name: {spark2.sparkContext.appName}\")\n",
    "print(f\"SparkContext ID: {id(spark2.sparkContext)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Observation:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Notice that both SparkSessions share the same SparkContext!\")\n",
    "print(\"This is because SparkContext is created once per JVM/application.\")\n",
    "print(\"\\nEvery Spark Application has:\")\n",
    "print(\"- One Driver (Master) - where your code runs\")\n",
    "print(\"- Multiple Executors (Workers) - where tasks execute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context: <SparkContext master=local[*] appName=PySpark Introduction>\n",
      "Default parallelism: 11\n",
      "Master URL: local[*]\n"
     ]
    }
   ],
   "source": [
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Get some information about the Spark environment\n",
    "print(f\"Spark Context: {sc}\")\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Master URL: {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First DataFrame\n",
    "\n",
    "Let's create a simple DataFrame to get started. PySpark DataFrames are similar to Pandas DataFrames but are distributed across a cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|    Bob| 30|  London|\n",
      "|Charlie| 35|   Tokyo|\n",
      "|  Diana| 28|   Paris|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple DataFrame from a list of tuples\n",
    "data = [\n",
    "    (\"Alice\", 25, \"New York\"),\n",
    "    (\"Bob\", 30, \"London\"),\n",
    "    (\"Charlie\", 35, \"Tokyo\"),\n",
    "    (\"Diana\", 28, \"Paris\")\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"Name\", \"Age\", \"City\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame created:\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get DataFrame schema (structure)\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Ways of Creating DataFrames\n",
    "\n",
    "There are several ways to create DataFrames in PySpark. Understanding all these methods gives you flexibility in different scenarios. Let's explore each method:\n",
    "\n",
    "### 1. Using `spark.read` (Reading from Files)\n",
    "\n",
    "This is the most common method for reading data from files. We've already seen examples of this with CSV, JSON, and Parquet files.\n",
    "\n",
    "**When to Use**: When you have data stored in files (CSV, JSON, Parquet, etc.)\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created using spark.read:\n",
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|    Bob| 30|  London|\n",
      "|Charlie| 35|   Tokyo|\n",
      "+-------+---+--------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Using spark.read to create DataFrame from CSV\n",
    "# This is the most common way to read data from files\n",
    "\n",
    "# Create a sample CSV file for demonstration\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "sample_csv = \"\"\"Name,Age,City\n",
    "Alice,25,New York\n",
    "Bob,30,London\n",
    "Charlie,35,Tokyo\"\"\"\n",
    "\n",
    "with open(\"data/sample_read.csv\", \"w\") as f:\n",
    "    f.write(sample_csv)\n",
    "\n",
    "# Read using spark.read\n",
    "df_read = spark.read.csv(\"data/sample_read.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"DataFrame created using spark.read:\")\n",
    "df_read.show()\n",
    "df_read.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using `spark.sql` (Executing SQL Queries)\n",
    "\n",
    "You can create DataFrames by executing SQL queries. This is useful when you want to use SQL syntax or query existing tables/views.\n",
    "\n",
    "**When to Use**: \n",
    "- When you prefer SQL syntax over DataFrame API\n",
    "- When querying existing tables or views\n",
    "- When working with complex queries that are easier to express in SQL\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "df = spark.sql(\"SELECT * FROM employees WHERE age > 25\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created using spark.sql:\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n",
      "\n",
      "DataFrame created using spark.sql with inline VALUES:\n",
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|    Bob| 30|  London|\n",
      "|Charlie| 35|   Tokyo|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Using spark.sql to create DataFrame from SQL query\n",
    "# First, let's create a temporary view from existing DataFrame\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Now we can use spark.sql to query it\n",
    "df_sql = spark.sql(\"SELECT Name, Age FROM people WHERE Age > 28\")\n",
    "\n",
    "print(\"DataFrame created using spark.sql:\")\n",
    "df_sql.show()\n",
    "\n",
    "# You can also use spark.sql with inline data using VALUES\n",
    "df_sql_inline = spark.sql(\"\"\"\n",
    "    SELECT * FROM VALUES \n",
    "    ('Alice', 25, 'New York'),\n",
    "    ('Bob', 30, 'London'),\n",
    "    ('Charlie', 35, 'Tokyo')\n",
    "    AS t(Name, Age, City)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDataFrame created using spark.sql with inline VALUES:\")\n",
    "df_sql_inline.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using `spark.table` (Reading from Tables/Views)\n",
    "\n",
    "You can create DataFrames by reading from existing tables or views in Spark's catalog.\n",
    "\n",
    "**When to Use**: \n",
    "- When you have registered tables or views\n",
    "- When working with Hive tables\n",
    "- When you want to read from a table that was created earlier\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "df = spark.table(\"employees\")  # Reads from a table named \"employees\"\n",
    "df = spark.table(\"database.employees\")  # Reads from a table in a specific database\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created using spark.table:\n",
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|    Bob| 30|  London|\n",
      "|Charlie| 35|   Tokyo|\n",
      "|  Diana| 28|   Paris|\n",
      "+-------+---+--------+\n",
      "\n",
      "\n",
      "Note: spark.table() is equivalent to spark.sql('SELECT * FROM table_name')\n",
      "It's a convenient way to read from registered tables or views.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using spark.table to create DataFrame from a table/view\n",
    "# First, register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"people_table\")\n",
    "\n",
    "# Now read from the table using spark.table\n",
    "df_table = spark.table(\"people_table\")\n",
    "\n",
    "print(\"DataFrame created using spark.table:\")\n",
    "df_table.show()\n",
    "\n",
    "print(\"\\nNote: spark.table() is equivalent to spark.sql('SELECT * FROM table_name')\")\n",
    "print(\"It's a convenient way to read from registered tables or views.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using `spark.range` (Creating Sequential Data)\n",
    "\n",
    "`spark.range()` creates a DataFrame with a single column containing a sequence of numbers. This is useful for generating test data or creating sequences.\n",
    "\n",
    "**Syntax**:\n",
    "```python\n",
    "spark.range(start, end, step, numPartitions)\n",
    "```\n",
    "\n",
    "**Parameters**:\n",
    "- `start`: Starting value (inclusive, default: 0)\n",
    "- `end`: Ending value (exclusive)\n",
    "- `step`: Step size (default: 1)\n",
    "- `numPartitions`: Number of partitions (optional)\n",
    "\n",
    "**Important**: `spark.range()` creates a DataFrame with **one column** named `id` containing the sequence of numbers.\n",
    "\n",
    "**When to Use**: \n",
    "- Generating test data\n",
    "- Creating sequences for joins or iterations\n",
    "- Creating sample data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created using spark.range(10):\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "\n",
      "DataFrame created using spark.range(5, 15):\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n",
      "\n",
      "DataFrame created using spark.range(0, 20, 2) - step of 2:\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "| 10|\n",
      "| 12|\n",
      "| 14|\n",
      "| 16|\n",
      "| 18|\n",
      "+---+\n",
      "\n",
      "\n",
      "Key Point: spark.range() always creates a DataFrame with ONE column named 'id'\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Using spark.range to create DataFrame with sequential numbers\n",
    "# Creates a DataFrame with one column 'id' containing numbers from 0 to 9\n",
    "df_range = spark.range(10)\n",
    "\n",
    "print(\"DataFrame created using spark.range(10):\")\n",
    "df_range.show()\n",
    "df_range.printSchema()\n",
    "\n",
    "# Range with start and end\n",
    "df_range2 = spark.range(5, 15)\n",
    "\n",
    "print(\"\\nDataFrame created using spark.range(5, 15):\")\n",
    "df_range2.show()\n",
    "\n",
    "# Range with step\n",
    "df_range3 = spark.range(0, 20, 2)\n",
    "\n",
    "print(\"\\nDataFrame created using spark.range(0, 20, 2) - step of 2:\")\n",
    "df_range3.show()\n",
    "\n",
    "print(\"\\nKey Point: spark.range() always creates a DataFrame with ONE column named 'id'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using `spark.createDataFrame` (Creating from Local Data)\n",
    "\n",
    "You can create DataFrames from local Python data structures like lists, tuples, or dictionaries. This is useful when you have data in memory.\n",
    "\n",
    "**When to Use**: \n",
    "- When you have data in Python variables (lists, tuples, dictionaries)\n",
    "- When creating test data\n",
    "- When working with small datasets that fit in memory\n",
    "\n",
    "**Three Ways to Use `createDataFrame`**:\n",
    "\n",
    "#### Method 1: Simple List with Column Names\n",
    "```python\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "```\n",
    "\n",
    "#### Method 2: List with Column Names using `.toDF()`\n",
    "```python\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data).toDF(\"Name\", \"Age\")\n",
    "```\n",
    "\n",
    "#### Method 3: List with Explicit Schema\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "```\n",
    "\n",
    "**Key Differences**:\n",
    "- **Method 1**: Column names provided as second argument\n",
    "- **Method 2**: Uses `.toDF()` to specify column names (defaults to `_1`, `_2`, etc. if not specified)\n",
    "- **Method 3**: Explicit schema definition (best for production, gives you control over data types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: spark.createDataFrame(data, column_names)\n",
      "+-----+---+--------+\n",
      "| Name|Age|    City|\n",
      "+-----+---+--------+\n",
      "|Alice| 25|New York|\n",
      "|  Bob| 30|  London|\n",
      "+-----+---+--------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 2a: Without .toDF() - uses default column names:\n",
      "+-----+---+--------+\n",
      "|   _1| _2|      _3|\n",
      "+-----+---+--------+\n",
      "|Alice| 25|New York|\n",
      "|  Bob| 30|  London|\n",
      "+-----+---+--------+\n",
      "\n",
      "\n",
      "Method 2b: With .toDF() to specify column names:\n",
      "+-----+---+--------+\n",
      "| Name|Age|    City|\n",
      "+-----+---+--------+\n",
      "|Alice| 25|New York|\n",
      "|  Bob| 30|  London|\n",
      "+-----+---+--------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "Method 3: spark.createDataFrame(data, schema) - Explicit schema\n",
      "+-----+---+--------+\n",
      "| Name|Age|    City|\n",
      "+-----+---+--------+\n",
      "|Alice| 25|New York|\n",
      "|  Bob| 30|  London|\n",
      "+-----+---+--------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Summary of createDataFrame methods:\n",
      "============================================================\n",
      "1. df = spark.createDataFrame(list, column_names)\n",
      "   → Simple, quick way to specify column names\n",
      "\n",
      "2. df = spark.createDataFrame(list).toDF(column_names)\n",
      "   → Uses .toDF() to specify column names\n",
      "   → If .toDF() is not used, defaults to _1, _2, _3, etc.\n",
      "\n",
      "3. df = spark.createDataFrame(list, schema)\n",
      "   → Best for production - explicit control over data types\n",
      "   → Recommended when you need specific data types\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Using spark.createDataFrame - Method 1: With column names\n",
    "data = [(\"Alice\", 25, \"New York\"), (\"Bob\", 30, \"London\")]\n",
    "\n",
    "# Method 1: Provide column names as second argument\n",
    "df_method1 = spark.createDataFrame(data, [\"Name\", \"Age\", \"City\"])\n",
    "\n",
    "print(\"Method 1: spark.createDataFrame(data, column_names)\")\n",
    "df_method1.show()\n",
    "df_method1.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example 5b: Method 2: Using .toDF() to specify column names\n",
    "# If you don't specify column names, defaults are _1, _2, _3, etc.\n",
    "df_method2_default = spark.createDataFrame(data)\n",
    "print(\"Method 2a: Without .toDF() - uses default column names:\")\n",
    "df_method2_default.show()\n",
    "\n",
    "# With .toDF() to specify column names\n",
    "df_method2 = spark.createDataFrame(data).toDF(\"Name\", \"Age\", \"City\")\n",
    "print(\"\\nMethod 2b: With .toDF() to specify column names:\")\n",
    "df_method2.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example 5c: Method 3: With explicit schema (best for production)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_method3 = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"Method 3: spark.createDataFrame(data, schema) - Explicit schema\")\n",
    "df_method3.show()\n",
    "df_method3.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary of createDataFrame methods:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. df = spark.createDataFrame(list, column_names)\")\n",
    "print(\"   → Simple, quick way to specify column names\")\n",
    "print(\"\\n2. df = spark.createDataFrame(list).toDF(column_names)\")\n",
    "print(\"   → Uses .toDF() to specify column names\")\n",
    "print(\"   → If .toDF() is not used, defaults to _1, _2, _3, etc.\")\n",
    "print(\"\\n3. df = spark.createDataFrame(list, schema)\")\n",
    "print(\"   → Best for production - explicit control over data types\")\n",
    "print(\"   → Recommended when you need specific data types\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Different Ways to Create DataFrames\n",
    "\n",
    "| Method | Use Case | Example |\n",
    "|--------|----------|---------|\n",
    "| `spark.read` | Reading from files | `spark.read.csv(\"file.csv\")` |\n",
    "| `spark.sql` | Executing SQL queries | `spark.sql(\"SELECT * FROM table\")` |\n",
    "| `spark.table` | Reading from tables/views | `spark.table(\"employees\")` |\n",
    "| `spark.range` | Creating sequential numbers | `spark.range(10)` (creates one column) |\n",
    "| `spark.createDataFrame` | Creating from local data | `spark.createDataFrame(data, schema)` |\n",
    "\n",
    "**Key Takeaways**:\n",
    "- **`spark.read`**: Most common for reading files (CSV, JSON, Parquet, etc.)\n",
    "- **`spark.sql`**: Great for SQL-based queries and working with existing tables\n",
    "- **`spark.table`**: Convenient way to read from registered tables/views\n",
    "- **`spark.range`**: Useful for generating test data or sequences (creates one column DataFrame)\n",
    "- **`spark.createDataFrame`**: Essential for creating DataFrames from in-memory Python data\n",
    "\n",
    "**Best Practice**: Use explicit schema with `createDataFrame` for production code to ensure correct data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4\n",
      "Number of columns: 3\n",
      "Column names: ['Name', 'Age', 'City']\n"
     ]
    }
   ],
   "source": [
    "# Get basic information about the DataFrame\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"Column names: {df.columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame with Schema\n",
    "\n",
    "You can also create DataFrames with explicit schema definition. This is useful when you want to control data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   Name|Age| Salary|\n",
      "+-------+---+-------+\n",
      "|  Alice| 25|50000.0|\n",
      "|    Bob| 30|60000.0|\n",
      "|Charlie| 35|70000.0|\n",
      "+-------+---+-------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with schema\n",
    "data = [\n",
    "    (\"Alice\", 25, 50000.0),\n",
    "    (\"Bob\", 30, 60000.0),\n",
    "    (\"Charlie\", 35, 70000.0)\n",
    "]\n",
    "\n",
    "df_with_schema = spark.createDataFrame(data, schema)\n",
    "df_with_schema.show()\n",
    "df_with_schema.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame from Files\n",
    "\n",
    "PySpark can read data from various file formats. Let's explore reading from CSV, JSON, and Parquet files.\n",
    "\n",
    "### Standardized Reading Pattern\n",
    "\n",
    "PySpark uses a consistent pattern for reading files that helps build a clear mental model:\n",
    "\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferschema\", \"true\") \\\n",
    "    .load(\"<file-path>\")\n",
    "```\n",
    "\n",
    "**Pattern Components:**\n",
    "- `spark.read`: Entry point for reading data\n",
    "- `.format()`: Specifies the file format (csv, json, parquet, etc.)\n",
    "- `.option()`: Sets format-specific options (header, inferschema, delimiter, etc.)\n",
    "- `.schema()`: Optional - defines explicit schema (better for production)\n",
    "- `.load()`: Specifies the file or directory path\n",
    "\n",
    "**Note**: Option names are lowercase (e.g., `\"inferschema\"` not `\"inferSchema\"`).\n",
    "\n",
    "**Key Points:**\n",
    "- **CSV**: Common format, can infer schema automatically or use explicit schema\n",
    "- **JSON**: Structured format, schema is usually inferred from the data\n",
    "- **Parquet**: Columnar format, very efficient for analytics workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from JSON File\n",
    "\n",
    "JSON files are commonly used for structured data. PySpark can automatically infer the schema from JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from JSON:\n",
      "+---------------+----+-----------+-----------+--------------+------+\n",
      "|_corrupt_record| age| department|employee_id|          name|salary|\n",
      "+---------------+----+-----------+-----------+--------------+------+\n",
      "|              [|NULL|       NULL|       NULL|          NULL|  NULL|\n",
      "|           NULL|  28|Engineering|          1|      John Doe| 75000|\n",
      "|           NULL|  32|  Marketing|          2|    Jane Smith| 65000|\n",
      "|           NULL|  45|      Sales|          3|   Bob Johnson| 80000|\n",
      "|           NULL|  29|Engineering|          4|Alice Williams| 72000|\n",
      "|           NULL|  38|         HR|          5| Charlie Brown| 60000|\n",
      "|           NULL|  35|  Marketing|          6|  Diana Prince| 68000|\n",
      "|           NULL|  42|      Sales|          7|  Frank Miller| 85000|\n",
      "|           NULL|  31|Engineering|          8|     Grace Lee| 74000|\n",
      "|              ]|NULL|       NULL|       NULL|          NULL|  NULL|\n",
      "+---------------+----+-----------+-----------+--------------+------+\n",
      "\n",
      "\n",
      "Schema (automatically inferred):\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file using standardized format\n",
    "# PySpark automatically infers schema from JSON\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"data/employees.json\")\n",
    "\n",
    "print(\"DataFrame from JSON:\")\n",
    "json_df.show()\n",
    "\n",
    "print(\"\\nSchema (automatically inferred):\")\n",
    "json_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from CSV File\n",
    "\n",
    "CSV files are the most common format for data exchange. PySpark can:\n",
    "- **Infer schema automatically** (reads a sample of data to determine types)\n",
    "- **Use explicit schema** (better performance and type control)\n",
    "\n",
    "**Note**: Schema inference requires reading the data twice (once to infer, once to process), so for production, it's better to define schema explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Age', 'Department', 'Salary']\n"
     ]
    }
   ],
   "source": [
    "print(csv_df_inferred.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from CSV (with inferred schema):\n",
      "+-------+---+----------+------+\n",
      "|   Name|Age|Department|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Alice| 25|     Sales| 50000|\n",
      "|    Bob| 30|        IT| 60000|\n",
      "|Charlie| 35|     Sales| 70000|\n",
      "|  Diana| 28|        IT| 55000|\n",
      "|    Eve| 32|     Sales| 65000|\n",
      "|  Frank| 27|        HR| 52000|\n",
      "|  Grace| 29|        IT| 58000|\n",
      "|  Henry| 31|     Sales| 62000|\n",
      "+-------+---+----------+------+\n",
      "\n",
      "\n",
      "Inferred Schema:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n",
      "\n",
      "Data types:\n",
      "Name: StringType()\n",
      "Age: IntegerType()\n",
      "Salary: IntegerType()\n"
     ]
    }
   ],
   "source": [
    "# Read CSV with schema inference using standardized format\n",
    "# format(\"csv\") specifies the file format\n",
    "# option(\"header\", \"true\") means first row contains column names\n",
    "# option(\"inferschema\", \"true\") tells Spark to automatically detect data types\n",
    "# load() specifies the file path\n",
    "csv_df_inferred = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferschema\", \"true\") \\\n",
    "    .load(\"data/employees.csv\")\n",
    "\n",
    "print(\"DataFrame from CSV (with inferred schema):\")\n",
    "csv_df_inferred.show()\n",
    "\n",
    "print(\"\\nInferred Schema:\")\n",
    "csv_df_inferred.printSchema()\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "# Use the names that appeared in your .columns list\n",
    "print(f\"Name: {csv_df_inferred.schema['Name'].dataType}\")\n",
    "print(f\"Age: {csv_df_inferred.schema['Age'].dataType}\")\n",
    "print(f\"Salary: {csv_df_inferred.schema['Salary'].dataType}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from CSV (with explicit schema):\n",
      "+-----------+----+----+----------+------+\n",
      "|employee_id|name| age|department|salary|\n",
      "+-----------+----+----+----------+------+\n",
      "|       NULL|  25|NULL|     50000|  NULL|\n",
      "|       NULL|  30|NULL|     60000|  NULL|\n",
      "|       NULL|  35|NULL|     70000|  NULL|\n",
      "|       NULL|  28|NULL|     55000|  NULL|\n",
      "|       NULL|  32|NULL|     65000|  NULL|\n",
      "|       NULL|  27|NULL|     52000|  NULL|\n",
      "|       NULL|  29|NULL|     58000|  NULL|\n",
      "|       NULL|  31|NULL|     62000|  NULL|\n",
      "+-----------+----+----+----------+------+\n",
      "\n",
      "\n",
      "Explicit Schema:\n",
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 21:29:45 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 5\n",
      "CSV file: file:///Users/rohityadav/ry_workspace/dev_de_tr/12%20Pyspark%20Structured/data/employees.csv\n"
     ]
    }
   ],
   "source": [
    "# Read CSV with explicit schema using standardized format (better for production)\n",
    "# This is faster and more reliable than schema inference\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "csv_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "csv_df_explicit = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(csv_schema) \\\n",
    "    .load(\"data/employees.csv\")\n",
    "\n",
    "print(\"DataFrame from CSV (with explicit schema):\")\n",
    "csv_df_explicit.show()\n",
    "\n",
    "print(\"\\nExplicit Schema:\")\n",
    "csv_df_explicit.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Differences: Schema Inference vs Explicit Schema**\n",
    "\n",
    "| Aspect | Schema Inference | Explicit Schema |\n",
    "|--------|------------------|----------------|\n",
    "| **Performance** | Slower (reads data twice) | Faster (single read) |\n",
    "| **Reliability** | May infer wrong types | Guaranteed correct types |\n",
    "| **Use Case** | Exploration, prototyping | Production, large datasets |\n",
    "| **Code** | Less code | More code (define schema) |\n",
    "\n",
    "**Best Practice**: Use explicit schema in production for better performance and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Parquet File\n",
    "\n",
    "Parquet is a columnar storage format optimized for analytics workloads. It's the preferred format for big data processing because:\n",
    "- **Efficient**: Columnar format allows reading only needed columns\n",
    "- **Compressed**: Automatic compression reduces storage\n",
    "- **Schema embedded**: Schema is stored in the file metadata\n",
    "- **Partitioned**: Can be split into multiple files (parts) for parallel processing\n",
    "\n",
    "**Note**: Parquet files are often stored as directories with multiple part files, which is normal and allows parallel reading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file created with multiple parts in data/products.parquet/\n",
      "Parquet files are stored as directories with part files for parallel processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# First, let's create a Parquet file with multiple parts for demonstration\n",
    "# This simulates how Parquet files are typically stored in production\n",
    "\n",
    "# Create sample product data\n",
    "product_data = [\n",
    "    (1, \"Product A\", 100, \"Electronics\"),\n",
    "    (2, \"Product B\", 200, \"Clothing\"),\n",
    "    (3, \"Product C\", 150, \"Electronics\"),\n",
    "    (4, \"Product D\", 300, \"Home\"),\n",
    "    (5, \"Product E\", 250, \"Clothing\"),\n",
    "    (6, \"Product F\", 180, \"Electronics\"),\n",
    "    (7, \"Product G\", 220, \"Home\"),\n",
    "    (8, \"Product H\", 120, \"Clothing\"),\n",
    "    (9, \"Product I\", 350, \"Electronics\"),\n",
    "    (10, \"Product J\", 280, \"Home\")\n",
    "]\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "products_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# Write to Parquet with 2 partitions (creates 2 part files)\n",
    "# This is how Parquet files are typically stored - as directories with multiple parts\n",
    "products_df.coalesce(2).write.mode(\"overwrite\").parquet(\"data/products.parquet\")\n",
    "\n",
    "print(\"Parquet file created with multiple parts in data/products.parquet/\")\n",
    "print(\"Parquet files are stored as directories with part files for parallel processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from Parquet:\n",
      "+----------+------------+-----+-----------+\n",
      "|product_id|product_name|price|   category|\n",
      "+----------+------------+-----+-----------+\n",
      "|         5|   Product E|  250|   Clothing|\n",
      "|         6|   Product F|  180|Electronics|\n",
      "|         7|   Product G|  220|       Home|\n",
      "|         8|   Product H|  120|   Clothing|\n",
      "|         9|   Product I|  350|Electronics|\n",
      "|        10|   Product J|  280|       Home|\n",
      "|         1|   Product A|  100|Electronics|\n",
      "|         2|   Product B|  200|   Clothing|\n",
      "|         3|   Product C|  150|Electronics|\n",
      "|         4|   Product D|  300|       Home|\n",
      "+----------+------------+-----+-----------+\n",
      "\n",
      "\n",
      "Schema (preserved from Parquet file):\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n",
      "\n",
      "Number of rows: 10\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Read Parquet file using standardized format\n",
    "# Parquet files preserve schema, so no need to specify it\n",
    "parquet_df = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"data/products.parquet\")\n",
    "\n",
    "print(\"DataFrame from Parquet:\")\n",
    "parquet_df.show()\n",
    "\n",
    "print(\"\\nSchema (preserved from Parquet file):\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "print(f\"\\nNumber of rows: {parquet_df.count()}\")\n",
    "print(f\"Number of partitions: {parquet_df.rdd.getNumPartitions()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Parquet Part Files**\n",
    "\n",
    "When you write a DataFrame to Parquet, Spark creates a directory with multiple part files:\n",
    "- Each part file contains a portion of the data\n",
    "- This allows parallel reading and processing\n",
    "- The number of parts depends on the number of partitions in your DataFrame\n",
    "- This is normal and expected behavior - you read the directory, not individual files\n",
    "\n",
    "**Example**: `data/products.parquet/` contains:\n",
    "- `part-00000-*.parquet` (first partition)\n",
    "- `part-00001-*.parquet` (second partition)\n",
    "- `_SUCCESS` (indicates successful write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet directory contents:\n",
      "  - .part-00001-3e6ca9b2-190b-4d41-95c8-84652e85a7aa-c000.snappy.parquet.crc\n",
      "  - ._SUCCESS.crc\n",
      "  - part-00001-3e6ca9b2-190b-4d41-95c8-84652e85a7aa-c000.snappy.parquet\n",
      "  - .part-00000-3e6ca9b2-190b-4d41-95c8-84652e85a7aa-c000.snappy.parquet.crc\n",
      "  - _SUCCESS\n",
      "  - part-00000-3e6ca9b2-190b-4d41-95c8-84652e85a7aa-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Verify the Parquet directory structure\n",
    "import os\n",
    "\n",
    "parquet_path = \"data/products.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"Parquet directory contents:\")\n",
    "    for item in os.listdir(parquet_path):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"Parquet directory not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Lazy Evaluation\n",
    "\n",
    "**Key Concept**: Spark uses **lazy evaluation**. This means:\n",
    "\n",
    "- **Transformations** (like `filter`, `select`, `groupBy`) are NOT executed immediately\n",
    "- They are recorded as a plan\n",
    "- **Actions** (like `show()`, `count()`, `collect()`) trigger the actual execution\n",
    "- Spark optimizes the entire plan before executing\n",
    "\n",
    "**Why Lazy Evaluation?**\n",
    "- Allows Spark to optimize the entire query plan\n",
    "- Can combine multiple operations efficiently\n",
    "- Only executes what's needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is just a plan, not executed yet!\n",
      "Type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "Now executing the plan:\n",
      "+-------+---+------+\n",
      "|   Name|Age|  City|\n",
      "+-------+---+------+\n",
      "|    Bob| 30|London|\n",
      "|Charlie| 35| Tokyo|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of lazy evaluation\n",
    "# This doesn't execute immediately - it just creates a plan\n",
    "filtered_df = df.filter(df.Age > 28)\n",
    "\n",
    "print(\"This is just a plan, not executed yet!\")\n",
    "print(f\"Type: {type(filtered_df)}\")\n",
    "\n",
    "# Only when we call an ACTION (like show()), the execution happens\n",
    "print(\"\\nNow executing the plan:\")\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **What is PySpark**: Python API for Apache Spark, a distributed computing framework\n",
    "2. **Why PySpark**: Handles big data, scalable, faster for large datasets\n",
    "3. **SparkSession**: Entry point to PySpark (like a database connection)\n",
    "4. **DataFrames**: Distributed data structures similar to Pandas DataFrames\n",
    "5. **Lazy Evaluation**: Operations are optimized before execution\n",
    "6. **Creating DataFrames**: \n",
    "   - From lists (in-memory data)\n",
    "   - From CSV files (with schema inference or explicit schema)\n",
    "   - From JSON files (schema automatically inferred)\n",
    "   - From Parquet files (schema preserved, efficient columnar format)\n",
    "7. **Schema Management**: Understanding when to use schema inference vs explicit schema\n",
    "\n",
    "**Key Takeaways**: \n",
    "- PySpark is designed for big data processing. It uses lazy evaluation to optimize operations and can scale from a single machine to thousands of machines.\n",
    "- Different file formats have different characteristics: CSV is common but slower, JSON is good for structured data, Parquet is optimal for analytics workloads.\n",
    "- Schema inference is convenient for exploration but explicit schema is better for production.\n",
    "\n",
    "**Next Steps**: In Module 2, we'll learn about reading and writing data from various file formats.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
