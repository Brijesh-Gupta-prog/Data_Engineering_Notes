{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Data Transformations & Aggregations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module covers advanced DataFrame transformations including grouping, aggregations, column manipulation, and null value handling. These operations are essential for data analysis and ETL tasks.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Grouping data and performing aggregations\n",
    "- Adding and renaming columns\n",
    "- Handling null values\n",
    "- Combining multiple transformations\n",
    "- Common aggregation functions\n",
    "- Conditional logic with when/otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 21:35:33 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/12/28 21:35:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/28 21:35:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 21:35:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/28 21:35:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/28 21:35:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/12/28 21:35:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/12/28 21:35:33 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Grace| 29|      NULL| 58000|  London|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, when, sum, avg, count, max, min\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Transformations & Aggregations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"New York\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"London\"),\n",
    "    (\"Charlie\", 35, \"Sales\", 70000, \"Tokyo\"),\n",
    "    (\"Diana\", 28, \"IT\", 55000, \"Paris\"),\n",
    "    (\"Eve\", 32, \"HR\", 65000, \"Sydney\"),\n",
    "    (\"Frank\", 27, \"Sales\", 52000, \"New York\"),\n",
    "    (\"Grace\", 29, None, 58000, \"London\")  # Department is null\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregations\n",
    "\n",
    "Group data by one or more columns and perform aggregations. Similar to SQL's GROUP BY clause.\n",
    "\n",
    "### Aggregate Functions\n",
    "\n",
    "**Aggregate functions** combine multiple input rows together to give a consolidated output. They are essential for summarizing data and performing calculations across groups of rows.\n",
    "\n",
    "PySpark provides three different styles for writing aggregations:\n",
    "\n",
    "1. **Programmatic Style**: Using PySpark functions and column objects\n",
    "2. **Column Expression Style**: Using `expr()` or `selectExpr()` with SQL-like expressions\n",
    "3. **Spark SQL Style**: Using SQL queries directly with `spark.sql()`\n",
    "\n",
    "Each style has its advantages and use cases. Let's explore all three approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count by Department:\n",
      "+----------+-----+\n",
      "|Department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|        IT|    2|\n",
      "|        HR|    1|\n",
      "|      NULL|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by Department and count\n",
    "df_grouped = df.groupBy(\"Department\").count()\n",
    "print(\"Count by Department:\")\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple aggregations by Department:\n",
      "+----------+-------------+------------------+-----------+---------+---------+\n",
      "|Department|EmployeeCount|         AvgSalary|TotalSalary|MaxSalary|MinSalary|\n",
      "+----------+-------------+------------------+-----------+---------+---------+\n",
      "|     Sales|            3|57333.333333333336|     172000|    70000|    50000|\n",
      "|        IT|            2|           57500.0|     115000|    60000|    55000|\n",
      "|        HR|            1|           65000.0|      65000|    65000|    65000|\n",
      "|      NULL|            1|           58000.0|      58000|    58000|    58000|\n",
      "+----------+-------------+------------------+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple aggregations using agg()\n",
    "from pyspark.sql.functions import avg, sum, max, min\n",
    "\n",
    "df_agg = df.groupBy(\"Department\").agg(\n",
    "    count(\"Name\").alias(\"EmployeeCount\"),\n",
    "    avg(\"Salary\").alias(\"AvgSalary\"),\n",
    "    sum(\"Salary\").alias(\"TotalSalary\"),\n",
    "    max(\"Salary\").alias(\"MaxSalary\"),\n",
    "    min(\"Salary\").alias(\"MinSalary\")\n",
    ")\n",
    "\n",
    "print(\"Multiple aggregations by Department:\")\n",
    "df_agg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Aggregations - Three Different Styles\n",
    "\n",
    "Consider you have an orders dataset and you are required to:\n",
    "- Count the total number of records\n",
    "- Count number of distinct invoice IDs\n",
    "- Sum of Quantities\n",
    "- Average unit price\n",
    "\n",
    "Let's see 3 ways of solving the above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. PROGRAMMATIC STYLE\n",
      "======================================================================\n",
      "Using PySpark functions and column objects:\n",
      "\n",
      "+---------+--------------+--------------+---------+\n",
      "|row_count|unique_invoice|total_quantity|avg_price|\n",
      "+---------+--------------+--------------+---------+\n",
      "|        5|             3|            15|     16.6|\n",
      "+---------+--------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Simple Aggregations - Three Styles\n",
    "# Note: This example uses a sample orders DataFrame structure\n",
    "\n",
    "# Create sample orders data for demonstration\n",
    "orders_data = [\n",
    "    (\"INV001\", 5, 10.5, \"USA\"),\n",
    "    (\"INV001\", 3, 15.0, \"USA\"),\n",
    "    (\"INV002\", 2, 20.0, \"UK\"),\n",
    "    (\"INV003\", 4, 12.5, \"USA\"),\n",
    "    (\"INV002\", 1, 25.0, \"UK\"),\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, [\"invoiceno\", \"quantity\", \"unitprice\", \"country\"])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"1. PROGRAMMATIC STYLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using PySpark functions and column objects:\")\n",
    "print()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "result_programmatic = orders_df.select(\n",
    "    count(\"*\").alias(\"row_count\"),\n",
    "    countDistinct(\"invoiceno\").alias(\"unique_invoice\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    avg(\"unitprice\").alias(\"avg_price\")\n",
    ")\n",
    "\n",
    "result_programmatic.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "2. COLUMN EXPRESSION STYLE\n",
      "======================================================================\n",
      "Using selectExpr() with SQL-like expressions:\n",
      "\n",
      "+---------+--------------+--------------+---------+\n",
      "|row_count|unique_invoice|total_quantity|avg_price|\n",
      "+---------+--------------+--------------+---------+\n",
      "|        5|             3|            15|     16.6|\n",
      "+---------+--------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"2. COLUMN EXPRESSION STYLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using selectExpr() with SQL-like expressions:\")\n",
    "print()\n",
    "\n",
    "result_expr = orders_df.selectExpr(\n",
    "    \"count(*) as row_count\",\n",
    "    \"count(distinct(invoiceno)) as unique_invoice\",\n",
    "    \"sum(quantity) as total_quantity\",\n",
    "    \"avg(unitprice) as avg_price\"\n",
    ")\n",
    "\n",
    "result_expr.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "3. SPARK SQL STYLE\n",
      "======================================================================\n",
      "Using SQL queries directly:\n",
      "\n",
      "+---------+--------------+--------------+---------+\n",
      "|row_count|unique_invoice|total_quantity|avg_price|\n",
      "+---------+--------------+--------------+---------+\n",
      "|        5|             3|            15|     16.6|\n",
      "+---------+--------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"3. SPARK SQL STYLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using SQL queries directly:\")\n",
    "print()\n",
    "\n",
    "# Create a temporary view\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        count(*) as row_count,\n",
    "        count(distinct(invoiceno)) as unique_invoice,\n",
    "        sum(quantity) as total_quantity,\n",
    "        avg(unitprice) as avg_price\n",
    "    FROM orders\n",
    "\"\"\")\n",
    "\n",
    "result_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Aggregations - Three Different Styles\n",
    "\n",
    "Consider you have an orders dataset and you are required to group based on invoice number and country:\n",
    "- Find the total quantity for each group\n",
    "- Find the total invoice amount (Amount = Quantity * UnitPrice)\n",
    "\n",
    "Let's see 3 ways of solving the above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. PROGRAMMATIC STYLE\n",
      "======================================================================\n",
      "Using PySpark functions with groupBy and agg():\n",
      "\n",
      "+-------+---------+--------------+-------------+\n",
      "|country|invoiceno|total_quantity|invoice_value|\n",
      "+-------+---------+--------------+-------------+\n",
      "|    USA|   INV001|             8|         97.5|\n",
      "|     UK|   INV002|             3|         65.0|\n",
      "|    USA|   INV003|             4|         50.0|\n",
      "+-------+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Grouping Aggregations - Three Styles\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"1. PROGRAMMATIC STYLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using PySpark functions with groupBy and agg():\")\n",
    "print()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "summary_df_programmatic = orders_df \\\n",
    "    .groupBy(\"country\", \"invoiceno\") \\\n",
    "    .agg(\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        sum(expr(\"quantity * unitprice\")).alias(\"invoice_value\")\n",
    "    ) \\\n",
    "    .sort(\"invoiceno\")\n",
    "\n",
    "summary_df_programmatic.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "2. COLUMN EXPRESSION STYLE\n",
      "======================================================================\n",
      "Using expr() with groupBy and agg():\n",
      "\n",
      "+-------+---------+--------------+-------------+\n",
      "|country|invoiceno|total_quantity|invoice_value|\n",
      "+-------+---------+--------------+-------------+\n",
      "|    USA|   INV001|             8|         97.5|\n",
      "|     UK|   INV002|             3|         65.0|\n",
      "|    USA|   INV003|             4|         50.0|\n",
      "+-------+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"2. COLUMN EXPRESSION STYLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using expr() with groupBy and agg():\")\n",
    "print()\n",
    "\n",
    "summary_df_expr = orders_df \\\n",
    "    .groupBy(\"country\", \"invoiceno\") \\\n",
    "    .agg(\n",
    "        expr(\"sum(quantity) as total_quantity\"),\n",
    "        expr(\"sum(quantity * unitprice) as invoice_value\")\n",
    "    ) \\\n",
    "    .sort(\"invoiceno\")\n",
    "\n",
    "summary_df_expr.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "3. SPARK SQL STYLE\n",
      "======================================================================\n",
      "Using SQL queries with GROUP BY:\n",
      "\n",
      "+-------+---------+--------------+-------------+\n",
      "|country|invoiceno|total_quantity|invoice_value|\n",
      "+-------+---------+--------------+-------------+\n",
      "|    USA|   INV001|             8|         97.5|\n",
      "|     UK|   INV002|             3|         65.0|\n",
      "|    USA|   INV003|             4|         50.0|\n",
      "+-------+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"3. SPARK SQL STYLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using SQL queries with GROUP BY:\")\n",
    "print()\n",
    "\n",
    "summary_df_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country, \n",
    "        invoiceno, \n",
    "        sum(quantity) as total_quantity, \n",
    "        sum(quantity * unitprice) as invoice_value\n",
    "    FROM orders\n",
    "    GROUP BY country, invoiceno\n",
    "    ORDER BY invoiceno\n",
    "\"\"\")\n",
    "\n",
    "summary_df_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and Renaming Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Bonus column:\n",
      "+-------+---+----------+------+--------+------+\n",
      "|   Name|Age|Department|Salary|    City| Bonus|\n",
      "+-------+---+----------+------+--------+------+\n",
      "|  Alice| 25|     Sales| 50000|New York|5000.0|\n",
      "|    Bob| 30|        IT| 60000|  London|6000.0|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|7000.0|\n",
      "|  Diana| 28|        IT| 55000|   Paris|5500.0|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|6500.0|\n",
      "|  Frank| 27|     Sales| 52000|New York|5200.0|\n",
      "|  Grace| 29|      NULL| 58000|  London|5800.0|\n",
      "+-------+---+----------+------+--------+------+\n",
      "\n",
      "\n",
      "Renamed Department to Dept:\n",
      "+-------+---+-----+------+--------+\n",
      "|   Name|Age| Dept|Salary|    City|\n",
      "+-------+---+-----+------+--------+\n",
      "|  Alice| 25|Sales| 50000|New York|\n",
      "|    Bob| 30|   IT| 60000|  London|\n",
      "|Charlie| 35|Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|   IT| 55000|   Paris|\n",
      "|    Eve| 32|   HR| 65000|  Sydney|\n",
      "|  Frank| 27|Sales| 52000|New York|\n",
      "|  Grace| 29| NULL| 58000|  London|\n",
      "+-------+---+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column\n",
    "df_with_bonus = df.withColumn(\"Bonus\", col(\"Salary\") * 0.1)\n",
    "print(\"DataFrame with Bonus column:\")\n",
    "df_with_bonus.show()\n",
    "\n",
    "# Rename a column\n",
    "df_renamed = df.withColumnRenamed(\"Department\", \"Dept\")\n",
    "print(\"\\nRenamed Department to Dept:\")\n",
    "df_renamed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with SalaryCategory:\n",
      "+-------+---+----------+------+--------+--------------+\n",
      "|   Name|Age|Department|Salary|    City|SalaryCategory|\n",
      "+-------+---+----------+------+--------+--------------+\n",
      "|  Alice| 25|     Sales| 50000|New York|           Low|\n",
      "|    Bob| 30|        IT| 60000|  London|        Medium|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|          High|\n",
      "|  Diana| 28|        IT| 55000|   Paris|           Low|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|        Medium|\n",
      "|  Frank| 27|     Sales| 52000|New York|           Low|\n",
      "|  Grace| 29|      NULL| 58000|  London|        Medium|\n",
      "+-------+---+----------+------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add column with conditional logic\n",
    "df_with_category = df.withColumn(\n",
    "    \"SalaryCategory\",\n",
    "    when(col(\"Salary\") > 65000, \"High\")\n",
    "    .when(col(\"Salary\") > 55000, \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ")\n",
    "print(\"DataFrame with SalaryCategory:\")\n",
    "df_with_category.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null Values\n",
    "\n",
    "Dealing with null/missing values is crucial in data engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with non-null Department:\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "+-------+---+----------+------+--------+\n",
      "\n",
      "\n",
      "Filled null Department with 'Unknown':\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "|  Grace| 29|   Unknown| 58000|  London|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out null values\n",
    "df_no_null = df.filter(df.Department.isNotNull())\n",
    "print(\"Rows with non-null Department:\")\n",
    "df_no_null.show()\n",
    "\n",
    "# Fill null values\n",
    "from pyspark.sql.functions import lit\n",
    "df_filled = df.fillna({\"Department\": \"Unknown\"})\n",
    "print(\"\\nFilled null Department with 'Unknown':\")\n",
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows with null Department:\n",
      "+-------+---+----------+------+--------+\n",
      "|   Name|Age|Department|Salary|    City|\n",
      "+-------+---+----------+------+--------+\n",
      "|  Alice| 25|     Sales| 50000|New York|\n",
      "|    Bob| 30|        IT| 60000|  London|\n",
      "|Charlie| 35|     Sales| 70000|   Tokyo|\n",
      "|  Diana| 28|        IT| 55000|   Paris|\n",
      "|    Eve| 32|        HR| 65000|  Sydney|\n",
      "|  Frank| 27|     Sales| 52000|New York|\n",
      "+-------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values\n",
    "df_dropped = df.dropna(subset=[\"Department\"])\n",
    "print(\"Dropped rows with null Department:\")\n",
    "df_dropped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Transformations\n",
    "\n",
    "You can chain multiple transformations together. This is a common pattern in PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chained transformations:\n",
      "+-------+----------+------+------+\n",
      "|   Name|Department|Salary| Bonus|\n",
      "+-------+----------+------+------+\n",
      "|Charlie|     Sales| 70000|7000.0|\n",
      "|    Eve|        HR| 65000|6500.0|\n",
      "|    Bob|        IT| 60000|6000.0|\n",
      "+-------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain multiple transformations\n",
    "result = df \\\n",
    "    .filter(df.Age > 28) \\\n",
    "    .filter(df.Department.isNotNull()) \\\n",
    "    .select(\"Name\", \"Department\", \"Salary\") \\\n",
    "    .withColumn(\"Bonus\", col(\"Salary\") * 0.1) \\\n",
    "    .orderBy(col(\"Salary\").desc())\n",
    "\n",
    "print(\"Chained transformations:\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Aggregation Functions\n",
    "\n",
    "Here are some commonly used aggregation functions:\n",
    "\n",
    "- `count()`: Count number of rows\n",
    "- `sum()`: Sum of values\n",
    "- `avg()` / `mean()`: Average value\n",
    "- `max()`: Maximum value\n",
    "- `min()`: Minimum value\n",
    "- `collect_list()`: Collect values into a list\n",
    "- `collect_set()`: Collect unique values into a set\n",
    "- `stddev()`: Standard deviation\n",
    "- `variance()`: Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Various aggregation functions:\n",
      "+----------+----------+------------------+------------------+-----------------------+\n",
      "|Department|TotalCount|AvgSalary         |SalaryStdDev      |EmployeeNames          |\n",
      "+----------+----------+------------------+------------------+-----------------------+\n",
      "|Sales     |3         |57333.333333333336|11015.141094572204|[Alice, Charlie, Frank]|\n",
      "|IT        |2         |57500.0           |3535.5339059327375|[Bob, Diana]           |\n",
      "|HR        |1         |65000.0           |NULL              |[Eve]                  |\n",
      "|NULL      |1         |58000.0           |NULL              |[Grace]                |\n",
      "+----------+----------+------------------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example with various aggregation functions\n",
    "from pyspark.sql.functions import collect_list, collect_set, stddev\n",
    "\n",
    "df_agg_examples = df.groupBy(\"Department\").agg(\n",
    "    count(\"*\").alias(\"TotalCount\"),\n",
    "    avg(\"Salary\").alias(\"AvgSalary\"),\n",
    "    stddev(\"Salary\").alias(\"SalaryStdDev\"),\n",
    "    collect_list(\"Name\").alias(\"EmployeeNames\")\n",
    ")\n",
    "\n",
    "print(\"Various aggregation functions:\")\n",
    "df_agg_examples.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "1. **Grouping and Aggregations**: Using `groupBy()` and `agg()` for aggregations (SQL GROUP BY equivalent)\n",
    "2. **Adding Columns**: Using `withColumn()` to add new columns\n",
    "3. **Renaming Columns**: Using `withColumnRenamed()`\n",
    "4. **Handling Nulls**: Using `fillna()`, `dropna()`, and `isNotNull()`\n",
    "5. **Combining Transformations**: Chaining multiple transformations for complex data processing\n",
    "6. **Common Aggregation Functions**: `sum()`, `avg()`, `count()`, `max()`, `min()`, and more\n",
    "\n",
    "**Key Takeaway**: Transformations are lazy - they create a plan but don't execute until an action is called. You can chain transformations for readable, efficient code.\n",
    "\n",
    "**Next Steps**: In Module 5, we'll learn about Spark SQL - using SQL syntax to work with DataFrames.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
