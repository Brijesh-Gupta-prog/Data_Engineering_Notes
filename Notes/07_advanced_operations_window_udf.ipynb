{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 - Advanced Operations: Window Functions & UDFs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers advanced PySpark transformations including window functions, user-defined functions (UDFs), and other powerful operations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Window functions\n",
    "- User-defined functions (UDFs)\n",
    "- Pivot and unpivot operations\n",
    "- Union and distinct operations\n",
    "- Advanced column operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 21:42:14 WARN Utils: Your hostname, N-MacBookPro-37.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/12/28 21:42:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/28 21:42:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/12/28 21:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+\n",
      "|   Name|Department|Salary|  Month|\n",
      "+-------+----------+------+-------+\n",
      "|  Alice|     Sales| 50000|2024-01|\n",
      "|    Bob|        IT| 60000|2024-01|\n",
      "|Charlie|     Sales| 70000|2024-01|\n",
      "|  Diana|        IT| 55000|2024-01|\n",
      "|  Alice|     Sales| 52000|2024-02|\n",
      "|    Bob|        IT| 61000|2024-02|\n",
      "|Charlie|     Sales| 72000|2024-02|\n",
      "|  Diana|        IT| 56000|2024-02|\n",
      "+-------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Transformations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"Sales\", 50000, \"2024-01\"),\n",
    "    (\"Bob\", \"IT\", 60000, \"2024-01\"),\n",
    "    (\"Charlie\", \"Sales\", 70000, \"2024-01\"),\n",
    "    (\"Diana\", \"IT\", 55000, \"2024-01\"),\n",
    "    (\"Alice\", \"Sales\", 52000, \"2024-02\"),\n",
    "    (\"Bob\", \"IT\", 61000, \"2024-02\"),\n",
    "    (\"Charlie\", \"Sales\", 72000, \"2024-02\"),\n",
    "    (\"Diana\", \"IT\", 56000, \"2024-02\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"Month\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Window functions perform calculations across a set of rows related to the current row. Unlike `groupBy()`, window functions don't collapse rows.\n",
    "\n",
    "**Common Use Cases:**\n",
    "- Running totals\n",
    "- Rankings\n",
    "- Moving averages\n",
    "- Comparing rows to group aggregates\n",
    "\n",
    "### Windowing Aggregations\n",
    "\n",
    "When working with window functions, you need to define three key parameters:\n",
    "\n",
    "1. **Partition Column**: Partition by based on one or more columns (similar to GROUP BY)\n",
    "2. **Sorting Column**: Sort the data within each partition\n",
    "3. **Window Size**: Define the size by mentioning the start row and end row\n",
    "\n",
    "**Example**: Consider you have a windowdata dataset and you are required to:\n",
    "- Partition by country\n",
    "- Sort based on week number\n",
    "- Define window size (from start to current row)\n",
    "- Find the running total of invoice value\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window specification\n",
    "mywindow = Window.partitionBy(\"country\") \\\n",
    "    .orderBy(\"weeknum\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Apply window function\n",
    "result_df = orders_df.withColumn(\"running_total\", sum(\"invoicevalue\").over(mywindow))\n",
    "result_df.show()\n",
    "```\n",
    "\n",
    "### Common Windowing Functions\n",
    "\n",
    "PySpark provides several windowing functions for different analytical needs:\n",
    "\n",
    "1. **`rank()`**: Assigns ranks to rows within a window partition, with gaps in ranking sequence when there are ties\n",
    "2. **`dense_rank()`**: Assigns ranks to rows within a window partition, without gaps in ranking sequence (consecutive ranks)\n",
    "3. **`row_number()`**: Assigns a unique sequential number to each row within a window partition\n",
    "4. **`lead()`**: Accesses data from a subsequent row in the same window partition\n",
    "5. **`lag()`**: Accesses data from a previous row in the same window partition\n",
    "\n",
    "**Key Differences:**\n",
    "- `rank()` vs `dense_rank()`: `rank()` leaves gaps when there are ties, `dense_rank()` doesn't\n",
    "- `row_number()`: Always assigns unique sequential numbers, even for ties\n",
    "- `lead()` vs `lag()`: `lead()` looks ahead, `lag()` looks behind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+----+\n",
      "|   Name|Department|Salary|  Month|Rank|\n",
      "+-------+----------+------+-------+----+\n",
      "|    Bob|        IT| 61000|2024-02|   1|\n",
      "|    Bob|        IT| 60000|2024-01|   2|\n",
      "|  Diana|        IT| 56000|2024-02|   3|\n",
      "|  Diana|        IT| 55000|2024-01|   4|\n",
      "|Charlie|     Sales| 72000|2024-02|   1|\n",
      "|Charlie|     Sales| 70000|2024-01|   2|\n",
      "|  Alice|     Sales| 52000|2024-02|   3|\n",
      "|  Alice|     Sales| 50000|2024-01|   4|\n",
      "+-------+----------+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window partitioned by Department, ordered by Salary\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Rank employees within each department\n",
    "df_with_rank = df.withColumn(\"Rank\", rank().over(window_spec))\n",
    "df_with_rank.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+------------+\n",
      "|   Name|Department|Salary|  Month|RunningTotal|\n",
      "+-------+----------+------+-------+------------+\n",
      "|    Bob|        IT| 60000|2024-01|       60000|\n",
      "|  Diana|        IT| 55000|2024-01|      115000|\n",
      "|    Bob|        IT| 61000|2024-02|      176000|\n",
      "|  Diana|        IT| 56000|2024-02|      232000|\n",
      "|  Alice|     Sales| 50000|2024-01|       50000|\n",
      "|Charlie|     Sales| 70000|2024-01|      120000|\n",
      "|  Alice|     Sales| 52000|2024-02|      172000|\n",
      "|Charlie|     Sales| 72000|2024-02|      244000|\n",
      "+-------+----------+------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate running total within department\n",
    "window_sum = Window.partitionBy(\"Department\").orderBy(\"Month\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_with_running_total = df.withColumn(\"RunningTotal\", sum(\"Salary\").over(window_sum))\n",
    "df_with_running_total.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+-------------+\n",
      "|   Name|Department|Salary|  Month|DeptAvgSalary|\n",
      "+-------+----------+------+-------+-------------+\n",
      "|    Bob|        IT| 60000|2024-01|      58000.0|\n",
      "|  Diana|        IT| 55000|2024-01|      58000.0|\n",
      "|    Bob|        IT| 61000|2024-02|      58000.0|\n",
      "|  Diana|        IT| 56000|2024-02|      58000.0|\n",
      "|  Alice|     Sales| 50000|2024-01|      61000.0|\n",
      "|Charlie|     Sales| 70000|2024-01|      61000.0|\n",
      "|  Alice|     Sales| 52000|2024-02|      61000.0|\n",
      "|Charlie|     Sales| 72000|2024-02|      61000.0|\n",
      "+-------+----------+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average salary per department (using window)\n",
    "window_avg = Window.partitionBy(\"Department\")\n",
    "\n",
    "df_with_avg = df.withColumn(\"DeptAvgSalary\", avg(\"Salary\").over(window_avg))\n",
    "df_with_avg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Windowing Functions: rank, dense_rank, row_number, lead, lag\n",
    "\n",
    "Let's see examples of the common windowing functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rankings within each Department (ordered by Salary descending):\n",
      "+-------+----------+------+----+----------+----------+\n",
      "|   Name|Department|Salary|rank|dense_rank|row_number|\n",
      "+-------+----------+------+----+----------+----------+\n",
      "|    Bob|        IT| 61000|   1|         1|         1|\n",
      "|    Bob|        IT| 60000|   2|         2|         2|\n",
      "|  Diana|        IT| 56000|   3|         3|         3|\n",
      "|  Diana|        IT| 55000|   4|         4|         4|\n",
      "|Charlie|     Sales| 72000|   1|         1|         1|\n",
      "|Charlie|     Sales| 70000|   2|         2|         2|\n",
      "|  Alice|     Sales| 52000|   3|         3|         3|\n",
      "|  Alice|     Sales| 50000|   4|         4|         4|\n",
      "+-------+----------+------+----+----------+----------+\n",
      "\n",
      "\n",
      "Key Differences:\n",
      "- rank(): Leaves gaps when there are ties (e.g., if two people have same salary)\n",
      "- dense_rank(): No gaps, consecutive ranks even with ties\n",
      "- row_number(): Always unique sequential numbers, even for ties\n"
     ]
    }
   ],
   "source": [
    "# Example: rank(), dense_rank(), and row_number()\n",
    "\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window specification - partition by Department, order by Salary descending\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "\n",
    "# Apply different ranking functions\n",
    "df_rankings = df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "print(\"Rankings within each Department (ordered by Salary descending):\")\n",
    "df_rankings.select(\"Name\", \"Department\", \"Salary\", \"rank\", \"dense_rank\", \"row_number\").show()\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"- rank(): Leaves gaps when there are ties (e.g., if two people have same salary)\")\n",
    "print(\"- dense_rank(): No gaps, consecutive ranks even with ties\")\n",
    "print(\"- row_number(): Always unique sequential numbers, even for ties\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead and Lag Examples:\n",
      "lead(): Accesses data from the NEXT row\n",
      "lag(): Accesses data from the PREVIOUS row\n",
      "\n",
      "+-------+----------+-------+------+---------------+-----------+-------------+\n",
      "|   Name|Department|  Month|Salary|previous_salary|next_salary|salary_change|\n",
      "+-------+----------+-------+------+---------------+-----------+-------------+\n",
      "|    Bob|        IT|2024-01| 60000|           NULL|      55000|         NULL|\n",
      "|  Diana|        IT|2024-01| 55000|          60000|      61000|        -5000|\n",
      "|    Bob|        IT|2024-02| 61000|          55000|      56000|         6000|\n",
      "|  Diana|        IT|2024-02| 56000|          61000|       NULL|        -5000|\n",
      "|  Alice|     Sales|2024-01| 50000|           NULL|      70000|         NULL|\n",
      "|Charlie|     Sales|2024-01| 70000|          50000|      52000|        20000|\n",
      "|  Alice|     Sales|2024-02| 52000|          70000|      72000|       -18000|\n",
      "|Charlie|     Sales|2024-02| 72000|          52000|       NULL|        20000|\n",
      "+-------+----------+-------+------+---------------+-----------+-------------+\n",
      "\n",
      "\n",
      "Note:\n",
      "- lag(Salary, 1): Previous row's salary\n",
      "- lead(Salary, 1): Next row's salary\n",
      "- The second parameter (1) is the offset (how many rows ahead/behind)\n"
     ]
    }
   ],
   "source": [
    "# Example: lead() and lag()\n",
    "\n",
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "# Define window specification - partition by Department, order by Month\n",
    "window_spec_lead_lag = Window.partitionBy(\"Department\").orderBy(\"Month\")\n",
    "\n",
    "# Apply lead and lag functions\n",
    "df_lead_lag = df.withColumn(\"previous_salary\", lag(\"Salary\", 1).over(window_spec_lead_lag)) \\\n",
    "    .withColumn(\"next_salary\", lead(\"Salary\", 1).over(window_spec_lead_lag)) \\\n",
    "    .withColumn(\"salary_change\", col(\"Salary\") - col(\"previous_salary\"))\n",
    "\n",
    "print(\"Lead and Lag Examples:\")\n",
    "print(\"lead(): Accesses data from the NEXT row\")\n",
    "print(\"lag(): Accesses data from the PREVIOUS row\")\n",
    "print()\n",
    "\n",
    "df_lead_lag.select(\"Name\", \"Department\", \"Month\", \"Salary\", \"previous_salary\", \"next_salary\", \"salary_change\").show()\n",
    "\n",
    "print(\"\\nNote:\")\n",
    "print(\"- lag(Salary, 1): Previous row's salary\")\n",
    "print(\"- lead(Salary, 1): Next row's salary\")\n",
    "print(\"- The second parameter (1) is the offset (how many rows ahead/behind)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDFs)\n",
    "\n",
    "UDFs allow you to apply custom Python functions to DataFrame columns. **Note**: UDFs are slower than built-in functions - use them only when necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+--------------+\n",
      "|   Name|Department|Salary|  Month|SalaryCategory|\n",
      "+-------+----------+------+-------+--------------+\n",
      "|  Alice|     Sales| 50000|2024-01|           Low|\n",
      "|    Bob|        IT| 60000|2024-01|        Medium|\n",
      "|Charlie|     Sales| 70000|2024-01|          High|\n",
      "|  Diana|        IT| 55000|2024-01|           Low|\n",
      "|  Alice|     Sales| 52000|2024-02|           Low|\n",
      "|    Bob|        IT| 61000|2024-02|        Medium|\n",
      "|Charlie|     Sales| 72000|2024-02|          High|\n",
      "|  Diana|        IT| 56000|2024-02|        Medium|\n",
      "+-------+----------+------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a simple UDF\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def categorize_salary(salary):\n",
    "    if salary > 65000:\n",
    "        return \"High\"\n",
    "    elif salary > 55000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# Register UDF\n",
    "categorize_udf = udf(categorize_salary, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df_with_category = df.withColumn(\"SalaryCategory\", categorize_udf(col(\"Salary\")))\n",
    "df_with_category.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Operations\n",
    "\n",
    "Pivot transforms rows into columns. Useful for creating summary tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   Name|    IT| Sales|\n",
      "+-------+------+------+\n",
      "|  Diana|111000|  NULL|\n",
      "|Charlie|  NULL|142000|\n",
      "|    Bob|121000|  NULL|\n",
      "|  Alice|  NULL|102000|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot: Transform Department values into columns\n",
    "df_pivot = df.groupBy(\"Name\").pivot(\"Department\").agg(sum(\"Salary\").alias(\"TotalSalary\"))\n",
    "df_pivot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union Operations\n",
    "\n",
    "Combine multiple DataFrames with the same schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of DataFrames:\n",
      "+-------+----------+------+-------+\n",
      "|   Name|Department|Salary|  Month|\n",
      "+-------+----------+------+-------+\n",
      "|  Alice|     Sales| 50000|2024-01|\n",
      "|    Bob|        IT| 60000|2024-01|\n",
      "|Charlie|     Sales| 70000|2024-01|\n",
      "|  Diana|        IT| 55000|2024-01|\n",
      "|  Alice|     Sales| 52000|2024-02|\n",
      "|    Bob|        IT| 61000|2024-02|\n",
      "|Charlie|     Sales| 72000|2024-02|\n",
      "|  Diana|        IT| 56000|2024-02|\n",
      "|    Eve|        HR| 65000|2024-01|\n",
      "|  Frank|     Sales| 52000|2024-01|\n",
      "+-------+----------+------+-------+\n",
      "\n",
      "\n",
      "Union with distinct:\n",
      "+-------+----------+------+-------+\n",
      "|   Name|Department|Salary|  Month|\n",
      "+-------+----------+------+-------+\n",
      "|  Alice|     Sales| 50000|2024-01|\n",
      "|    Bob|        IT| 60000|2024-01|\n",
      "|Charlie|     Sales| 70000|2024-01|\n",
      "|  Diana|        IT| 55000|2024-01|\n",
      "|  Alice|     Sales| 52000|2024-02|\n",
      "|    Bob|        IT| 61000|2024-02|\n",
      "|Charlie|     Sales| 72000|2024-02|\n",
      "|  Diana|        IT| 56000|2024-02|\n",
      "|    Eve|        HR| 65000|2024-01|\n",
      "|  Frank|     Sales| 52000|2024-01|\n",
      "+-------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create another DataFrame\n",
    "data2 = [\n",
    "    (\"Eve\", \"HR\", 65000, \"2024-01\"),\n",
    "    (\"Frank\", \"Sales\", 52000, \"2024-01\")\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema)\n",
    "\n",
    "# Union DataFrames\n",
    "df_union = df.union(df2)\n",
    "print(\"Union of DataFrames:\")\n",
    "df_union.show()\n",
    "\n",
    "# Union with distinct (removes duplicates)\n",
    "df_union_distinct = df.union(df2).distinct()\n",
    "print(\"\\nUnion with distinct:\")\n",
    "df_union_distinct.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct and Drop Duplicates\n",
    "\n",
    "Remove duplicate rows from DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct rows:\n",
      "+-------+----------+------+-------+\n",
      "|   Name|Department|Salary|  Month|\n",
      "+-------+----------+------+-------+\n",
      "|  Alice|     Sales| 50000|2024-01|\n",
      "|    Bob|        IT| 60000|2024-01|\n",
      "|Charlie|     Sales| 70000|2024-01|\n",
      "|  Diana|        IT| 55000|2024-01|\n",
      "|  Alice|     Sales| 52000|2024-02|\n",
      "|    Bob|        IT| 61000|2024-02|\n",
      "|Charlie|     Sales| 72000|2024-02|\n",
      "|  Diana|        IT| 56000|2024-02|\n",
      "+-------+----------+------+-------+\n",
      "\n",
      "\n",
      "Drop duplicates based on Name and Department:\n",
      "+-------+----------+------+-------+\n",
      "|   Name|Department|Salary|  Month|\n",
      "+-------+----------+------+-------+\n",
      "|  Alice|     Sales| 50000|2024-01|\n",
      "|    Bob|        IT| 60000|2024-01|\n",
      "|Charlie|     Sales| 70000|2024-01|\n",
      "|  Diana|        IT| 55000|2024-01|\n",
      "+-------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get distinct rows\n",
    "df_distinct = df.distinct()\n",
    "print(\"Distinct rows:\")\n",
    "df_distinct.show()\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "df_no_duplicates = df.dropDuplicates([\"Name\", \"Department\"])\n",
    "print(\"\\nDrop duplicates based on Name and Department:\")\n",
    "df_no_duplicates.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Window Functions**: Perform calculations across related rows without collapsing data\n",
    "2. **UDFs**: Custom functions for complex transformations (use sparingly - they're slower)\n",
    "3. **Pivot**: Transform rows into columns for summary tables\n",
    "4. **Union**: Combine DataFrames with the same schema\n",
    "5. **Distinct/Drop Duplicates**: Remove duplicate rows\n",
    "\n",
    "**Key Takeaway**: Window functions are powerful for analytical queries. UDFs should be used only when built-in functions aren't sufficient, as they have performance overhead.\n",
    "\n",
    "**Next Steps**: In Module 8, we'll learn about performance optimization techniques including partitioning, caching, and bucketing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
