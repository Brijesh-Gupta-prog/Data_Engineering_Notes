{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11 - Pandas vs Spark: Understanding the Differences\n",
        "\n",
        "## Introduction\n",
        "\n",
        "As a data engineer, you'll work with both Pandas and Spark (PySpark). Understanding when to use each is crucial for building efficient data pipelines. This notebook explains the key differences and helps you choose the right tool for your task.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- What is Pandas and what is Spark?\n",
        "- Key differences between Pandas and Spark\n",
        "- When to use Pandas vs Spark\n",
        "- Similar operations in both libraries\n",
        "- Performance considerations\n",
        "- Real-world use cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Pandas?\n",
        "\n",
        "**Pandas** is a Python library for data manipulation and analysis:\n",
        "- Works on a **single machine** (your laptop or server)\n",
        "- Processes data **in-memory** (RAM)\n",
        "- Best for **small to medium datasets** (typically < 10-50 GB)\n",
        "- Fast for interactive analysis and data exploration\n",
        "- Easy to learn and use\n",
        "- Great for data cleaning, transformation, and analysis\n",
        "\n",
        "**Think of Pandas as:** Excel on steroids, but for Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Spark (PySpark)?\n",
        "\n",
        "**Apache Spark** (PySpark is the Python API) is a distributed computing framework:\n",
        "- Works on **multiple machines** (clusters)\n",
        "- Processes data **distributed across cluster**\n",
        "- Best for **large datasets** (hundreds of GB to TB+)\n",
        "- Designed for big data processing\n",
        "- Can handle streaming data\n",
        "- More complex to set up and use\n",
        "\n",
        "**Think of Spark as:** Pandas that can work across many computers simultaneously\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Differences\n",
        "\n",
        "| Aspect | Pandas | Spark (PySpark) |\n",
        "|--------|--------|-----------------|\n",
        "| **Architecture** | Single machine | Distributed cluster |\n",
        "| **Data Size** | Small to medium (< 50 GB) | Large (GB to TB+) |\n",
        "| **Memory** | In-memory (RAM) | Distributed across nodes |\n",
        "| **Speed** | Very fast for small data | Fast for large data (parallel processing) |\n",
        "| **Learning Curve** | Easy | Moderate to difficult |\n",
        "| **Setup** | Simple (`pip install pandas`) | Complex (requires cluster) |\n",
        "| **Use Case** | Data analysis, ETL on small data | Big data processing, ETL on large data |\n",
        "| **Lazy Evaluation** | No (eager) | Yes (lazy) |\n",
        "| **Streaming** | Limited | Excellent support |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use Pandas?\n",
        "\n",
        "✅ **Use Pandas when:**\n",
        "- Working with datasets that fit in memory (< 10-50 GB)\n",
        "- Doing exploratory data analysis\n",
        "- Quick data cleaning and transformation\n",
        "- Building prototypes and proof-of-concepts\n",
        "- Working on a single machine\n",
        "- Need fast iteration and interactive analysis\n",
        "- Data fits comfortably in RAM\n",
        "\n",
        "**Example scenarios:**\n",
        "- Analyzing sales data from a single store\n",
        "- Cleaning customer data from a CSV file\n",
        "- Creating reports from a database query result\n",
        "- Data science projects with sample datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use Spark?\n",
        "\n",
        "✅ **Use Spark when:**\n",
        "- Working with datasets too large for a single machine\n",
        "- Processing data across multiple machines (clusters)\n",
        "- Handling streaming data in real-time\n",
        "- Need to process terabytes of data\n",
        "- Building production ETL pipelines for big data\n",
        "- Data doesn't fit in memory\n",
        "\n",
        "**Example scenarios:**\n",
        "- Processing logs from thousands of servers\n",
        "- Analyzing years of transaction data\n",
        "- Real-time processing of clickstream data\n",
        "- ETL pipelines processing millions of records daily\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Similar Operations: Pandas vs PySpark\n",
        "\n",
        "Let's see how similar operations look in both libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas DataFrame:\n",
            "      Name  Age      City  Salary\n",
            "0    Alice   25  New York   50000\n",
            "1      Bob   30    London   60000\n",
            "2  Charlie   35     Tokyo   70000\n",
            "3    Diana   28     Paris   55000\n",
            "4      Eve   32    Sydney   65000\n",
            "Type: <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Example: Reading CSV file\n",
        "\n",
        "# PANDAS\n",
        "import pandas as pd\n",
        "df_pandas = pd.read_csv('sample_data.csv')\n",
        "print(\"Pandas DataFrame:\")\n",
        "print(df_pandas.head())\n",
        "print(f\"Type: {type(df_pandas)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: PySpark requires Spark installation and cluster setup\n",
            "The syntax is similar but Spark uses lazy evaluation\n"
          ]
        }
      ],
      "source": [
        "# PYSPARK (commented out - requires Spark installation)\n",
        "# from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "# df_spark = spark.read.csv('sample_data.csv', header=True, inferSchema=True)\n",
        "# print(\"Spark DataFrame:\")\n",
        "# df_spark.show()\n",
        "# print(f\"Type: {type(df_spark)}\")\n",
        "\n",
        "print(\"Note: PySpark requires Spark installation and cluster setup\")\n",
        "print(\"The syntax is similar but Spark uses lazy evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code Comparison Examples\n",
        "\n",
        "Let's compare common operations side by side:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Filtering Data\n",
        "\n",
        "**Pandas:**\n",
        "```python\n",
        "df_filtered = df[df['Age'] > 30]\n",
        "```\n",
        "\n",
        "**PySpark:**\n",
        "```python\n",
        "df_filtered = df.filter(df['Age'] > 30)\n",
        "# or\n",
        "df_filtered = df.where(df['Age'] > 30)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Selecting Columns\n",
        "\n",
        "**Pandas:**\n",
        "```python\n",
        "df_selected = df[['Name', 'Age', 'Salary']]\n",
        "```\n",
        "\n",
        "**PySpark:**\n",
        "```python\n",
        "df_selected = df.select('Name', 'Age', 'Salary')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. GroupBy and Aggregation\n",
        "\n",
        "**Pandas:**\n",
        "```python\n",
        "result = df.groupby('Department')['Salary'].mean()\n",
        "```\n",
        "\n",
        "**PySpark:**\n",
        "```python\n",
        "result = df.groupBy('Department').avg('Salary')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Joining DataFrames\n",
        "\n",
        "**Pandas:**\n",
        "```python\n",
        "merged = pd.merge(df1, df2, on='ID', how='inner')\n",
        "```\n",
        "\n",
        "**PySpark:**\n",
        "```python\n",
        "merged = df1.join(df2, on='ID', how='inner')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Conceptual Differences\n",
        "\n",
        "### 1. Eager vs Lazy Evaluation\n",
        "\n",
        "**Pandas (Eager):**\n",
        "- Operations execute immediately\n",
        "- You see results right away\n",
        "- Easy to debug\n",
        "\n",
        "```python\n",
        "df = pd.read_csv('data.csv')  # Reads immediately\n",
        "df_filtered = df[df['Age'] > 30]  # Filters immediately\n",
        "print(df_filtered)  # Shows results\n",
        "```\n",
        "\n",
        "**Spark (Lazy):**\n",
        "- Operations build a plan (don't execute immediately)\n",
        "- Execution happens when you \"trigger\" it (e.g., `.show()`, `.collect()`)\n",
        "- More efficient for large datasets\n",
        "\n",
        "```python\n",
        "df = spark.read.csv('data.csv')  # Just creates a plan\n",
        "df_filtered = df.filter(df['Age'] > 30)  # Adds to plan\n",
        "df_filtered.show()  # NOW it executes!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Types\n",
        "\n",
        "**Pandas:**\n",
        "- Uses NumPy arrays and Python objects\n",
        "- DataFrame is a collection of Series\n",
        "- Native Python types\n",
        "\n",
        "**Spark:**\n",
        "- Uses Spark SQL types\n",
        "- DataFrame is a distributed collection\n",
        "- Spark-specific types (StringType, IntegerType, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Performance Characteristics\n",
        "\n",
        "**Pandas:**\n",
        "- Very fast for small data (milliseconds)\n",
        "- Slows down as data grows\n",
        "- Limited by RAM\n",
        "- Single-threaded by default (mostly)\n",
        "\n",
        "**Spark:**\n",
        "- Slower startup (cluster overhead)\n",
        "- Fast for large data (parallel processing)\n",
        "- Can handle data larger than RAM\n",
        "- Multi-threaded and distributed by design\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Decision Guide\n",
        "\n",
        "### Scenario 1: Analyzing Sales Data\n",
        "- **Data size:** 1 GB CSV file\n",
        "- **Use:** Pandas ✅\n",
        "- **Reason:** Fits in memory, fast to process, easy to work with\n",
        "\n",
        "### Scenario 2: Processing Web Server Logs\n",
        "- **Data size:** 500 GB of log files\n",
        "- **Use:** Spark ✅\n",
        "- **Reason:** Too large for single machine, needs distributed processing\n",
        "\n",
        "### Scenario 3: Real-time Clickstream Analysis\n",
        "- **Data:** Streaming data (continuous)\n",
        "- **Use:** Spark ✅\n",
        "- **Reason:** Spark has excellent streaming capabilities\n",
        "\n",
        "### Scenario 4: Data Cleaning for ML Model\n",
        "- **Data size:** 100 MB dataset\n",
        "- **Use:** Pandas ✅\n",
        "- **Reason:** Small data, need quick iteration for experimentation\n",
        "\n",
        "### Scenario 5: ETL Pipeline for Data Warehouse\n",
        "- **Data size:** 10 TB daily\n",
        "- **Use:** Spark ✅\n",
        "- **Reason:** Production pipeline, large scale, needs reliability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Can You Use Both?\n",
        "\n",
        "**Yes!** Many data engineers use both:\n",
        "\n",
        "1. **Use Pandas for:**\n",
        "   - Development and testing\n",
        "   - Quick data exploration\n",
        "   - Small data processing\n",
        "   - Prototyping\n",
        "\n",
        "2. **Use Spark for:**\n",
        "   - Production pipelines\n",
        "   - Large-scale data processing\n",
        "   - When data doesn't fit in memory\n",
        "\n",
        "**Common Workflow:**\n",
        "- Develop and test with Pandas on sample data\n",
        "- Port to Spark for production with full dataset\n",
        "- Use Pandas for ad-hoc analysis and reports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Path\n",
        "\n",
        "**For Data Engineering Freshers:**\n",
        "\n",
        "1. **Start with Pandas** (what you're learning now!)\n",
        "   - Easier to learn\n",
        "   - Faster to get productive\n",
        "   - Covers most data engineering concepts\n",
        "   - Great foundation\n",
        "\n",
        "2. **Then learn Spark (PySpark)**\n",
        "   - Builds on pandas concepts\n",
        "   - Similar syntax in many cases\n",
        "   - Essential for big data roles\n",
        "   - Opens up more opportunities\n",
        "\n",
        "If you know Pandas well, learning PySpark is easier because:\n",
        "- Similar concepts (DataFrames, filtering, grouping)\n",
        "- Similar operations (just different syntax)\n",
        "- Same mental model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Comparison Table\n",
        "\n",
        "| Feature | Pandas | Spark |\n",
        "|---------|--------|-------|\n",
        "| **Best For** | Small-medium data | Large-big data |\n",
        "| **Architecture** | Single machine | Distributed cluster |\n",
        "| **Memory** | In-memory | Distributed |\n",
        "| **Speed (small data)** | Very fast | Slower (overhead) |\n",
        "| **Speed (large data)** | Slow/impossible | Fast (parallel) |\n",
        "| **Learning Curve** | Easy | Moderate |\n",
        "| **Setup** | Simple | Complex |\n",
        "| **Streaming** | Limited | Excellent |\n",
        "| **Cost** | Free (just library) | Free but needs infrastructure |\n",
        "| **Use Cases** | Analysis, ETL (small) | Big data ETL, streaming |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "✅ **Pandas:**\n",
        "- Perfect for data that fits in memory\n",
        "- Great for learning and development\n",
        "- Fast iteration and exploration\n",
        "- Essential skill for data engineers\n",
        "\n",
        "✅ **Spark:**\n",
        "- Essential for big data processing\n",
        "- Required for large-scale production systems\n",
        "- Handles data beyond memory limits\n",
        "- Industry standard for big data\n",
        "\n",
        "✅ **Both:**\n",
        "- Learn Pandas first (easier)\n",
        "- Then learn Spark (builds on Pandas knowledge)\n",
        "- Use the right tool for the job\n",
        "- Many concepts are similar\n",
        "\n",
        "**Remember:** Master Pandas well first. It's the foundation that makes learning Spark much easier!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "After mastering Pandas:\n",
        "1. ✅ You'll understand DataFrame concepts\n",
        "2. ✅ You'll know data manipulation operations\n",
        "3. ✅ You'll be ready to learn PySpark\n",
        "4. ✅ You'll understand when to use each tool\n",
        "\n",
        "**The concepts you learn in Pandas directly transfer to Spark!**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
