{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Partitions: ADLS File Partitions vs Spark Executor Partitions\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **The difference** between file partitions in storage (ADLS) and partitions in Spark executors\n",
        "2. **Why idle partitions occur** and how they waste resources\n",
        "3. **How to diagnose** partition-related performance issues\n",
        "4. **Best practices** for optimizing partition counts\n",
        "5. **Common mistakes** and how to avoid them\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Basic understanding of Spark architecture (executors, cores, tasks)\n",
        "- Familiarity with reading data from cloud storage (ADLS, S3, etc.)\n",
        "- Understanding of distributed computing concepts\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:** This is the **concepts notebook**. For hands-on practical demonstration with real data and Spark UI monitoring, see `08_a_Partitions_Practice.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Understanding Your Configuration\n",
        "\n",
        "Before we dive into partitions, let's understand the setup we're working with.\n",
        "\n",
        "### Cluster Configuration\n",
        "\n",
        "```python\n",
        "# Your Spark cluster setup\n",
        "num_executors = 4\n",
        "cores_per_executor = 4\n",
        "total_cores = num_executors * cores_per_executor  # 16 cores total\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- You have **4 executors** (JVM processes running on worker nodes)\n",
        "- Each executor has **4 CPU cores** available\n",
        "- Total available parallelism: **16 cores**\n",
        "\n",
        "**Common Misconception:**\n",
        "> \"I have 16 cores, so I have 16-way parallelism\"\n",
        "\n",
        "**Reality:** This is only true if you have **at least 16 partitions**. Having 16 cores doesn't automatically give you 16-way parallelism - you need 16 tasks (one per partition) to utilize all cores.\n",
        "\n",
        "### Data Storage Configuration (ADLS)\n",
        "\n",
        "```python\n",
        "# Your data layout in ADLS\n",
        "data_path = \"abfss://container@storageaccount.dfs.core.windows.net/sales/\"\n",
        "\n",
        "# Files in the sales directory:\n",
        "# sales/\n",
        "#   ├── part-00000.parquet (256 GB)\n",
        "#   ├── part-00001.parquet (256 GB)\n",
        "#   ├── part-00002.parquet (256 GB)\n",
        "#   └── part-00003.parquet (256 GB)\n",
        "```\n",
        "\n",
        "**What this means:**\n",
        "- You have **4 large Parquet files** in your storage\n",
        "- Each file is **256 GB** (1 TB total)\n",
        "- These are **file partitions** - a way to organize data in storage\n",
        "\n",
        "**Important Distinction:**\n",
        "- **File partitions** (storage level) ≠ **Spark partitions** (compute level)\n",
        "- The number of files does NOT automatically equal the number of Spark partitions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: What We're Trying to Achieve\n",
        "\n",
        "### Goal: Maximum Resource Utilization\n",
        "\n",
        "**What we want:**\n",
        "- All 16 cores working simultaneously\n",
        "- No idle executors or cores\n",
        "- Efficient data processing with minimal waste\n",
        "\n",
        "**Why this matters:**\n",
        "- **Cost efficiency**: You're paying for all 16 cores - use them!\n",
        "- **Performance**: More parallelism = faster job completion\n",
        "- **Scalability**: Understanding this helps you scale properly\n",
        "\n",
        "### The Ideal Scenario\n",
        "\n",
        "```\n",
        "Executor 1: [Task 1] [Task 2] [Task 3] [Task 4] → All 4 cores busy\n",
        "Executor 2: [Task 5] [Task 6] [Task 7] [Task 8] → All 4 cores busy\n",
        "Executor 3: [Task 9] [Task 10] [Task 11] [Task 12] → All 4 cores busy\n",
        "Executor 4: [Task 13] [Task 14] [Task 15] [Task 16] → All 4 cores busy\n",
        "```\n",
        "\n",
        "**Key Principle:**\n",
        "> **Number of partitions should be ≥ number of cores (ideally 2-4× for better load balancing)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Common Mistake - What Happens When You Read Data Naively\n",
        "\n",
        "### The Problematic Code\n",
        "\n",
        "```python\n",
        "# This is what many beginners do (and it's WRONG for our scenario)\n",
        "sales_df = spark.read.parquet(\"abfss://container@storageaccount.dfs.core.windows.net/sales/\")\n",
        "```\n",
        "\n",
        "### What Spark Actually Does\n",
        "\n",
        "When you read data, Spark follows this rule:\n",
        "\n",
        "> **One input partition = one task = one core**\n",
        "\n",
        "**What Spark sees:**\n",
        "- 4 Parquet files in the directory\n",
        "- Parquet files are columnar and technically splittable, but **file count dominates** the partition creation\n",
        "- Spark creates: **4 partitions** → **4 tasks**\n",
        "\n",
        "**Visual Representation:**\n",
        "\n",
        "```\n",
        "Spark's View:\n",
        "├── Partition 0 (from part-00000.parquet) → Task 0\n",
        "├── Partition 1 (from part-00001.parquet) → Task 1\n",
        "├── Partition 2 (from part-00002.parquet) → Task 2\n",
        "└── Partition 3 (from part-00003.parquet) → Task 3\n",
        "```\n",
        "\n",
        "### The Painful Reality: Executor-Level Task Assignment\n",
        "\n",
        "**How Spark distributes tasks:**\n",
        "\n",
        "```\n",
        "Executor 1: \n",
        "  ├── Task 0 (processing part-00000.parquet, 256 GB)\n",
        "  └── Cores: [●] [○] [○] [○]  → 1 core used, 3 cores IDLE\n",
        "\n",
        "Executor 2:\n",
        "  ├── Task 1 (processing part-00001.parquet, 256 GB)\n",
        "  └── Cores: [●] [○] [○] [○]  → 1 core used, 3 cores IDLE\n",
        "\n",
        "Executor 3:\n",
        "  ├── Task 2 (processing part-00002.parquet, 256 GB)\n",
        "  └── Cores: [●] [○] [○] [○]  → 1 core used, 3 cores IDLE\n",
        "\n",
        "Executor 4:\n",
        "  ├── Task 3 (processing part-00003.parquet, 256 GB)\n",
        "  └── Cores: [●] [○] [○] [○]  → 1 core used, 3 cores IDLE\n",
        "```\n",
        "\n",
        "**Resource Utilization:**\n",
        "- **Used cores:** 4 out of 16\n",
        "- **Idle cores:** 12 out of 16\n",
        "- **Utilization:** 25% (75% waste!)\n",
        "\n",
        "**This is not \"suboptimal\" - this is embarrassingly bad for production!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Why Spark Behaves This Way (Understanding the Fundamentals)\n",
        "\n",
        "### Key Spark Rules (No Myths)\n",
        "\n",
        "**Spark does NOT:**\n",
        "- ❌ Split one task across multiple cores\n",
        "- ❌ Parallelize processing within a single partition\n",
        "- ❌ Allow threads to cooperate on the same partition\n",
        "\n",
        "**Why?**\n",
        "- A partition is processed by **one thread** (one core)\n",
        "- Threads don't cooperate on the same partition\n",
        "- JVM safety and determinism reasons\n",
        "- Ensures consistent, predictable results\n",
        "\n",
        "### Common Misconception\n",
        "\n",
        "**Wrong belief:**\n",
        "> \"My executor has 4 cores, so it will process the 256 GB partition 4 times faster\"\n",
        "\n",
        "**Reality:**\n",
        "> \"An executor with 4 cores can process **4 partitions in parallel**, not one partition faster\"\n",
        "\n",
        "**Think of it this way:**\n",
        "- Each core is like a worker\n",
        "- Each partition is like a job\n",
        "- One worker can only do one job at a time\n",
        "- To use 4 workers, you need 4 jobs (partitions)\n",
        "\n",
        "### Why Spark Doesn't Auto-Fix This\n",
        "\n",
        "**You might ask:** \"Why doesn't Spark just split the data better automatically?\"\n",
        "\n",
        "**Answer:** Spark respects input layout and avoids assumptions:\n",
        "\n",
        "1. **File boundaries matter**: Spark respects how data was written\n",
        "2. **Existing partitioning**: If data was partitioned a certain way, Spark assumes it was intentional\n",
        "3. **Avoids accidental shuffles**: Spark won't automatically repartition to avoid expensive operations\n",
        "4. **Assumption**: \"If you wrote 4 huge files, you probably meant it\"\n",
        "\n",
        "**That assumption is often wrong - but Spark won't guess. You need to tell it explicitly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: How to Diagnose Partition Issues\n",
        "\n",
        "### Check Your Partition Count\n",
        "\n",
        "**Always check this first:**\n",
        "\n",
        "```python\n",
        "# Read your data\n",
        "sales_df = spark.read.parquet(\"abfss://container@storageaccount.dfs.core.windows.net/sales/\")\n",
        "\n",
        "# Check partition count\n",
        "num_partitions = sales_df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions: {num_partitions}\")\n",
        "\n",
        "# Compare with available cores\n",
        "total_cores = spark.sparkContext.defaultParallelism\n",
        "print(f\"Total available cores: {total_cores}\")\n",
        "\n",
        "# Diagnosis\n",
        "if num_partitions < total_cores:\n",
        "    print(f\"⚠️  WARNING: You have {num_partitions} partitions but {total_cores} cores!\")\n",
        "    print(f\"⚠️  You are wasting {total_cores - num_partitions} cores!\")\n",
        "else:\n",
        "    print(\"✅ Partition count looks good!\")\n",
        "```\n",
        "\n",
        "### Understanding the Output\n",
        "\n",
        "For our scenario:\n",
        "```\n",
        "Number of partitions: 4\n",
        "Total available cores: 16\n",
        "⚠️  WARNING: You have 4 partitions but 16 cores!\n",
        "⚠️  You are wasting 12 cores!\n",
        "```\n",
        "\n",
        "**Rule of thumb:**\n",
        "- If `num_partitions < total_cores` → You're wasting resources\n",
        "- Ideal: `num_partitions = 2-4 × total_cores` (for better load balancing and hiding I/O wait times)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: The Right Way - Optimized Partition Strategy\n",
        "\n",
        "### Option 1: Repartition After Read (Simple & Explicit)\n",
        "\n",
        "**Recommended for beginners:**\n",
        "\n",
        "```python\n",
        "# Step 1: Read the data (creates 4 partitions from 4 files)\n",
        "sales_df = spark.read.parquet(\"abfss://container@storageaccount.dfs.core.windows.net/sales/\")\n",
        "\n",
        "# Step 2: Explicitly repartition to match your cluster\n",
        "# Target: 2-4× your core count for optimal performance\n",
        "sales_df_optimized = sales_df.repartition(32)  # 2× your 16 cores\n",
        "\n",
        "# Verify\n",
        "print(f\"Partitions after repartition: {sales_df_optimized.rdd.getNumPartitions()}\")\n",
        "```\n",
        "\n",
        "**What happens:**\n",
        "1. Spark reads 4 files → creates 4 initial partitions\n",
        "2. `repartition(32)` triggers a **shuffle** to redistribute data into 32 partitions\n",
        "3. Each partition is now ~32 GB instead of 256 GB\n",
        "4. Spark creates 32 tasks → can utilize all 16 cores (with tasks queued)\n",
        "\n",
        "### Option 2: Repartition During Read (More Efficient)\n",
        "\n",
        "**Better approach - avoids initial 4-partition creation:**\n",
        "\n",
        "```python\n",
        "# Read and repartition in one step\n",
        "sales_df = (\n",
        "    spark.read.parquet(\"abfss://container@storageaccount.dfs.core.windows.net/sales/\")\n",
        "    .repartition(32)\n",
        ")\n",
        "```\n",
        "\n",
        "### Optimized Task Distribution\n",
        "\n",
        "**After repartitioning to 32 partitions:**\n",
        "\n",
        "```\n",
        "Executor 1: \n",
        "  ├── Tasks: [1] [2] [3] [4] [5] [6] [7] [8]\n",
        "  └── Cores: [●] [●] [●] [●] [●] [●] [●] [●]  → All 4 cores busy, 4 tasks queued\n",
        "\n",
        "Executor 2:\n",
        "  ├── Tasks: [9] [10] [11] [12] [13] [14] [15] [16]\n",
        "  └── Cores: [●] [●] [●] [●] [●] [●] [●] [●]  → All 4 cores busy, 4 tasks queued\n",
        "\n",
        "Executor 3:\n",
        "  ├── Tasks: [17] [18] [19] [20] [21] [22] [23] [24]\n",
        "  └── Cores: [●] [●] [●] [●] [●] [●] [●] [●]  → All 4 cores busy, 4 tasks queued\n",
        "\n",
        "Executor 4:\n",
        "  ├── Tasks: [25] [26] [27] [28] [29] [30] [31] [32]\n",
        "  └── Cores: [●] [●] [●] [●] [●] [●] [●] [●]  → All 4 cores busy, 4 tasks queued\n",
        "```\n",
        "\n",
        "**Resource Utilization:**\n",
        "- **Used cores:** 16 out of 16\n",
        "- **Idle cores:** 0\n",
        "- **Utilization:** 100% ✅\n",
        "- **Task queue:** As soon as one task finishes, the next starts immediately\n",
        "\n",
        "**This is what a healthy Spark job looks like!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Understanding the Impact on Operations\n",
        "\n",
        "### Example: Broadcast Join with Optimized Partitions\n",
        "\n",
        "**Scenario:** Joining sales data (large) with product catalog (small)\n",
        "\n",
        "```python\n",
        "# Small table (will be broadcast)\n",
        "products_df = spark.read.parquet(\"abfss://.../products/\")  # 100 MB\n",
        "\n",
        "# Large table (our sales data)\n",
        "sales_df = (\n",
        "    spark.read.parquet(\"abfss://.../sales/\")\n",
        "    .repartition(32)  # Optimized partitions\n",
        ")\n",
        "\n",
        "# Broadcast join\n",
        "result = sales_df.join(\n",
        "    broadcast(products_df),\n",
        "    on=\"product_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "```\n",
        "\n",
        "### What Happens in the Optimized Setup\n",
        "\n",
        "**Broadcast side (small table):**\n",
        "- Still broadcast once per executor (no change)\n",
        "- Memory impact unchanged\n",
        "\n",
        "**Fact side (sales data):**\n",
        "- Each executor processes **8 smaller partitions** (32 partitions ÷ 4 executors)\n",
        "- Each partition is **~32 GB** instead of 256 GB\n",
        "- **Benefits:**\n",
        "  - ✅ Faster task completion (smaller chunks)\n",
        "  - ✅ Better garbage collection behavior (less memory pressure)\n",
        "  - ✅ Reduced spill risk (less data per task)\n",
        "  - ✅ Better load balancing (if some partitions are slower, others compensate)\n",
        "\n",
        "### Visual Comparison\n",
        "\n",
        "**❌ Bad Layout (4 partitions):**\n",
        "```\n",
        "4 partitions → 4 tasks → 16 cores → 12 idle\n",
        "Each task processes 256 GB\n",
        "Slow, inefficient, wasteful\n",
        "```\n",
        "\n",
        "**✅ Optimized Layout (32 partitions):**\n",
        "```\n",
        "32 partitions → 32 tasks → 16 cores → always busy\n",
        "Each task processes ~32 GB\n",
        "Fast, efficient, all resources utilized\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Production-Grade Rules to Follow\n",
        "\n",
        "### Rule 1: Files Are NOT Partitions\n",
        "\n",
        "**Critical distinction:**\n",
        "- **Storage layout** (file partitions) ≠ **Compute layout** (Spark partitions)\n",
        "- Stop conflating them!\n",
        "\n",
        "**Example:**\n",
        "- You might have 4 files in ADLS (storage partitioning)\n",
        "- But you need 32+ Spark partitions for optimal compute\n",
        "\n",
        "### Rule 2: Always Check Partition Count\n",
        "\n",
        "**Before running expensive operations:**\n",
        "\n",
        "```python\n",
        "# Always verify\n",
        "num_partitions = df.rdd.getNumPartitions()\n",
        "total_cores = spark.sparkContext.defaultParallelism\n",
        "\n",
        "if num_partitions < total_cores:\n",
        "    print(f\"⚠️  WARNING: Only {num_partitions} partitions for {total_cores} cores!\")\n",
        "    print(\"Consider repartitioning!\")\n",
        "```\n",
        "\n",
        "**If `num_partitions < total_cores`:**\n",
        "- You are wasting money and time\n",
        "- Your cluster is underutilized\n",
        "- Your job will run slower than necessary\n",
        "\n",
        "### Rule 3: Broadcast Join ≠ Performance Fix\n",
        "\n",
        "**Common mistake:**\n",
        "> \"I'll use broadcast join to fix my performance issues\"\n",
        "\n",
        "**Reality:**\n",
        "- Broadcast join removes shuffle (good!)\n",
        "- But if your partitions are wrong, broadcast won't save you\n",
        "- You still need proper partitioning for the large table\n",
        "\n",
        "**Both matter:**\n",
        "- ✅ Broadcast small tables (avoid shuffle)\n",
        "- ✅ Properly partition large tables (utilize cores)\n",
        "\n",
        "### Rule 4: Choose Partition Count Wisely\n",
        "\n",
        "**Guidelines:**\n",
        "- **Minimum:** `num_partitions ≥ total_cores`\n",
        "- **Ideal:** `num_partitions = 2-4 × total_cores`\n",
        "- **Why 2-4×?**\n",
        "  - Hides I/O wait times (while one task waits for I/O, others run)\n",
        "  - Better load balancing (handles data skew)\n",
        "  - Allows for task queuing (smooth execution)\n",
        "\n",
        "**Too many partitions:**\n",
        "- Overhead from task scheduling\n",
        "- Small tasks are inefficient\n",
        "- Generally avoid: `num_partitions > 10 × total_cores`\n",
        "\n",
        "### Rule 5: Understand When Repartitioning Happens\n",
        "\n",
        "**`repartition()`:**\n",
        "- Triggers a **full shuffle**\n",
        "- Redistributes data across all partitions\n",
        "- Use when you need to change partition count significantly\n",
        "\n",
        "**`coalesce()`:**\n",
        "- Reduces partition count **without shuffle**\n",
        "- Combines adjacent partitions\n",
        "- Use when reducing partitions (more efficient than repartition)\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# If you have 100 partitions and want 32\n",
        "df.coalesce(32)  # More efficient (no shuffle)\n",
        "\n",
        "# If you have 4 partitions and want 32\n",
        "df.repartition(32)  # Necessary (requires shuffle)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Practical Example - Before and After\n",
        "\n",
        "### Scenario Setup\n",
        "\n",
        "Let's see a complete example with code:\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PartitionOptimization\") \\\n",
        "    .config(\"spark.executor.instances\", \"4\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Check your cluster configuration\n",
        "print(\"=== Cluster Configuration ===\")\n",
        "print(f\"Default parallelism: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"Executor instances: {spark.conf.get('spark.executor.instances')}\")\n",
        "print(f\"Cores per executor: {spark.conf.get('spark.executor.cores')}\")\n",
        "```\n",
        "\n",
        "### Before Optimization (The Problem)\n",
        "\n",
        "```python\n",
        "# ❌ BAD: Naive read\n",
        "sales_df_bad = spark.read.parquet(\"abfss://container@storageaccount.dfs.core.windows.net/sales/\")\n",
        "\n",
        "print(\"=== Before Optimization ===\")\n",
        "print(f\"Partitions: {sales_df_bad.rdd.getNumPartitions()}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"Utilization: {sales_df_bad.rdd.getNumPartitions() / spark.sparkContext.defaultParallelism * 100:.1f}%\")\n",
        "\n",
        "# This will be slow and wasteful!\n",
        "# result_bad = sales_df_bad.groupBy(\"region\").agg({\"sales\": \"sum\"})\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```\n",
        "=== Before Optimization ===\n",
        "Partitions: 4\n",
        "Available cores: 16\n",
        "Utilization: 25.0%\n",
        "```\n",
        "\n",
        "### After Optimization (The Solution)\n",
        "\n",
        "```python\n",
        "# ✅ GOOD: Optimized read with repartition\n",
        "sales_df_good = (\n",
        "    spark.read.parquet(\"abfss://container@storageaccount.dfs.core.windows.net/sales/\")\n",
        "    .repartition(32)  # 2× your core count\n",
        ")\n",
        "\n",
        "print(\"=== After Optimization ===\")\n",
        "print(f\"Partitions: {sales_df_good.rdd.getNumPartitions()}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"Utilization: {sales_df_good.rdd.getNumPartitions() / spark.sparkContext.defaultParallelism * 100:.1f}%\")\n",
        "\n",
        "# This will be fast and efficient!\n",
        "# result_good = sales_df_good.groupBy(\"region\").agg({\"sales\": \"sum\"})\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```\n",
        "=== After Optimization ===\n",
        "Partitions: 32\n",
        "Available cores: 16\n",
        "Utilization: 200.0%  (2× for better load balancing)\n",
        "```\n",
        "\n",
        "### Performance Comparison\n",
        "\n",
        "**Expected improvements:**\n",
        "- **Execution time:** 3-4× faster (utilizing all cores)\n",
        "- **Resource utilization:** 25% → 100%+\n",
        "- **Cost efficiency:** Same cost, 3-4× more work done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Key Takeaways and Mental Model\n",
        "\n",
        "### The Core Concept (Lock This In)\n",
        "\n",
        "**Visual Mental Model:**\n",
        "\n",
        "```\n",
        "❌ BAD LAYOUT:\n",
        "Storage:  [File1] [File2] [File3] [File4]\n",
        "Spark:    [Part1] [Part2] [Part3] [Part4]\n",
        "Tasks:    [Task1] [Task2] [Task3] [Task4]\n",
        "Cores:    [●] [○] [○] [○]  [●] [○] [○] [○]  [●] [○] [○] [○]  [●] [○] [○] [○]\n",
        "          ↑ Only 4 cores used, 12 idle\n",
        "\n",
        "✅ OPTIMIZED LAYOUT:\n",
        "Storage:  [File1] [File2] [File3] [File4]\n",
        "Spark:    [P1][P2][P3]...[P32]  (repartitioned)\n",
        "Tasks:    [T1][T2][T3]...[T32]\n",
        "Cores:    [●][●][●][●] [●][●][●][●] [●][●][●][●] [●][●][●][●] ... (all busy)\n",
        "          ↑ All 16 cores utilized, tasks queued for smooth execution\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Storage partitions ≠ Compute partitions**\n",
        "   - Files in ADLS are storage-level organization\n",
        "   - Spark partitions are compute-level organization\n",
        "   - They can (and often should) be different!\n",
        "\n",
        "2. **One partition = one task = one core**\n",
        "   - This is a fundamental Spark rule\n",
        "   - More partitions = more parallelism potential\n",
        "   - But only if you have enough cores\n",
        "\n",
        "3. **Always check partition count**\n",
        "   - Use `df.rdd.getNumPartitions()`\n",
        "   - Compare with `spark.sparkContext.defaultParallelism`\n",
        "   - If partitions < cores, you're wasting resources\n",
        "\n",
        "4. **Optimal partition count**\n",
        "   - Minimum: Equal to number of cores\n",
        "   - Ideal: 2-4× number of cores\n",
        "   - Too many: Overhead and inefficiency\n",
        "\n",
        "5. **Repartition when needed**\n",
        "   - Use `repartition()` to increase partitions (triggers shuffle)\n",
        "   - Use `coalesce()` to decrease partitions (no shuffle)\n",
        "   - Always verify the result\n",
        "\n",
        "### Common Mistakes to Avoid\n",
        "\n",
        "1. ❌ Assuming file count = optimal partition count\n",
        "2. ❌ Not checking partition count before expensive operations\n",
        "3. ❌ Thinking broadcast join fixes all performance issues\n",
        "4. ❌ Creating too many or too few partitions\n",
        "5. ❌ Not understanding the difference between storage and compute partitions\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Practice:** See `08_a_Partitions_Practice.ipynb` for hands-on demonstration\n",
        "- **Experiment:** Try reading your own data and checking partition counts\n",
        "- **Monitor:** Use Spark UI to visualize task distribution\n",
        "- **Learn:** Understand how partitioning affects joins, aggregations, and shuffles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Configuration Understanding**\n",
        "   - Cluster: 4 executors × 4 cores = 16 total cores\n",
        "   - Storage: 4 files × 256 GB = 1 TB total data\n",
        "\n",
        "2. **The Problem**\n",
        "   - Naive read creates 4 partitions → 4 tasks\n",
        "   - Only 4 cores used → 12 cores idle (75% waste)\n",
        "\n",
        "3. **The Solution**\n",
        "   - Repartition to 32 partitions (2× core count)\n",
        "   - Creates 32 tasks → all 16 cores utilized\n",
        "   - Better load balancing and I/O hiding\n",
        "\n",
        "4. **Best Practices**\n",
        "   - Always check partition count\n",
        "   - Aim for 2-4× core count\n",
        "   - Understand storage vs compute partitions\n",
        "   - Use repartition/coalesce appropriately\n",
        "\n",
        "### Remember\n",
        "\n",
        "> **\"Files are not partitions. Storage layout ≠ Compute layout. Always verify your partition count matches your cluster capacity.\"**\n",
        "\n",
        "This understanding is crucial for writing efficient, production-grade Spark applications!\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Hands-On Practice\n",
        "\n",
        "Now that you understand the concepts, proceed to **`08_a_Partitions_Practice.ipynb`** for a hands-on demonstration where you'll:\n",
        "- Create actual Parquet files\n",
        "- Read them and observe partition behavior\n",
        "- Monitor Spark UI to see parallelism\n",
        "- Repartition based on your machine's cores\n",
        "- Compare performance before and after optimization\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
