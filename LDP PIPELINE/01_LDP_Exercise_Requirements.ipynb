{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ca6f42-c1a4-4143-9145-e608fe8d0565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Declarative Pipeline Exercise - Requirements\n",
    "\n",
    "## Overview\n",
    "\n",
    "You are tasked with building a **Lakeflow Declarative Pipeline** to process e-commerce data from raw JSON files. The data contains various **data quality issues** that you must identify and resolve as you build your pipeline through the Bronze, Silver, and Gold layers.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "- Design and implement a Bronze layer to ingest raw data\n",
    "- Design and implement a Silver layer to clean and transform data\n",
    "- Design and implement a Gold layer to create analytical tables for dashboards\n",
    "- Handle data quality issues including duplicates, missing values, invalid formats, and data type inconsistencies\n",
    "- Create a Lakeflow Declarative Pipeline using SQL or PySpark (Delta Live Tables)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **REQUIRED**: Run `00_LDP_Exercise_Setup_Environment.ipynb` first to set up the environment and generate sample data\n",
    "2. Familiarity with SQL and data transformation concepts\n",
    "3. Understanding of medallion architecture (Bronze, Silver, Gold)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "Your raw data is located in Unity Catalog volumes at:\n",
    "```\n",
    "/Volumes/ldp_exercise/exercise_schema/raw/\n",
    "```\n",
    "\n",
    "The following data sources are available:\n",
    "\n",
    "1. **Products** (`products/00.json`)\n",
    "   - Product catalog information\n",
    "   - Fields: `product_id`, `name`, `category`, `price`, `sku`, `stock_quantity`, `created_at`\n",
    "\n",
    "2. **Customers** (`customers/00.json`)\n",
    "   - Customer master data\n",
    "   - Fields: `customer_id`, `name`, `email`, `address`, `city`, `state`, `zip_code`, `phone`, `created_at`\n",
    "\n",
    "3. **Orders** (`orders/00.json`)\n",
    "   - Order transactions\n",
    "   - Fields: `order_id`, `customer_id`, `order_date`, `status`, `total_amount`\n",
    "\n",
    "4. **Payments** (`payments/00.json`)\n",
    "   - Payment transactions\n",
    "   - Fields: `payment_id`, `order_id`, `amount`, `payment_method`, `payment_status`, `payment_date`\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Data Quality Issues\n",
    "\n",
    "**IMPORTANT**: The sample data contains intentional data quality issues. You must identify and handle these issues in your pipeline:\n",
    "\n",
    "- **Duplicates**: Some records appear multiple times\n",
    "- **Missing Values**: NULL values in critical fields\n",
    "- **Invalid Formats**: Incorrect date formats, invalid email addresses, negative prices\n",
    "- **Data Type Issues**: Numbers stored as strings, dates as strings in wrong format\n",
    "- **Inconsistent Data**: Mixed case values, inconsistent state codes\n",
    "- **Orphaned Records**: References to non-existent entities (e.g., orders referencing non-existent customers)\n",
    "- **Invalid Values**: Negative amounts, zero prices, invalid status codes\n",
    "\n",
    "**Your task**: Identify these issues and implement appropriate data quality checks and transformations in your pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise Requirements\n",
    "\n",
    "### Bronze Layer (Your Design)\n",
    "\n",
    "**No specific instructions provided** - You must design this layer yourself.\n",
    "\n",
    "Consider:\n",
    "- How to ingest raw JSON files from the volume\n",
    "- What metadata to capture (file names, processing timestamps, etc.)\n",
    "- Whether to use streaming tables or materialized views\n",
    "- How to handle schema evolution\n",
    "\n",
    "### Silver Layer (Your Design)\n",
    "\n",
    "**No specific instructions provided** - You must design this layer yourself.\n",
    "\n",
    "Consider:\n",
    "- How to clean and validate data\n",
    "- How to handle duplicates\n",
    "- How to standardize data formats (dates, strings, numbers)\n",
    "- How to handle missing values\n",
    "- How to validate referential integrity (e.g., orders must reference valid customers)\n",
    "- How to filter out invalid records or flag them appropriately\n",
    "- Data type conversions and standardization\n",
    "\n",
    "### Gold Layer (Specific Requirements)\n",
    "\n",
    "You **MUST** create the following analytical tables/views for dashboard consumption:\n",
    "\n",
    "#### 1. **Sales Summary Dashboard Table**\n",
    "\n",
    "Table Name: `sales_summary_dashboard`\n",
    "\n",
    "This table should provide daily sales metrics:\n",
    "\n",
    "**Required Columns:**\n",
    "- `sale_date` (DATE) - The date of the sale\n",
    "- `total_revenue` (DECIMAL) - Total revenue for the day (sum of order amounts)\n",
    "- `total_orders` (BIGINT) - Total number of orders for the day\n",
    "- `avg_order_value` (DECIMAL) - Average order value for the day\n",
    "- `total_customers` (BIGINT) - Number of unique customers who placed orders\n",
    "- `completed_orders` (BIGINT) - Number of orders with status 'delivered'\n",
    "- `cancelled_orders` (BIGINT) - Number of orders with status 'cancelled'\n",
    "\n",
    "**Business Rules:**\n",
    "- Only include orders with valid, positive `total_amount`\n",
    "- Only include orders with status 'delivered', 'shipped', 'processing', or 'cancelled'\n",
    "- Exclude orders with NULL or invalid dates\n",
    "- Calculate metrics based on the order date\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Customer Analytics Dashboard Table**\n",
    "\n",
    "Table Name: `customer_analytics_dashboard`\n",
    "\n",
    "This table should provide customer-level analytics:\n",
    "\n",
    "**Required Columns:**\n",
    "- `customer_id` (BIGINT) - Customer identifier\n",
    "- `customer_name` (STRING) - Customer full name\n",
    "- `customer_state` (STRING) - Customer state (standardized to uppercase)\n",
    "- `total_orders` (BIGINT) - Total number of orders placed by customer\n",
    "- `total_spent` (DECIMAL) - Total amount spent by customer across all orders\n",
    "- `avg_order_value` (DECIMAL) - Average order value for the customer\n",
    "- `first_order_date` (DATE) - Date of customer's first order\n",
    "- `last_order_date` (DATE) - Date of customer's most recent order\n",
    "- `customer_lifetime_value` (DECIMAL) - Same as total_spent (for consistency)\n",
    "\n",
    "**Business Rules:**\n",
    "- Only include customers with valid email addresses (must contain '@')\n",
    "- Only include customers with complete address information (address, city, state, zip_code all not NULL)\n",
    "- Only count orders with valid, positive amounts\n",
    "- Only count orders with status 'delivered' or 'shipped'\n",
    "- Handle duplicate customers (keep the most recent record based on created_at)\n",
    "- Standardize state codes to uppercase\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Product Performance Dashboard Table**\n",
    "\n",
    "Table Name: `product_performance_dashboard`\n",
    "\n",
    "This table should provide product-level performance metrics:\n",
    "\n",
    "**Required Columns:**\n",
    "- `product_id` (BIGINT) - Product identifier\n",
    "- `product_name` (STRING) - Product name\n",
    "- `category` (STRING) - Product category\n",
    "- `price` (DECIMAL) - Product price\n",
    "- `stock_quantity` (BIGINT) - Current stock quantity\n",
    "- `total_revenue` (DECIMAL) - Total revenue generated by this product (from orders)\n",
    "- `total_units_sold` (BIGINT) - Total units sold (if available from order items, otherwise NULL)\n",
    "- `is_in_stock` (BOOLEAN) - Whether product is in stock (stock_quantity > 0)\n",
    "- `is_active` (BOOLEAN) - Whether product is active (price > 0 and valid SKU)\n",
    "\n",
    "**Business Rules:**\n",
    "- Only include products with valid, positive prices\n",
    "- Only include products with non-empty SKU\n",
    "- Set `stock_quantity` to 0 if it's negative\n",
    "- Calculate revenue from orders (you may need to infer this from order amounts if order items are not available)\n",
    "- If order items are not available, set `total_units_sold` to NULL\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Payment Analytics Dashboard Table**\n",
    "\n",
    "Table Name: `payment_analytics_dashboard`\n",
    "\n",
    "This table should provide payment transaction analytics:\n",
    "\n",
    "**Required Columns:**\n",
    "- `payment_date` (DATE) - Date of the payment\n",
    "- `payment_method` (STRING) - Payment method used\n",
    "- `total_transactions` (BIGINT) - Total number of transactions\n",
    "- `total_amount` (DECIMAL) - Total amount processed\n",
    "- `successful_transactions` (BIGINT) - Number of transactions with status 'completed'\n",
    "- `failed_transactions` (BIGINT) - Number of transactions with status 'failed'\n",
    "- `refunded_transactions` (BIGINT) - Number of transactions with status 'refunded'\n",
    "- `success_rate` (DECIMAL) - Percentage of successful transactions (successful / total * 100)\n",
    "\n",
    "**Business Rules:**\n",
    "- Only include payments with valid, positive amounts\n",
    "- Only include payments linked to valid orders (order_id exists in orders table)\n",
    "- Only include payments with valid payment methods (credit_card, debit_card, paypal, bank_transfer, cash)\n",
    "- Exclude payments with NULL or invalid dates\n",
    "- Calculate success rate as: (successful_transactions / total_transactions) * 100\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Requirements\n",
    "\n",
    "1. **Pipeline Structure**:\n",
    "   - Create your pipeline file(s) in the `solution/` folder\n",
    "   - You can use either:\n",
    "     - **SQL**: Lakeflow Spark Declarative Pipeline SQL syntax (`.sql` file)\n",
    "     - **PySpark**: Delta Live Tables (DLT) with PySpark (`.py` file)\n",
    "   - Follow medallion architecture: Bronze â†’ Silver â†’ Gold\n",
    "\n",
    "2. **Table Naming Convention**:\n",
    "   - Bronze tables: `{entity}_bronze` (e.g., `orders_bronze`, `customers_bronze`)\n",
    "   - Silver tables: `{entity}_silver` (e.g., `orders_silver`, `customers_silver`)\n",
    "   - Gold tables: Use the exact names specified above for dashboard tables\n",
    "\n",
    "3. **Catalog and Schema**:\n",
    "   - All tables must be created in: `ldp_exercise.exercise_schema`\n",
    "\n",
    "4. **Data Quality**:\n",
    "   - Implement appropriate data quality checks\n",
    "   - Handle or filter invalid records\n",
    "   - Document your data quality decisions in comments\n",
    "\n",
    "5. **Performance**:\n",
    "   - Use appropriate table types (STREAMING TABLE for Bronze/Silver, MATERIALIZED VIEW for Gold)\n",
    "   - Consider partitioning if needed\n",
    "   - Use TBLPROPERTIES to mark quality layers\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. **Lakeflow Declarative Pipeline file(s)** in the `solution/` folder\n",
    "   - Your pipeline should process all four data sources (products, customers, orders, payments)\n",
    "   - Implement Bronze, Silver, and Gold layers\n",
    "   - Create all four required Gold layer dashboard tables\n",
    "   - You can submit either:\n",
    "     - SQL file(s) using Lakeflow Spark Declarative Pipeline syntax (`.sql`)\n",
    "     - PySpark file(s) using Delta Live Tables (`.py`)\n",
    "     - Or both if you want to demonstrate proficiency in both approaches\n",
    "\n",
    "2. **Documentation** (optional but recommended):\n",
    "   - Comments in your code explaining data quality decisions\n",
    "   - Brief explanation of your Bronze and Silver layer design choices\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your solution will be evaluated on:\n",
    "\n",
    "1. **Correctness**: All four Gold layer tables must match the specified schema and business rules\n",
    "2. **Data Quality**: Proper handling of data quality issues (duplicates, nulls, invalid formats, etc.)\n",
    "3. **Architecture**: Proper implementation of medallion architecture (Bronze â†’ Silver â†’ Gold)\n",
    "4. **Code Quality**: Clean, readable code (SQL or PySpark) with appropriate comments\n",
    "5. **Completeness**: All data sources processed and integrated appropriately\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Review the data by exploring the raw JSON files in the volume\n",
    "2. Identify the data quality issues present in each data source\n",
    "3. Design your Bronze layer to ingest raw data\n",
    "4. Design your Silver layer to clean and transform data\n",
    "5. Implement the Gold layer according to the specifications above\n",
    "6. Test your pipeline and verify the Gold layer tables\n",
    "\n",
    "---\n",
    "\n",
    "## Tips\n",
    "\n",
    "- Start by exploring the raw data to understand its structure and issues\n",
    "- Use `STREAM read_files()` for Bronze layer ingestion\n",
    "- Consider using `DISTINCT` or window functions to handle duplicates\n",
    "- Use `TRY_CAST` or `CAST` for data type conversions with error handling\n",
    "- Use `FILTER` or `WHERE` clauses to exclude invalid records\n",
    "- Test incrementally: build Bronze first, then Silver, then Gold\n",
    "- Use the Databricks SQL editor or notebook cells to test your queries before creating the pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_LDP_Exercise_Requirements",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
