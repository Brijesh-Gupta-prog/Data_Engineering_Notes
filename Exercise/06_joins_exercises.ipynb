{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06 - Joins - Exercises\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook contains exercises based on the concepts learned in Module 06.\n",
    "\n",
    "- Complete each exercise in the provided code cells\n",
    "- Run the data setup cells first to generate/create necessary data\n",
    "- Test your solutions by running the verification cells (if provided)\n",
    "- Refer back to the main module notebook if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up the data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 06 Exercises\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# Create data for join exercises\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Sales\", 50000),\n",
    "    (2, \"Bob\", \"IT\", 60000),\n",
    "    (3, \"Charlie\", \"Sales\", 70000),\n",
    "    (4, \"Diana\", \"HR\", 55000),\n",
    "    (5, \"Eve\", \"IT\", 65000),\n",
    "    (6, \"Frank\", \"Marketing\", 52000),  # Department not in departments initially\n",
    "    (7, \"Grace\", \"Sales\", 58000),\n",
    "    (8, \"Henry\", \"IT\", 62000),\n",
    "    (9, \"Iris\", \"HR\", 54000),\n",
    "    (10, \"Jack\", \"Finance\", 60000)\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "departments_data = [\n",
    "    (\"Sales\", \"New York\", 100000),\n",
    "    (\"IT\", \"San Francisco\", 150000),\n",
    "    (\"HR\", \"Chicago\", 80000),\n",
    "    (\"Finance\", \"Boston\", 90000),  # Department not in employees initially\n",
    "    (\"Marketing\", \"Los Angeles\", 70000)\n",
    "]\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Budget\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_departments = spark.createDataFrame(departments_data, departments_schema)\n",
    "\n",
    "# Create additional DataFrames for complex join exercises\n",
    "projects_data = [\n",
    "    (1, \"Project Alpha\", \"Sales\"),\n",
    "    (2, \"Project Beta\", \"IT\"),\n",
    "    (3, \"Project Gamma\", \"Sales\"),\n",
    "    (4, \"Project Delta\", \"HR\"),\n",
    "    (5, \"Project Epsilon\", \"IT\")\n",
    "]\n",
    "\n",
    "projects_schema = StructType([\n",
    "    StructField(\"ProjectID\", IntegerType(), True),\n",
    "    StructField(\"ProjectName\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_projects = spark.createDataFrame(projects_data, projects_schema)\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "df_employees.show()\n",
    "\n",
    "print(\"\\nDepartments DataFrame:\")\n",
    "df_departments.show()\n",
    "\n",
    "print(\"\\nProjects DataFrame:\")\n",
    "df_projects.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Complete the following exercises based on the concepts from Module 06.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Inner Join\n",
    "\n",
    "Perform an inner join between df_employees and df_departments on the 'Department' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Left Join\n",
    "\n",
    "Perform a left join to get all employees with their department information (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Broadcast Join\n",
    "\n",
    "Use broadcast join to join df_employees with df_departments (broadcast the smaller table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Right Join\n",
    "\n",
    "Perform a right join to get all departments with their employees (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Full Outer Join\n",
    "\n",
    "Perform a full outer join between df_employees and df_departments on the 'Department' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Left Semi Join\n",
    "\n",
    "Perform a left semi join to get employees whose department exists in the departments table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Left Anti Join\n",
    "\n",
    "Perform a left anti join to get employees whose department does NOT exist in the departments table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Join with Different Column Names\n",
    "\n",
    "Create a new DataFrame df_employees_alt with column 'Dept' instead of 'Department', then join it with df_departments using the join condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# First create df_employees_alt with 'Dept' column\n",
    "# Then perform the join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Multiple Joins\n",
    "\n",
    "Join df_employees with df_departments, then join the result with df_projects on Department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Join with Select Specific Columns\n",
    "\n",
    "Perform an inner join and select only Name, Department, and Location columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: Join with Filter\n",
    "\n",
    "Perform a left join and then filter to show only employees with salary greater than 60000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Join with Aggregate\n",
    "\n",
    "Join df_employees with df_departments, then calculate the average salary per department location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: Self Join\n",
    "\n",
    "Create a self-join on df_employees to find pairs of employees in the same department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use aliases for the same DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14: Join with Multiple Conditions\n",
    "\n",
    "Join df_employees with df_departments on Department, and add an additional condition that Budget > 80000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15: Join and Count\n",
    "\n",
    "Perform an inner join and count the number of employees in each department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16: Join with Order By\n",
    "\n",
    "Perform a left join and order the result by salary in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17: Join with Distinct\n",
    "\n",
    "Perform a join and get distinct department names from the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18: Join with CASE Statement\n",
    "\n",
    "Join df_employees with df_departments and add a column 'BudgetCategory' that is 'High' if Budget >= 100000, else 'Low'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19: Join Performance - Filter Before Join\n",
    "\n",
    "Filter df_employees to only Sales department employees, then join with df_departments. This demonstrates the best practice of filtering before joining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20: Complex Join with Multiple DataFrames\n",
    "\n",
    "Join df_employees, df_departments, and df_projects together. Show employee name, department, location, and project name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Great job completing the exercises! Review your solutions and compare them with the solutions notebook if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
