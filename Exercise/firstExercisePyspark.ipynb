{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXiPRqJKqn3w"
      },
      "source": [
        "# Module 01 - Introduction & SparkSession - Exercises\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This notebook contains exercises based on the concepts learned in Module 01.\n",
        "\n",
        "- Complete each exercise in the provided code cells\n",
        "- Run the data setup cells first to generate/create necessary data\n",
        "- Test your solutions by running the verification cells (if provided)\n",
        "- Refer back to the main module notebook if you need help\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6EyeBcXqn3x"
      },
      "source": [
        "## Data Setup\n",
        "\n",
        "Run the cells below to set up the data needed for the exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHkTFp4bqn3x",
        "outputId": "9269716b-f9d8-46d2-f9b2-3f8394f60eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession created successfully!\n",
            "Data directory: /data\n",
            "Sample CSV file created: employees.csv\n",
            "Sample JSON file created: people.json\n",
            "Sample Parquet file created: products.parquet\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "import os\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Module 01 Exercises\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set data directory\n",
        "data_dir = \"../data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "print(\"SparkSession created successfully!\")\n",
        "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
        "\n",
        "# Create sample CSV file for exercises\n",
        "sample_csv = \"\"\"Name,Age,Department,Salary\n",
        "Alice,25,Sales,50000\n",
        "Bob,30,IT,60000\n",
        "Charlie,35,Sales,70000\n",
        "Diana,28,IT,55000\n",
        "Eve,32,HR,65000\"\"\"\n",
        "\n",
        "with open(f\"{data_dir}/employees.csv\", \"w\") as f:\n",
        "    f.write(sample_csv)\n",
        "print(\"Sample CSV file created: employees.csv\")\n",
        "\n",
        "# Create sample JSON file for exercises\n",
        "sample_json = \"\"\"{\"name\": \"John\", \"age\": 28, \"city\": \"NYC\"}\n",
        "{\"name\": \"Jane\", \"age\": 32, \"city\": \"LA\"}\n",
        "{\"name\": \"Mike\", \"age\": 25, \"city\": \"Chicago\"}\n",
        "{\"name\": \"Sarah\", \"age\": 30, \"city\": \"Houston\"}\"\"\"\n",
        "\n",
        "with open(f\"{data_dir}/people.json\", \"w\") as f:\n",
        "    f.write(sample_json)\n",
        "print(\"Sample JSON file created: people.json\")\n",
        "\n",
        "# Create sample Parquet file for exercises\n",
        "product_data = [\n",
        "    (1, \"Product A\", 100, \"Electronics\"),\n",
        "    (2, \"Product B\", 200, \"Clothing\"),\n",
        "    (3, \"Product C\", 150, \"Electronics\"),\n",
        "    (4, \"Product D\", 300, \"Home\"),\n",
        "    (5, \"Product E\", 250, \"Clothing\")\n",
        "]\n",
        "\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"price\", IntegerType(), True),\n",
        "    StructField(\"category\", StringType(), True)\n",
        "])\n",
        "\n",
        "df_products_temp = spark.createDataFrame(product_data, product_schema)\n",
        "df_products_temp.write.mode(\"overwrite\").parquet(f\"{data_dir}/products.parquet\")\n",
        "print(\"Sample Parquet file created: products.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0iqxxRfqn3y"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Complete the following exercises based on the concepts from Module 01.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLWXPCHcqn3y"
      },
      "source": [
        "### Exercise 1: Create a SparkSession\n",
        "\n",
        "Create a SparkSession with app name 'MyFirstSparkApp' and master set to 'local[*]'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9uvlWZPvqn3y"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "spark = SparkSession.builder \\\n",
        "          .appName('MyFirstSparkApp') \\\n",
        "          .master('local[*]') \\\n",
        "          .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svPlRaKzqn3y"
      },
      "source": [
        "### Exercise 2: Create a DataFrame\n",
        "\n",
        "Create a DataFrame with the following data:\n",
        "- Names: ['John', 'Jane', 'Mike', 'Sarah']\n",
        "- Ages: [28, 32, 25, 30]\n",
        "- Cities: ['NYC', 'LA', 'Chicago', 'Houston']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JH0F5GZqn3y",
        "outputId": "fa9ba97b-561f-49e1-c2e0-f149ad13dd82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name|Age|   City|\n",
            "+-----+---+-------+\n",
            "| John| 28|    NYC|\n",
            "| Jane| 32|     LA|\n",
            "| Mike| 25|Chicago|\n",
            "|Sarah| 30|Houston|\n",
            "+-----+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "data = [\n",
        "    ('John', 28, 'NYC'),\n",
        "    ('Jane', 32, 'LA'),\n",
        "    ('Mike', 25, 'Chicago'),\n",
        "    ('Sarah', 30, 'Houston')\n",
        "]\n",
        "df_2 = spark.createDataFrame(data,['Name','Age','City'])\n",
        "df_2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMRx4Rf9qn3y"
      },
      "source": [
        "### Exercise 3: Display DataFrame\n",
        "\n",
        "Display the first 3 rows of the DataFrame you created in Exercise 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRhDkXnAqn3z",
        "outputId": "0a734e91-ab03-4e86-97af-63eef3673183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------+\n",
            "|Name|Age|   City|\n",
            "+----+---+-------+\n",
            "|John| 28|    NYC|\n",
            "|Jane| 32|     LA|\n",
            "|Mike| 25|Chicago|\n",
            "+----+---+-------+\n",
            "only showing top 3 rows\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df.show(3)\n",
        "# Hint: Use the show() method with the numRows parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mZ1mX9aqn3z"
      },
      "source": [
        "### Exercise 4: Create DataFrame with Explicit Schema\n",
        "\n",
        "Create a DataFrame with the following data and explicit schema:\n",
        "- Data: [(\"Product A\", 100.50, 10), (\"Product B\", 200.75, 5), (\"Product C\", 150.25, 8)]\n",
        "- Schema:\n",
        "  - Name: StringType\n",
        "  - Price: DoubleType\n",
        "  - Quantity: IntegerType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGW26eOsqn3z",
        "outputId": "9bd55c9e-bebb-41a1-820a-1dd0a33c7d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------+\n",
            "|product_id|product_price|No_Of_Product|\n",
            "+----------+-------------+-------------+\n",
            "| Product A|        100.5|           10|\n",
            "| Product B|       200.75|            5|\n",
            "| Product C|       150.25|            8|\n",
            "+----------+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "product = [(\"Product A\", 100.50, 10), (\"Product B\", 200.75, 5), (\"Product C\", 150.25, 8)]\n",
        "product_schema = StructType([\n",
        "    StructField('product_id',StringType(),True),\n",
        "    StructField('product_price',DoubleType(),True),\n",
        "    StructField('No_Of_Product',IntegerType(),True)\n",
        "])\n",
        "df_4 = spark.createDataFrame(product,product_schema)\n",
        "df_4.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCMI0O45qn3z"
      },
      "source": [
        "### Exercise 5: Create DataFrame using spark.range\n",
        "\n",
        "Create a DataFrame using `spark.range()` that contains numbers from 1 to 20 (inclusive of 1, exclusive of 20). Then display the DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rRRen4Qqn3z",
        "outputId": "7cc294eb-d1c2-4932-d6cc-371a0c17a12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "| 11|\n",
            "| 12|\n",
            "| 13|\n",
            "| 14|\n",
            "| 15|\n",
            "| 16|\n",
            "| 17|\n",
            "| 18|\n",
            "| 19|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df = spark.range(1,20)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0z_hETyqn3z"
      },
      "source": [
        "### Exercise 6: Create DataFrame from CSV File\n",
        "\n",
        "Read the CSV file `employees.csv` from the data directory using `spark.read`. Use schema inference (header=True, inferSchema=True). Display the DataFrame and print its schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N9MNh-Pqn3z",
        "outputId": "82b80ee3-e1e4-4aea-ad10-e0d8c28428e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|Age|Department|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|     Sales| 50000|\n",
            "|    Bob| 30|        IT| 60000|\n",
            "|Charlie| 35|     Sales| 70000|\n",
            "|  Diana| 28|        IT| 55000|\n",
            "|    Eve| 32|        HR| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_6 = spark.read \\\n",
        "      .format('csv') \\\n",
        "      .option('header','true') \\\n",
        "      .option('inferSchema','true') \\\n",
        "      .load(f'{data_dir}/employees.csv')\n",
        "df_6.show()\n",
        "df_6.printSchema()\n",
        "# Hint: Use spark.read.format(\"csv\") with appropriate options\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3FFwgU5qn3z"
      },
      "source": [
        "### Exercise 7: Create DataFrame from JSON File\n",
        "\n",
        "Read the JSON file `people.json` from the data directory using `spark.read`. Display the DataFrame and print its schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68n6z-Wkqn3z",
        "outputId": "8d91ae2a-d02b-4a50-caf0-4c45d3b942ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----+\n",
            "|age|   city| name|\n",
            "+---+-------+-----+\n",
            "| 28|    NYC| John|\n",
            "| 32|     LA| Jane|\n",
            "| 25|Chicago| Mike|\n",
            "| 30|Houston|Sarah|\n",
            "+---+-------+-----+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df = spark.read \\\n",
        "     .format(\"json\") \\\n",
        "     .option(\"header\",\"true\") \\\n",
        "     .option(\"inferSchema\",\"true\") \\\n",
        "     .load(f\"{data_dir}/people.json\")\n",
        "df.show()\n",
        "df.printSchema()\n",
        "# Hint: Use spark.read.format(\"json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl3Lv_LLqn3z"
      },
      "source": [
        "### Exercise 8: Create DataFrame using spark.sql\n",
        "\n",
        "First, create a temporary view from the DataFrame you created in Exercise 2 (the one with John, Jane, Mike, Sarah). Then use `spark.sql()` to create a new DataFrame that selects only the Name and Age columns where Age is greater than 26. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa6yqxZUqn3z",
        "outputId": "04d8c5a2-5c47-424d-e185-2cd095891d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "| John| 28|\n",
            "| Jane| 32|\n",
            "|Sarah| 30|\n",
            "+-----+---+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_2.createOrReplaceTempView(\"random\")\n",
        "df = spark.sql('SELECT Name,Age from random where age>26')\n",
        "df.show()\n",
        "df.printSchema()\n",
        "# Hint: Use createOrReplaceTempView() to register the DataFrame, then use spark.sql()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnJkdAMsqn3z"
      },
      "source": [
        "### Exercise 9: Create DataFrame from CSV with Explicit Schema\n",
        "\n",
        "Read the `employees.csv` file again, but this time use an explicit schema instead of schema inference. Define a schema with:\n",
        "- Name: StringType\n",
        "- Age: IntegerType\n",
        "- Department: StringType\n",
        "- Salary: IntegerType\n",
        "\n",
        "Display the DataFrame and print its schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC_9OVJhqn3z",
        "outputId": "b7568c9c-a083-49ef-ce7f-eb9c8be7c316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+------+\n",
            "|   Name| Age|Department|Salary|\n",
            "+-------+----+----------+------+\n",
            "|   Name|NULL|Department|  NULL|\n",
            "|  Alice|  25|     Sales| 50000|\n",
            "|    Bob|  30|        IT| 60000|\n",
            "|Charlie|  35|     Sales| 70000|\n",
            "|  Diana|  28|        IT| 55000|\n",
            "|    Eve|  32|        HR| 65000|\n",
            "+-------+----+----------+------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "schema = StructType([\n",
        "    StructField(\"Name\",StringType(),True),\n",
        "    StructField(\"Age\",IntegerType(),True),\n",
        "    StructField(\"Department\",StringType(),True),\n",
        "    StructField(\"Salary\",IntegerType(),True)\n",
        "])\n",
        "df = spark.read.schema(schema).csv(f\"{data_dir}/employees.csv\")\n",
        "df.show()\n",
        "df.printSchema()\n",
        "# Hint: Define a StructType schema first, then use .schema() in the read operation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg4L5ZUwqn30"
      },
      "source": [
        "### Exercise 10: Access DataFrame Schema Information\n",
        "\n",
        "Using the DataFrame from Exercise 6 (CSV file), print the schema and then access the data type of the \"Age\" column from the schema. Display both the full schema and the specific column's data type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-cQUhieqn30",
        "outputId": "c4130b47-5b75-445b-c399-54c521e68d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "IntegerType()\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_6.printSchema()\n",
        "data_type = df_6.schema['Age'].dataType\n",
        "print(data_type)\n",
        "# Hint: Use printSchema() and then access schema['ColumnName'].dataType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOmdHeZUqn30"
      },
      "source": [
        "### Exercise 11: Get DataFrame Basic Information\n",
        "\n",
        "Using the DataFrame from Exercise 2, get and print:\n",
        "1. The number of rows\n",
        "2. The list of column names\n",
        "3. The number of columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg5JBuW-qn30",
        "outputId": "de6c94f3-5a81-4a43-c20f-ba425ca982e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows 4\n",
            "The list of column names ['Name', 'Age', 'City']\n",
            "The number of columns 3\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "print(f\"Number of Rows {df_2.count()}\")\n",
        "print(f\"The list of column names {df_2.columns}\")\n",
        "print(f\"The number of columns {len(df_2.columns)}\")\n",
        "# Hint: Use count(), columns, and len()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHD7xN6Aqn30"
      },
      "source": [
        "### Exercise 12: Create DataFrame using spark.table\n",
        "\n",
        "First, create a temporary view called \"employees_view\" from the DataFrame you created in Exercise 6 (CSV DataFrame). Then use `spark.table()` to read from this view and create a new DataFrame. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiBU-973qn30",
        "outputId": "8d3c512e-c295-4571-a395-ef79f6d01d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|Age|Department|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|     Sales| 50000|\n",
            "|    Bob| 30|        IT| 60000|\n",
            "|Charlie| 35|     Sales| 70000|\n",
            "|  Diana| 28|        IT| 55000|\n",
            "|    Eve| 32|        HR| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_6.createOrReplaceTempView(\"employees_view\")\n",
        "df_11 = spark.table(\"employees_view\")\n",
        "df_11.show()\n",
        "# Hint: Use createOrReplaceTempView() first, then spark.table()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6lt2qtDqn30"
      },
      "source": [
        "### Exercise 13: Create DataFrame using spark.sql with VALUES\n",
        "\n",
        "Use `spark.sql()` with the VALUES clause to create a DataFrame with the following data:\n",
        "- ('Apple', 1.50, 10)\n",
        "- ('Banana', 0.75, 20)\n",
        "- ('Orange', 2.00, 15)\n",
        "\n",
        "Name the columns: Product, Price, Stock. Display the DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-OgGF37qn30",
        "outputId": "0fe86b7a-d998-46db-9739-9a6175717ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|Product|Price|Stock|\n",
            "+-------+-----+-----+\n",
            "|  Apple| 1.50|   10|\n",
            "| Banana| 0.75|   20|\n",
            "| Orange| 2.00|   15|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_13 = spark.sql(\"SELECT * FROM VALUES ('Apple', 1.50, 10),('Banana', 0.75, 20),('Orange', 2.00, 15) AS t(Product,Price,Stock)\")\n",
        "df_13.show()\n",
        "# Hint: Use spark.sql(\"SELECT * FROM VALUES (...) AS t(columns)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3Gtu21qn30"
      },
      "source": [
        "### Exercise 14: Create DataFrame from Parquet File\n",
        "\n",
        "Read the Parquet file `products.parquet` from the data directory using `spark.read`. Display the DataFrame, print its schema, and show the number of rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n49jcnuqn30",
        "outputId": "ab142d64-319c-4d9f-b644-ba8502cafd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----+-----------+\n",
            "|product_id|product_name|price|   category|\n",
            "+----------+------------+-----+-----------+\n",
            "|         3|   Product C|  150|Electronics|\n",
            "|         4|   Product D|  300|       Home|\n",
            "|         5|   Product E|  250|   Clothing|\n",
            "|         1|   Product A|  100|Electronics|\n",
            "|         2|   Product B|  200|   Clothing|\n",
            "+----------+------------+-----+-----------+\n",
            "\n",
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n",
            "Number of rows 5\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_14 = spark.read \\\n",
        "        .format(\"parquet\") \\\n",
        "        .load(f\"{data_dir}/products.parquet\")\n",
        "\n",
        "df_14.show()\n",
        "df_14.printSchema()\n",
        "print(f\"Number of rows {df_14.count()}\")\n",
        "# Hint: Use spark.read.format(\"parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbX9IRjeqn30"
      },
      "source": [
        "### Exercise 15: Create DataFrame using .toDF() Method\n",
        "\n",
        "Create a DataFrame from a list of tuples without specifying column names in `createDataFrame()`. Then use the `.toDF()` method to assign column names: \"Student\", \"Score\", \"Grade\". Display the DataFrame.\n",
        "\n",
        "Data: [(\"Alice\", 95, \"A\"), (\"Bob\", 87, \"B\"), (\"Charlie\", 92, \"A\")]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3opUut9qn30",
        "outputId": "183639f2-6c98-47a7-d176-da2eab07bbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|   Name|Marks|Grade|\n",
            "+-------+-----+-----+\n",
            "|  Alice|   95|    A|\n",
            "|    Bob|   87|    B|\n",
            "|Charlie|   92|    A|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "Data = [(\"Alice\", 95, \"A\"), (\"Bob\", 87, \"B\"), (\"Charlie\", 92, \"A\")]\n",
        "df_15 = spark.createDataFrame(Data).toDF(\"Name\",\"Marks\",\"Grade\")\n",
        "df_15.show()\n",
        "# Hint: spark.createDataFrame(data).toDF(\"col1\", \"col2\", \"col3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_cTJj70qn30"
      },
      "source": [
        "### Exercise 16: Get DataFrame Column Data Types\n",
        "\n",
        "Using the DataFrame from Exercise 4 (products with explicit schema), get and print the data types of all columns. Use the `dtypes` property of the DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvXk8leQqn30",
        "outputId": "0cca602e-27af-4bf8-f554-3ad3d4c537fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column :product_id And Data Types :string\n",
            "Column :product_price And Data Types :double\n",
            "Column :No_Of_Product And Data Types :int\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "for column_name,data_type in df_4.dtypes:\n",
        "  print(f\"Column :{column_name} And Data Types :{data_type}\")\n",
        "# Hint: Use the .dtypes property\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7AmIevaqn30"
      },
      "source": [
        "### Exercise 17: Create DataFrame with spark.range and Custom Step\n",
        "\n",
        "Create a DataFrame using `spark.range()` that contains even numbers from 0 to 20 (exclusive of 20). Use the step parameter. Display the DataFrame and verify it contains only even numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkMVbHigqn31",
        "outputId": "7d09ba10-4dd7-41f1-de0f-36bc64b50f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  2|\n",
            "|  4|\n",
            "|  6|\n",
            "|  8|\n",
            "| 10|\n",
            "| 12|\n",
            "| 14|\n",
            "| 16|\n",
            "| 18|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_17 = spark.range(0,20,2)\n",
        "df_17.show()\n",
        "# Hint: spark.range(start, end, step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrIewyNEqn31"
      },
      "source": [
        "### Exercise 18: Check if DataFrame is Empty\n",
        "\n",
        "Create an empty DataFrame using `spark.range(0, 0)` and check if it's empty using the `isEmpty()` method. Then create a non-empty DataFrame and check again. Print both results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5euaEObqn31",
        "outputId": "267f32eb-60c4-4865-e0cb-9cd68d118dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_first = spark.range(0,0)\n",
        "print(df_first.isEmpty())\n",
        "df_second = spark.range(3,16)\n",
        "print(df_second.isEmpty())\n",
        "# Hint: Use isEmpty() method on DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JR0y30Eqn31"
      },
      "source": [
        "### Exercise 19: Access Schema Fields\n",
        "\n",
        "Using the DataFrame from Exercise 4, access the schema object and print:\n",
        "1. The number of fields in the schema\n",
        "2. The name and data type of the first field\n",
        "3. The name and data type of the \"Price\" field\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6NrDxE3qn32",
        "outputId": "72a80906-9e42-465f-de08-ed3d3565760c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of fields in the schema is 3\n",
            "The name and data type of the first field is StructField('product_id', StringType(), True)\n",
            "field name : product_price and field datatype is DoubleType()\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "print(f\"The number of fields in the schema is {len(df_4.schema.fields)}\")\n",
        "print(f\"The name and data type of the first field is {df_4.schema.fields[0]}\")\n",
        "#print(f\"The name and data type of the price field is {df_4.schema.fields['product_price']}\")\n",
        "field = next((field for field in df_4.schema.fields if field.name == 'product_price'),None)\n",
        "if field:\n",
        "  print(f\"field name : {field.name} and field datatype is {field.dataType}\")\n",
        "# Hint: Use df.schema.fields and access field properties\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2S55DItqn32"
      },
      "source": [
        "### Exercise 20: Create DataFrame - Multiple Methods Comparison\n",
        "\n",
        "Create the same DataFrame using three different methods:\n",
        "1. Using `spark.createDataFrame()` with column names\n",
        "2. Using `spark.createDataFrame()` with `.toDF()`\n",
        "3. Using `spark.sql()` with VALUES\n",
        "\n",
        "Data: [(\"X\", 10), (\"Y\", 20), (\"Z\", 30)]\n",
        "Columns: \"Letter\", \"Number\"\n",
        "\n",
        "Display all three DataFrames to verify they're the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqKyCAMfqn32",
        "outputId": "94ec5409-def2-40bf-971d-bd84b7780268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|Letter|Number|\n",
            "+------+------+\n",
            "|     X|    10|\n",
            "|     Y|    20|\n",
            "|     Z|    30|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|Letter|Number|\n",
            "+------+------+\n",
            "|     X|    10|\n",
            "|     Y|    20|\n",
            "|     Z|    30|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|Letter|Number|\n",
            "+------+------+\n",
            "|     X|    10|\n",
            "|     Y|    20|\n",
            "|     Z|    30|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "Data = [(\"X\", 10), (\"Y\", 20), (\"Z\", 30)]\n",
        "first_method = spark.createDataFrame(Data,[\"Letter\", \"Number\"])\n",
        "second_method = spark.createDataFrame(Data).toDF(\"Letter\", \"Number\")\n",
        "third_method = spark.sql(\"SELECT * FROM VALUES ('X', 10), ('Y', 20), ('Z', 30) AS t(Letter, Number)\")\n",
        "first_method.show()\n",
        "second_method.show()\n",
        "third_method.show()\n",
        "# Create three DataFrames using different methods and display them\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz33U13Pqn32"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Review your solutions and compare them with the solutions notebook if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a6OvgDpqn32"
      },
      "source": [
        "<< end of notebook >>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}