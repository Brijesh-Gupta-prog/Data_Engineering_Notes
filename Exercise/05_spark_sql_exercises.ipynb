{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05 - Spark SQL - Exercises\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook contains exercises based on the concepts learned in Module 05.\n",
    "\n",
    "- Complete each exercise in the provided code cells\n",
    "- Run the data setup cells first to generate/create necessary data\n",
    "- Test your solutions by running the verification cells (if provided)\n",
    "- Refer back to the main module notebook if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up the data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n",
      "Data directory: c:\\Users\\Brijesh.Gupta\\Documents\\data\n",
      "Employee DataFrame created:\n",
      "+-------+---+----------+------+-------------+\n",
      "|   Name|Age|Department|Salary|         City|\n",
      "+-------+---+----------+------+-------------+\n",
      "|  Alice| 25|     Sales| 50000|          NYC|\n",
      "|    Bob| 30|        IT| 60000|           LA|\n",
      "|Charlie| 35|     Sales| 70000|      Chicago|\n",
      "|  Diana| 28|        IT| 55000|      Houston|\n",
      "|    Eve| 32|        HR| 65000|      Phoenix|\n",
      "|  Frank| 27|     Sales| 52000|          NYC|\n",
      "|  Grace| 29|        IT| 58000|           LA|\n",
      "|  Henry| 31|        HR| 62000|      Chicago|\n",
      "|   Iris| 26|     Sales| 48000|          NYC|\n",
      "|   Jack| 33|        IT| 64000|San Francisco|\n",
      "+-------+---+----------+------+-------------+\n",
      "\n",
      "\n",
      "Departments DataFrame created:\n",
      "+----------+-------------+------+\n",
      "|Department|     Location|Budget|\n",
      "+----------+-------------+------+\n",
      "|     Sales|     New York|100000|\n",
      "|        IT|San Francisco|150000|\n",
      "|        HR|      Chicago| 80000|\n",
      "|   Finance|       Boston| 90000|\n",
      "+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import os\n",
    " \n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = (\n",
    "    \"--add-opens=java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\"\n",
    ")\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 05 Exercises\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# Create sample data for Spark SQL exercises\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"NYC\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"LA\"),\n",
    "    (\"Charlie\", 35, \"Sales\", 70000, \"Chicago\"),\n",
    "    (\"Diana\", 28, \"IT\", 55000, \"Houston\"),\n",
    "    (\"Eve\", 32, \"HR\", 65000, \"Phoenix\"),\n",
    "    (\"Frank\", 27, \"Sales\", 52000, \"NYC\"),\n",
    "    (\"Grace\", 29, \"IT\", 58000, \"LA\"),\n",
    "    (\"Henry\", 31, \"HR\", 62000, \"Chicago\"),\n",
    "    (\"Iris\", 26, \"Sales\", 48000, \"NYC\"),\n",
    "    (\"Jack\", 33, \"IT\", 64000, \"San Francisco\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create departments DataFrame\n",
    "departments_data = [\n",
    "    (\"Sales\", \"New York\", 100000),\n",
    "    (\"IT\", \"San Francisco\", 150000),\n",
    "    (\"HR\", \"Chicago\", 80000),\n",
    "    (\"Finance\", \"Boston\", 90000)\n",
    "]\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Budget\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_departments = spark.createDataFrame(departments_data, departments_schema)\n",
    "\n",
    "print(\"Employee DataFrame created:\")\n",
    "df_employees.show()\n",
    "\n",
    "print(\"\\nDepartments DataFrame created:\")\n",
    "df_departments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Complete the following exercises based on the concepts from Module 05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create Temporary View\n",
    "\n",
    "Create a temporary view named 'employees_view' from df_employees using createOrReplaceTempView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_employees.createOrReplaceTempView(\"employee_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Basic SQL Query\n",
    "\n",
    "Write a SQL query to select all employees with salary greater than 55000. Display all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------------+\n",
      "|   Name|Age|Department|Salary|         City|\n",
      "+-------+---+----------+------+-------------+\n",
      "|    Bob| 30|        IT| 60000|           LA|\n",
      "|Charlie| 35|     Sales| 70000|      Chicago|\n",
      "|    Eve| 32|        HR| 65000|      Phoenix|\n",
      "|  Grace| 29|        IT| 58000|           LA|\n",
      "|  Henry| 31|        HR| 62000|      Chicago|\n",
      "|   Jack| 33|        IT| 64000|San Francisco|\n",
      "+-------+---+----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"SELECT * FROM employee_view\n",
    "                   WHERE Salary > 55000    \n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Aggregate SQL Query\n",
    "\n",
    "Write a SQL query to find the average salary by department, ordered by average salary descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|Department|Avarage_Salary|\n",
      "+----------+--------------+\n",
      "|        HR|       63500.0|\n",
      "|        IT|       59250.0|\n",
      "|     Sales|       55000.0|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"SELECT Department,AVG(Salary) AS Avarage_Salary FROM employee_view\n",
    "                   GROUP BY Department\n",
    "                   ORDER BY AVG(Salary) DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Create Global Temporary View\n",
    "\n",
    "Create a global temporary view named 'global_employees_view' from df_employees using createOrReplaceGlobalTempView.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_employees.createOrReplaceGlobalTempView(\"global_employees_view\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: SQL Query with Multiple Conditions\n",
    "\n",
    "Write a SQL query to select employees who are either in the Sales department OR have a salary greater than 60000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------------+\n",
      "|   Name|Age|Department|Salary|         City|\n",
      "+-------+---+----------+------+-------------+\n",
      "|  Alice| 25|     Sales| 50000|          NYC|\n",
      "|    Bob| 30|        IT| 60000|           LA|\n",
      "|Charlie| 35|     Sales| 70000|      Chicago|\n",
      "|  Diana| 28|        IT| 55000|      Houston|\n",
      "|    Eve| 32|        HR| 65000|      Phoenix|\n",
      "|  Frank| 27|     Sales| 52000|          NYC|\n",
      "|  Grace| 29|        IT| 58000|           LA|\n",
      "|  Henry| 31|        HR| 62000|      Chicago|\n",
      "|   Iris| 26|     Sales| 48000|          NYC|\n",
      "|   Jack| 33|        IT| 64000|San Francisco|\n",
      "+-------+---+----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"SELECT * from global_temp.global_employees_view\n",
    "                   WHERE Salary>60000 OR Department=='Sales' \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: SQL Query with ORDER BY\n",
    "\n",
    "Write a SQL query to select all employees, ordered by salary in descending order, then by name in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: SQL Query with COUNT and GROUP BY\n",
    "\n",
    "Write a SQL query to count the number of employees in each department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: SQL Query with HAVING Clause\n",
    "\n",
    "Write a SQL query to find departments with an average salary greater than 55000. Show department and average salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: SQL Query with MIN and MAX\n",
    "\n",
    "Write a SQL query to find the minimum and maximum salary for each department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: SQL Query with LIKE\n",
    "\n",
    "Write a SQL query to select employees whose name starts with 'A'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: SQL Query with IN Clause\n",
    "\n",
    "Write a SQL query to select employees whose city is either 'NYC' or 'LA'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: SQL Query with CASE Statement\n",
    "\n",
    "Write a SQL query to add a new column 'SalaryCategory' that categorizes salaries: 'High' if salary >= 60000, 'Medium' if salary >= 50000, else 'Low'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: SQL Query with Subquery\n",
    "\n",
    "First, create a view for departments. Then write a SQL query to find employees whose department has a budget greater than 90000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# First create departments view\n",
    "# Then write your query\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14: SQL Query with JOIN\n",
    "\n",
    "Create views for both employees and departments. Write a SQL query to join employees with departments on the Department column and show employee name, department, and department location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create views first\n",
    "# Then write your JOIN query\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15: SQL Query with SUM\n",
    "\n",
    "Write a SQL query to calculate the total salary for each department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16: Convert SQL Result to DataFrame\n",
    "\n",
    "Write a SQL query to select all employees, then convert the result to a DataFrame using spark.table() or by assigning the result directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Write SQL query and convert to DataFrame\n",
    "df_result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "print(\"Type:\", type(df_result))\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17: SQL Query with DISTINCT\n",
    "\n",
    "Write a SQL query to find all distinct cities where employees work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18: SQL Query with LIMIT\n",
    "\n",
    "Write a SQL query to select the top 3 employees with the highest salaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19: SQL Query with Aggregate Functions\n",
    "\n",
    "Write a SQL query to find the count, average, minimum, and maximum salary for each department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20: SQL Query with Window Function (ROW_NUMBER)\n",
    "\n",
    "Write a SQL query to rank employees within each department by salary using ROW_NUMBER() window function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result = spark.sql(\"\"\"\n",
    "    \n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Great job completing the exercises! Review your solutions and compare them with the solutions notebook if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
