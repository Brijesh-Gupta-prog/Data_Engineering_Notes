{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 - Introduction & SparkSession - Exercises\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook contains exercises based on the concepts learned in Module 01.\n",
    "\n",
    "- Complete each exercise in the provided code cells\n",
    "- Run the data setup cells first to generate/create necessary data\n",
    "- Test your solutions by running the verification cells (if provided)\n",
    "- Refer back to the main module notebook if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up the data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n",
      "Data directory: /Users/rohityadav/ry_workspace/dev_de_tr/12 Pyspark Structured/data\n",
      "Sample CSV file created: employees.csv\n",
      "Sample JSON file created: people.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:===============================>                          (6 + 5) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Parquet file created: products.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 01 Exercises\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# Create sample CSV file for exercises\n",
    "sample_csv = \"\"\"Name,Age,Department,Salary\n",
    "Alice,25,Sales,50000\n",
    "Bob,30,IT,60000\n",
    "Charlie,35,Sales,70000\n",
    "Diana,28,IT,55000\n",
    "Eve,32,HR,65000\"\"\"\n",
    "\n",
    "with open(f\"{data_dir}/employees.csv\", \"w\") as f:\n",
    "    f.write(sample_csv)\n",
    "print(\"Sample CSV file created: employees.csv\")\n",
    "\n",
    "# Create sample JSON file for exercises\n",
    "sample_json = \"\"\"{\"name\": \"John\", \"age\": 28, \"city\": \"NYC\"}\n",
    "{\"name\": \"Jane\", \"age\": 32, \"city\": \"LA\"}\n",
    "{\"name\": \"Mike\", \"age\": 25, \"city\": \"Chicago\"}\n",
    "{\"name\": \"Sarah\", \"age\": 30, \"city\": \"Houston\"}\"\"\"\n",
    "\n",
    "with open(f\"{data_dir}/people.json\", \"w\") as f:\n",
    "    f.write(sample_json)\n",
    "print(\"Sample JSON file created: people.json\")\n",
    "\n",
    "# Create sample Parquet file for exercises\n",
    "product_data = [\n",
    "    (1, \"Product A\", 100, \"Electronics\"),\n",
    "    (2, \"Product B\", 200, \"Clothing\"),\n",
    "    (3, \"Product C\", 150, \"Electronics\"),\n",
    "    (4, \"Product D\", 300, \"Home\"),\n",
    "    (5, \"Product E\", 250, \"Clothing\")\n",
    "]\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_products_temp = spark.createDataFrame(product_data, product_schema)\n",
    "df_products_temp.write.mode(\"overwrite\").parquet(f\"{data_dir}/products.parquet\")\n",
    "print(\"Sample Parquet file created: products.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Complete the following exercises based on the concepts from Module 01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a SparkSession\n",
    "\n",
    "Create a SparkSession with app name 'MyFirstSparkApp' and master set to 'local[*]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (717479698.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    spark =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "spark = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a DataFrame\n",
    "\n",
    "Create a DataFrame with the following data:\n",
    "- Names: ['John', 'Jane', 'Mike', 'Sarah']\n",
    "- Ages: [28, 32, 25, 30]\n",
    "- Cities: ['NYC', 'LA', 'Chicago', 'Houston']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Display DataFrame\n",
    "\n",
    "Display the first 3 rows of the DataFrame you created in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use the show() method with the numRows parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Create DataFrame with Explicit Schema\n",
    "\n",
    "Create a DataFrame with the following data and explicit schema:\n",
    "- Data: [(\"Product A\", 100.50, 10), (\"Product B\", 200.75, 5), (\"Product C\", 150.25, 8)]\n",
    "- Schema: \n",
    "  - Name: StringType\n",
    "  - Price: DoubleType\n",
    "  - Quantity: IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Create DataFrame using spark.range\n",
    "\n",
    "Create a DataFrame using `spark.range()` that contains numbers from 1 to 20 (inclusive of 1, exclusive of 20). Then display the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Create DataFrame from CSV File\n",
    "\n",
    "Read the CSV file `employees.csv` from the data directory using `spark.read`. Use schema inference (header=True, inferSchema=True). Display the DataFrame and print its schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use spark.read.format(\"csv\") with appropriate options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Create DataFrame from JSON File\n",
    "\n",
    "Read the JSON file `people.json` from the data directory using `spark.read`. Display the DataFrame and print its schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use spark.read.format(\"json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Create DataFrame using spark.sql\n",
    "\n",
    "First, create a temporary view from the DataFrame you created in Exercise 2 (the one with John, Jane, Mike, Sarah). Then use `spark.sql()` to create a new DataFrame that selects only the Name and Age columns where Age is greater than 26. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use createOrReplaceTempView() to register the DataFrame, then use spark.sql()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Create DataFrame from CSV with Explicit Schema\n",
    "\n",
    "Read the `employees.csv` file again, but this time use an explicit schema instead of schema inference. Define a schema with:\n",
    "- Name: StringType\n",
    "- Age: IntegerType\n",
    "- Department: StringType\n",
    "- Salary: IntegerType\n",
    "\n",
    "Display the DataFrame and print its schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Define a StructType schema first, then use .schema() in the read operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Access DataFrame Schema Information\n",
    "\n",
    "Using the DataFrame from Exercise 6 (CSV file), print the schema and then access the data type of the \"Age\" column from the schema. Display both the full schema and the specific column's data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use printSchema() and then access schema['ColumnName'].dataType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: Get DataFrame Basic Information\n",
    "\n",
    "Using the DataFrame from Exercise 2, get and print:\n",
    "1. The number of rows\n",
    "2. The list of column names\n",
    "3. The number of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use count(), columns, and len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Create DataFrame using spark.table\n",
    "\n",
    "First, create a temporary view called \"employees_view\" from the DataFrame you created in Exercise 6 (CSV DataFrame). Then use `spark.table()` to read from this view and create a new DataFrame. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use createOrReplaceTempView() first, then spark.table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: Create DataFrame using spark.sql with VALUES\n",
    "\n",
    "Use `spark.sql()` with the VALUES clause to create a DataFrame with the following data:\n",
    "- ('Apple', 1.50, 10)\n",
    "- ('Banana', 0.75, 20)\n",
    "- ('Orange', 2.00, 15)\n",
    "\n",
    "Name the columns: Product, Price, Stock. Display the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use spark.sql(\"SELECT * FROM VALUES (...) AS t(columns)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14: Create DataFrame from Parquet File\n",
    "\n",
    "Read the Parquet file `products.parquet` from the data directory using `spark.read`. Display the DataFrame, print its schema, and show the number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use spark.read.format(\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15: Create DataFrame using .toDF() Method\n",
    "\n",
    "Create a DataFrame from a list of tuples without specifying column names in `createDataFrame()`. Then use the `.toDF()` method to assign column names: \"Student\", \"Score\", \"Grade\". Display the DataFrame.\n",
    "\n",
    "Data: [(\"Alice\", 95, \"A\"), (\"Bob\", 87, \"B\"), (\"Charlie\", 92, \"A\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: spark.createDataFrame(data).toDF(\"col1\", \"col2\", \"col3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16: Get DataFrame Column Data Types\n",
    "\n",
    "Using the DataFrame from Exercise 4 (products with explicit schema), get and print the data types of all columns. Use the `dtypes` property of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use the .dtypes property\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17: Create DataFrame with spark.range and Custom Step\n",
    "\n",
    "Create a DataFrame using `spark.range()` that contains even numbers from 0 to 20 (exclusive of 20). Use the step parameter. Display the DataFrame and verify it contains only even numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: spark.range(start, end, step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18: Check if DataFrame is Empty\n",
    "\n",
    "Create an empty DataFrame using `spark.range(0, 0)` and check if it's empty using the `isEmpty()` method. Then create a non-empty DataFrame and check again. Print both results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use isEmpty() method on DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19: Access Schema Fields\n",
    "\n",
    "Using the DataFrame from Exercise 4, access the schema object and print:\n",
    "1. The number of fields in the schema\n",
    "2. The name and data type of the first field\n",
    "3. The name and data type of the \"Price\" field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use df.schema.fields and access field properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20: Create DataFrame - Multiple Methods Comparison\n",
    "\n",
    "Create the same DataFrame using three different methods:\n",
    "1. Using `spark.createDataFrame()` with column names\n",
    "2. Using `spark.createDataFrame()` with `.toDF()`\n",
    "3. Using `spark.sql()` with VALUES\n",
    "\n",
    "Data: [(\"X\", 10), (\"Y\", 20), (\"Z\", 30)]\n",
    "Columns: \"Letter\", \"Number\"\n",
    "\n",
    "Display all three DataFrames to verify they're the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create three DataFrames using different methods and display them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Review your solutions and compare them with the solutions notebook if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<< end of notebook >>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
