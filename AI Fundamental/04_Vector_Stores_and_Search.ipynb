{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89841f73-d569-4821-9224-09e9336ef492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 4: Vector Stores and Search\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what vector databases are and why they're important\n",
    "- Learn about vector similarity and distance metrics\n",
    "- Explore vector search strategies and algorithms\n",
    "- Understand filtering and re-ranking techniques\n",
    "- Learn about common vector database use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe329628-67b8-4b90-b503-076678bd395e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Introduction to Vector Stores\n",
    "\n",
    "### 1.1 What are Vector Databases?\n",
    "\n",
    "**Vector databases** (also called vector stores) are specialized databases designed to store, index, and query high-dimensional vector embeddings efficiently.\n",
    "\n",
    "**Key Characteristics**:\n",
    "- Optimized for similarity search\n",
    "- Handle high-dimensional vectors (typically 100-2000+ dimensions)\n",
    "- Support fast nearest neighbor queries\n",
    "- Store metadata alongside vectors\n",
    "\n",
    "### 1.2 In RAG Architecture\n",
    "\n",
    "In RAG architecture, **contextual information is stored as vectors**:\n",
    "\n",
    "1. Documents are converted to embeddings (vectors)\n",
    "2. Vectors are stored in a vector database\n",
    "3. Queries are converted to embeddings\n",
    "4. Vector database finds similar document vectors\n",
    "5. Retrieved documents provide context for generation\n",
    "\n",
    "### 1.3 Query Interface for Vector Databases\n",
    "\n",
    "**Primary Operation**: Similarity Search\n",
    "\n",
    "**Typical Query**:\n",
    "```python\n",
    "# Pseudo-code\n",
    "query_vector = embed(\"What is RAG?\")\n",
    "results = vector_db.similarity_search(\n",
    "    query_vector,\n",
    "    top_k=5,\n",
    "    filter={\"category\": \"AI\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Returns**: Most similar vectors (documents/chunks) to the query vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6495e74a-8ceb-4cfd-80e5-379d92be6fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Why are Vector Databases So Hot?\n",
    "\n",
    "### 2.1 Query Time and Scalability\n",
    "\n",
    "**Traditional Databases**:\n",
    "- Designed for exact matches\n",
    "- Index on keywords\n",
    "- Don't handle semantic similarity well\n",
    "\n",
    "**Vector Databases**:\n",
    "- Optimized for similarity search\n",
    "- Handle millions/billions of vectors\n",
    "- Sub-second query times even at scale\n",
    "\n",
    "**Example**:\n",
    "- Traditional: \"Find documents with keyword 'machine learning'\"\n",
    "- Vector: \"Find documents semantically similar to 'AI and data science'\"\n",
    "\n",
    "### 2.2 Approximate Nearest Neighbor (ANN)\n",
    "\n",
    "**Challenge**: Exact nearest neighbor search is too slow for large datasets\n",
    "\n",
    "**Solution**: Approximate Nearest Neighbor (ANN) algorithms\n",
    "\n",
    "**Trade-off**: \n",
    "- **Accuracy**: Slightly less precise (but still very good)\n",
    "- **Speed**: Orders of magnitude faster\n",
    "\n",
    "**Why it works**: For RAG, we don't need the absolute best match - we need good enough matches quickly.\n",
    "\n",
    "### 2.3 Organize Embeddings into Indices\n",
    "\n",
    "Vector databases use sophisticated indexing structures:\n",
    "- **HNSW**: Hierarchical Navigable Small World graphs\n",
    "- **IVF**: Inverted File Index\n",
    "- **LSH**: Locality Sensitive Hashing\n",
    "- **Trees**: Various tree-based structures\n",
    "\n",
    "**Purpose**: Enable fast similarity search at scale\n",
    "\n",
    "### 2.4 Inherent Database Properties\n",
    "\n",
    "Vector databases provide traditional database features:\n",
    "\n",
    "- **CRUD Operations**: Create, Read, Update, Delete vectors\n",
    "- **Metadata Storage**: Store additional information with vectors\n",
    "- **Filtering**: Filter by metadata before/after search\n",
    "- **Scalability**: Handle growing datasets\n",
    "- **Persistence**: Data survives restarts\n",
    "- **Concurrency**: Handle multiple queries simultaneously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0931a65-4e38-45f4-b39a-545df2b8b364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Common Use Cases for Vector Databases\n",
    "\n",
    "### 3.1 RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**Primary Use Case**: Store document embeddings for RAG applications\n",
    "\n",
    "**Flow**:\n",
    "1. Store document chunks as vectors\n",
    "2. Query with question embedding\n",
    "3. Retrieve relevant chunks\n",
    "4. Use in RAG pipeline\n",
    "\n",
    "### 3.2 Recommendation Engines\n",
    "\n",
    "**Use Case**: Find similar items, users, or content\n",
    "\n",
    "**Examples**:\n",
    "- Product recommendations\n",
    "- Content recommendations\n",
    "- User similarity matching\n",
    "\n",
    "**How**: Embed items/users, find similar vectors\n",
    "\n",
    "### 3.3 Similarity Search\n",
    "\n",
    "**Use Case**: Find similar items in large collections\n",
    "\n",
    "**Examples**:\n",
    "- Image similarity search\n",
    "- Document deduplication\n",
    "- Finding similar code snippets\n",
    "- Semantic search in knowledge bases\n",
    "\n",
    "### 3.4 Other Use Cases\n",
    "\n",
    "- **Anomaly Detection**: Find outliers in vector space\n",
    "- **Clustering**: Group similar items\n",
    "- **Classification**: Use vector similarity for classification\n",
    "- **Semantic Search**: Beyond keyword matching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6bab180-44e7-451c-8eba-b342c1563c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. A Sample Use Case: Spotify's Natural Language Search\n",
    "\n",
    "### Problem\n",
    "\n",
    "**Challenge**: Users search for music using natural language queries with many variations:\n",
    "- \"upbeat workout songs\"\n",
    "- \"songs for working out that are energetic\"\n",
    "- \"high energy exercise music\"\n",
    "- \"pump up songs\"\n",
    "\n",
    "**Traditional Approach Limitations**:\n",
    "- **Fuzzy matching**: Can't capture all variations\n",
    "- **Normalization**: Limited effectiveness\n",
    "- **Manual aliases**: Don't scale, miss variations\n",
    "\n",
    "### Solution: Vector Search\n",
    "\n",
    "**Approach**: Use embeddings to capture semantic meaning\n",
    "\n",
    "**How it works**:\n",
    "1. Embed songs with metadata (genre, mood, tempo, lyrics)\n",
    "2. Embed user queries\n",
    "3. Find songs with similar embeddings to query\n",
    "4. Return ranked results\n",
    "\n",
    "**Benefits**:\n",
    "- Understands semantic similarity\n",
    "- Handles query variations naturally\n",
    "- No manual aliases needed\n",
    "- Improves with better embedding models\n",
    "\n",
    "**Result**: Better search experience, higher user satisfaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39abc4d-5842-4058-a03b-6255b3b383d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Vector Search Process and Performance\n",
    "\n",
    "### 5.1 Vector Similarity\n",
    "\n",
    "**Core Concept**: Measure how similar two vectors are\n",
    "\n",
    "**Similar vectors** = Similar meaning\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Query: \"machine learning algorithms\"\n",
    "Document 1: \"ML models and techniques\" → High similarity\n",
    "Document 2: \"cooking recipes\" → Low similarity\n",
    "```\n",
    "\n",
    "### 5.2 Distance Metrics\n",
    "\n",
    "Distance metrics measure how far apart vectors are in the embedding space.\n",
    "\n",
    "#### Euclidean Distance (L2)\n",
    "\n",
    "**Formula**: √(Σ(xᵢ - yᵢ)²)\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Point A: (2, 3)\n",
    "Point B: (5, 7)\n",
    "Distance: √((5-2)² + (7-3)²) = √(9 + 16) = 5\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- Measures straight-line distance\n",
    "- Sensitive to magnitude\n",
    "- Good for dense vectors\n",
    "\n",
    "**Graph Representation**:\n",
    "```\n",
    "    y\n",
    "    |\n",
    "    |     B (5,7)\n",
    "    |    /\n",
    "    |   /\n",
    "    |  /\n",
    "    | /\n",
    "    |/________ x\n",
    "   A (2,3)\n",
    "```\n",
    "\n",
    "#### Manhattan Distance (L1)\n",
    "\n",
    "**Formula**: Σ|xᵢ - yᵢ|\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Point A: (2, 3)\n",
    "Point B: (5, 7)\n",
    "Distance: |5-2| + |7-3| = 3 + 4 = 7\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- Measures city-block distance\n",
    "- Sum of absolute differences\n",
    "- Less sensitive to outliers\n",
    "\n",
    "**Graph Representation**:\n",
    "```\n",
    "    y\n",
    "    |\n",
    "    |     B\n",
    "    |    |\n",
    "    |    |\n",
    "    |    |\n",
    "    |____|____ x\n",
    "   A\n",
    "```\n",
    "\n",
    "### 5.3 Similarity Metrics\n",
    "\n",
    "#### Cosine Similarity\n",
    "\n",
    "**Formula**: (A · B) / (||A|| × ||B||)\n",
    "\n",
    "**Range**: -1 to 1 (typically 0 to 1 for normalized vectors)\n",
    "\n",
    "**Characteristics**:\n",
    "- Measures angle between vectors\n",
    "- Not affected by vector magnitude\n",
    "- Most common for text embeddings\n",
    "- Range: -1 (opposite) to 1 (identical)\n",
    "\n",
    "**Why it's popular**:\n",
    "- Normalized vectors → cosine similarity = dot product\n",
    "- Focuses on direction, not magnitude\n",
    "- Works well for text embeddings\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Vector A: [1, 2, 3]\n",
    "Vector B: [2, 4, 6]  (same direction, different magnitude)\n",
    "Cosine Similarity: 1.0 (perfectly similar direction)\n",
    "```\n",
    "\n",
    "#### Dot Product\n",
    "\n",
    "**Formula**: A · B = Σ(xᵢ × yᵢ)\n",
    "\n",
    "**Characteristics**:\n",
    "- Simple and fast\n",
    "- Affected by vector magnitude\n",
    "- Good when vectors are normalized\n",
    "- Equivalent to cosine similarity for normalized vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a38bb4-e45e-4564-8cea-1f311e3fe821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Vector Search Strategies and Their Details\n",
    "\n",
    "### 6.1 K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Approach**: Find the k closest vectors to the query\n",
    "\n",
    "**Algorithm**: \n",
    "- Calculate distance to all vectors\n",
    "- Sort by distance\n",
    "- Return top k\n",
    "\n",
    "**Characteristics**:\n",
    "- **Exact**: Finds true nearest neighbors\n",
    "- **Slow**: O(n) for n vectors\n",
    "- **Use Case**: Small datasets (< 1M vectors)\n",
    "\n",
    "### 6.2 Approximate Nearest Neighbors (ANN)\n",
    "\n",
    "**Approach**: Find approximately the k closest vectors\n",
    "\n",
    "**Trade-off**: \n",
    "- **Accuracy**: Slightly less precise (but still very good)\n",
    "- **Speed**: Orders of magnitude faster\n",
    "\n",
    "**Why it works**: For RAG, we don't need the absolute best match - we need good enough matches quickly.\n",
    "\n",
    "### 6.3 Tree-Based: ANNOY (by Spotify)\n",
    "\n",
    "**Algorithm**: Approximate Nearest Neighbors Oh Yeah\n",
    "\n",
    "**How it works**:\n",
    "- Build binary trees by splitting space\n",
    "- Navigate trees to find nearest neighbors\n",
    "- Multiple trees for better accuracy\n",
    "\n",
    "**Characteristics**:\n",
    "- **Speed**: Fast query time\n",
    "- **Memory**: Efficient storage\n",
    "- **Accuracy**: Good with enough trees\n",
    "- **Use Case**: Medium to large datasets\n",
    "\n",
    "**Trade-offs**:\n",
    "- Build time can be slow\n",
    "- Accuracy depends on number of trees\n",
    "\n",
    "### 6.4 Clustering: FAISS (by Facebook)\n",
    "\n",
    "**Algorithm**: Facebook AI Similarity Search\n",
    "\n",
    "**How it works**:\n",
    "- Cluster vectors into groups\n",
    "- Search within relevant clusters\n",
    "- Use quantization for compression\n",
    "\n",
    "**Characteristics**:\n",
    "- **Speed**: Very fast\n",
    "- **Scalability**: Handles billions of vectors\n",
    "- **Memory**: Can use GPU acceleration\n",
    "- **Flexibility**: Many index types\n",
    "\n",
    "**Index Types**:\n",
    "- **IVF (Inverted File Index)**: Clustering-based\n",
    "- **HNSW**: Graph-based (also available)\n",
    "- **PQ (Product Quantization)**: Compression\n",
    "\n",
    "**Use Case**: Large-scale production systems\n",
    "\n",
    "### 6.5 Proximity Graphs: HNSW\n",
    "\n",
    "**Algorithm**: Hierarchical Navigable Small World\n",
    "\n",
    "**How it works**:\n",
    "- Build multi-layer graph\n",
    "- Each layer has fewer nodes\n",
    "- Navigate from top (sparse) to bottom (dense)\n",
    "- Greedy search through graph\n",
    "\n",
    "**Characteristics**:\n",
    "- **Speed**: Very fast query time\n",
    "- **Accuracy**: High quality results\n",
    "- **Memory**: Moderate memory usage\n",
    "- **Build Time**: Can be slow for large datasets\n",
    "\n",
    "**Advantages**:\n",
    "- Best accuracy/speed trade-off for many use cases\n",
    "- Widely used in production\n",
    "\n",
    "**Use Case**: Production RAG systems\n",
    "\n",
    "### 6.6 Hashing: LSH (Locality Sensitive Hashing)\n",
    "\n",
    "**Algorithm**: Hash similar vectors to same buckets\n",
    "\n",
    "**How it works**:\n",
    "- Create hash functions that preserve similarity\n",
    "- Similar vectors hash to same bucket\n",
    "- Search only in relevant buckets\n",
    "\n",
    "**Characteristics**:\n",
    "- **Speed**: Very fast\n",
    "- **Memory**: Efficient\n",
    "- **Accuracy**: Depends on hash functions\n",
    "- **Tuning**: Requires careful parameter tuning\n",
    "\n",
    "**Use Case**: Very large scale, when speed is critical\n",
    "\n",
    "### 6.7 Vector Compression: SCaNN (by Google) and Product Quantization (PQ)\n",
    "\n",
    "#### SCaNN (Scalable Nearest Neighbors)\n",
    "\n",
    "**Algorithm**: Google's approach combining multiple techniques\n",
    "\n",
    "**Features**:\n",
    "- Anisotropic quantization\n",
    "- Tree-based search\n",
    "- Optimized for modern hardware\n",
    "\n",
    "**Characteristics**:\n",
    "- **Speed**: Very fast\n",
    "- **Accuracy**: High quality\n",
    "- **Scalability**: Handles large datasets\n",
    "\n",
    "#### Product Quantization (PQ)\n",
    "\n",
    "**Algorithm**: Compress vectors by quantizing sub-vectors\n",
    "\n",
    "**How it works**:\n",
    "- Split vector into sub-vectors\n",
    "- Quantize each sub-vector\n",
    "- Store compressed representation\n",
    "\n",
    "**Benefits**:\n",
    "- **Memory**: Significant reduction\n",
    "- **Speed**: Faster search on compressed vectors\n",
    "- **Trade-off**: Some accuracy loss\n",
    "\n",
    "**Use Case**: When memory is constrained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10ce3c88-b89f-448d-bc30-33cbaa6800e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. How to Filter Only Highly Relevant Documents?\n",
    "\n",
    "### 7.1 The Challenge\n",
    "\n",
    "**Problem**: Initial retrieval may return many results, but not all are highly relevant\n",
    "\n",
    "**Example**:\n",
    "- Query: \"Python machine learning tutorial\"\n",
    "- Initial retrieval returns 20 documents\n",
    "- Only 5 are truly relevant\n",
    "- Need to filter/rank to get the best ones\n",
    "\n",
    "### 7.2 Re-ranking\n",
    "\n",
    "**Re-ranking** is the process of re-scoring and re-ordering retrieved documents to improve relevance.\n",
    "\n",
    "#### Two-Stage Retrieval\n",
    "\n",
    "**Stage 1: Initial Retrieval**\n",
    "- Fast, approximate search\n",
    "- Retrieve many candidates (e.g., top 100)\n",
    "- Uses vector similarity\n",
    "\n",
    "**Stage 2: Re-ranking**\n",
    "- Slower, more accurate\n",
    "- Re-score top candidates\n",
    "- Uses more sophisticated models\n",
    "\n",
    "**Benefits**:\n",
    "- **Speed**: Fast initial retrieval\n",
    "- **Accuracy**: Better final results\n",
    "- **Flexibility**: Can use different models for each stage\n",
    "\n",
    "### 7.3 Re-ranking Models\n",
    "\n",
    "**Types of Re-rankers**:\n",
    "\n",
    "1. **Cross-Encoders**:\n",
    "   - Process query and document together\n",
    "   - More accurate but slower\n",
    "   - Examples: BERT-based cross-encoders\n",
    "\n",
    "2. **Learned Re-rankers**:\n",
    "   - Trained on query-document pairs\n",
    "   - Optimized for relevance\n",
    "   - Examples: Cohere rerank, OpenAI rerank\n",
    "\n",
    "3. **Hybrid Approaches**:\n",
    "   - Combine multiple signals\n",
    "   - Semantic + keyword + metadata\n",
    "\n",
    "### 7.4 Benefits and Challenges of Re-ranking\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "1. **Improved Relevance**: Better final results\n",
    "2. **Flexibility**: Can use different models\n",
    "3. **Cost Efficiency**: Only re-rank top candidates\n",
    "4. **Customization**: Train on your data\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "1. **Latency**: Adds processing time\n",
    "2. **Cost**: Re-ranking models can be expensive\n",
    "3. **Complexity**: Additional component to manage\n",
    "4. **Tuning**: Requires optimization\n",
    "\n",
    "### 7.5 Filtering Strategies\n",
    "\n",
    "**Metadata Filtering**:\n",
    "- Filter by date, category, source before/after search\n",
    "- Reduces search space\n",
    "- Improves relevance\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "results = vector_db.search(\n",
    "    query_vector,\n",
    "    filter={\n",
    "        \"category\": \"technical\",\n",
    "        \"date\": \">2023-01-01\",\n",
    "        \"language\": \"en\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Score Thresholding**:\n",
    "- Only return results above similarity threshold\n",
    "- Reduces noise\n",
    "- May miss relevant results if threshold too high\n",
    "\n",
    "**Hybrid Filtering**:\n",
    "- Combine metadata filters with similarity search\n",
    "- Best of both worlds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315c4f78-cd96-4c1c-82d0-7f11a03cdfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Vector databases** are specialized for similarity search at scale\n",
    "2. **ANN algorithms** enable fast search on large datasets\n",
    "3. **Distance metrics** (cosine, Euclidean) measure vector similarity\n",
    "4. **Multiple strategies** exist (HNSW, FAISS, etc.) with different trade-offs\n",
    "5. **Re-ranking** improves final results but adds complexity\n",
    "\n",
    "### Choosing a Strategy\n",
    "\n",
    "**Consider**:\n",
    "- Dataset size\n",
    "- Query latency requirements\n",
    "- Accuracy needs\n",
    "- Memory constraints\n",
    "- Update frequency\n",
    "\n",
    "**Common Choices**:\n",
    "- **Small datasets (< 1M)**: KNN or simple ANN\n",
    "- **Medium datasets (1M-100M)**: HNSW or FAISS-IVF\n",
    "- **Large datasets (> 100M)**: FAISS with quantization or distributed systems\n",
    "\n",
    "### Next Module: Mosaic AI Vector Search in Databricks\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- Introduction to Mosaic AI Vector Search\n",
    "- How it works in Databricks\n",
    "- Setting up vector search\n",
    "- Integration with the Databricks ecosystem\n",
    "- Practical implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841937a0-2a93-4074-ba55-a5f3ead3eccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Exercise 1**: Explain the difference between KNN and ANN - when would you use each?\n",
    "2. **Exercise 2**: Compare cosine similarity vs Euclidean distance for text embeddings\n",
    "3. **Exercise 3**: Design a two-stage retrieval system (initial retrieval + re-ranking)\n",
    "4. **Exercise 4**: Choose an appropriate vector search strategy for a given use case\n",
    "5. **Exercise 5**: Explain how filtering can improve retrieval quality\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04_Vector_Stores_and_Search",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
