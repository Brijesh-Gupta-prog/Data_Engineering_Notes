{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f22ca0-8421-412c-985a-23e418244bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 1: Prompt Engineering Primer\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what prompts are and their components\n",
    "- Learn fundamental prompt engineering techniques\n",
    "- Explore advanced prompting strategies\n",
    "- Understand benefits and limitations of prompt engineering\n",
    "- Apply best practices for effective prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "614a73aa-6fb1-42cf-a15d-cf5f7e119d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. What is a Prompt?\n",
    "\n",
    "A **prompt** is the input text or instruction given to a language model to generate a desired output. It serves as the communication interface between humans and AI models.\n",
    "\n",
    "### Components of a Good Prompt\n",
    "\n",
    "A well-structured prompt typically consists of:\n",
    "\n",
    "1. **Instruction**: The task you want the model to perform\n",
    "2. **Context**: Background information or relevant details\n",
    "3. **Input/Question**: The specific input or question to process\n",
    "4. **Output Type/Format**: Specification of desired output format\n",
    "\n",
    "### Example: Basic Prompt Structure\n",
    "\n",
    "```\n",
    "Instruction: You are a helpful assistant that explains technical concepts.\n",
    "Context: The user is learning about machine learning.\n",
    "Input: What is gradient descent?\n",
    "Output Format: Provide a clear explanation with examples.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383898a5-627e-4ff6-80c3-32609658c1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. What is Prompt Engineering?\n",
    "\n",
    "**Prompt Engineering** is the practice of designing and refining prompts to:\n",
    "- Elicit desired responses from language models\n",
    "- Improve accuracy and relevance of outputs\n",
    "- Control model behavior and output format\n",
    "- Maximize the effectiveness of AI interactions\n",
    "\n",
    "### Why Prompt Engineering Matters\n",
    "\n",
    "- **Cost Efficiency**: Better prompts reduce the need for multiple API calls\n",
    "- **Quality**: Well-crafted prompts produce more accurate and useful outputs\n",
    "- **Consistency**: Structured prompts ensure reproducible results\n",
    "- **Flexibility**: Enables adaptation to different use cases without model retraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e371249f-fad8-4236-afea-e30f1c5c928b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Prompt Engineering Techniques\n",
    "\n",
    "### 3.1 Zero-Shot Prompting\n",
    "\n",
    "**Zero-shot prompting** involves asking the model to perform a task without providing any examples.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Classify the sentiment of the following text: \"I love this product!\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Simple, straightforward tasks\n",
    "- When the model has sufficient training data for the task\n",
    "- For general-purpose queries\n",
    "\n",
    "**Limitations:**\n",
    "- May not work well for complex or domain-specific tasks\n",
    "- Less control over output format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ecf4f4-8109-4d52-9241-524703935453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 Few-Shot Prompting\n",
    "\n",
    "**Few-shot prompting** provides the model with a few examples to guide its behavior.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Translate the following sentences from English to French:\n",
    "\n",
    "Example 1:\n",
    "English: Hello, how are you?\n",
    "French: Bonjour, comment allez-vous?\n",
    "\n",
    "Example 2:\n",
    "English: I am learning French.\n",
    "French: J'apprends le français.\n",
    "\n",
    "Now translate:\n",
    "English: The weather is nice today.\n",
    "French:\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Complex tasks requiring specific formatting\n",
    "- Domain-specific applications\n",
    "- When you need consistent output structure\n",
    "\n",
    "**Best Practices:**\n",
    "- Use 2-5 examples (more isn't always better)\n",
    "- Ensure examples are diverse and representative\n",
    "- Match the complexity of examples to your task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "549f4348-56c0-457d-905d-1f6619347a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Prompt Chaining\n",
    "\n",
    "**Prompt chaining** breaks complex tasks into smaller, sequential subtasks where the output of one prompt becomes input to the next.\n",
    "\n",
    "**Example: Multi-step Analysis**\n",
    "\n",
    "**Step 1: Extract Key Information**\n",
    "```\n",
    "Extract the main topics from the following article:\n",
    "[Article text]\n",
    "```\n",
    "\n",
    "**Step 2: Analyze Each Topic**\n",
    "```\n",
    "Based on the topics: [topics from step 1], provide a detailed analysis of each.\n",
    "```\n",
    "\n",
    "**Step 3: Generate Summary**\n",
    "```\n",
    "Summarize the following analysis: [analysis from step 2]\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Handles complex, multi-step reasoning\n",
    "- Improves accuracy by breaking down tasks\n",
    "- Enables error checking at each step\n",
    "- More transparent and debuggable\n",
    "\n",
    "**Use Cases:**\n",
    "- Document analysis and summarization\n",
    "- Multi-step problem solving\n",
    "- Content generation pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58d2ea4b-4e08-4cc6-a798-78f2e9b7831e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4 Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "**Chain-of-Thought prompting** encourages the model to show its reasoning process step-by-step.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Solve this math problem step by step:\n",
    "\n",
    "Problem: A store has 15 apples. They sell 6 apples and then receive 10 more. How many apples do they have now?\n",
    "\n",
    "Solution:\n",
    "Step 1: Start with 15 apples\n",
    "Step 2: After selling 6, they have 15 - 6 = 9 apples\n",
    "Step 3: After receiving 10 more, they have 9 + 10 = 19 apples\n",
    "Answer: 19 apples\n",
    "```\n",
    "\n",
    "**Research Findings:**\n",
    "\n",
    "Research in this area shows mixed results:\n",
    "- **Positive**: Improves performance on arithmetic, commonsense, and symbolic reasoning tasks\n",
    "- **Variable**: Effectiveness depends on model size and task complexity\n",
    "- **Key Papers**:\n",
    "  - \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" (Wei et al., 2022)\n",
    "  - \"Large Language Models are Zero-Shot Reasoners\" (Kojima et al., 2022)\n",
    "\n",
    "**When to use:**\n",
    "- Mathematical problem solving\n",
    "- Logical reasoning tasks\n",
    "- Complex analytical questions\n",
    "- When transparency in reasoning is important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f424cbf2-a46d-41c4-aca6-4cb955ad5cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Prompt Engineering Tips and Tricks\n",
    "\n",
    "### Tip 1: Prompts are Model-Specific\n",
    "\n",
    "Different models respond differently to the same prompt:\n",
    "\n",
    "- **GPT-4**: Often benefits from more detailed instructions\n",
    "- **Claude**: Works well with conversational, natural language\n",
    "- **Open-source models**: May require more explicit formatting\n",
    "\n",
    "**Best Practice**: Test and iterate prompts with your specific model.\n",
    "\n",
    "### Tip 2: Format Prompts Clearly\n",
    "\n",
    "Use clear structure and formatting:\n",
    "\n",
    "```\n",
    "### Task\n",
    "[Clear task description]\n",
    "\n",
    "### Context\n",
    "[Relevant background information]\n",
    "\n",
    "### Input\n",
    "[Your input/question]\n",
    "\n",
    "### Output Format\n",
    "[Desired format specification]\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Better model comprehension\n",
    "- Easier to maintain and update\n",
    "- More consistent results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af07fea-af6a-4301-9a12-3c8764e4a85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tip 3: Guide the Model for Better Responses\n",
    "\n",
    "**Techniques:**\n",
    "\n",
    "1. **Role Assignment**: Give the model a specific role\n",
    "   ```\n",
    "   You are an expert data scientist with 10 years of experience...\n",
    "   ```\n",
    "\n",
    "2. **Output Constraints**: Specify what to include/exclude\n",
    "   ```\n",
    "   Provide a response that:\n",
    "   - Is under 200 words\n",
    "   - Uses simple language\n",
    "   - Includes 3 key points\n",
    "   ```\n",
    "\n",
    "3. **Step-by-Step Instructions**: Break down complex tasks\n",
    "   ```\n",
    "   First, identify the main issue.\n",
    "   Second, analyze the root cause.\n",
    "   Finally, suggest solutions.\n",
    "   ```\n",
    "\n",
    "4. **Negative Instructions**: Tell the model what NOT to do\n",
    "   ```\n",
    "   Do not use technical jargon.\n",
    "   Avoid making assumptions.\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c3a38d-0986-4f9d-91e2-e99c1972e908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Benefits and Limitations of Prompt Engineering\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **No Model Retraining Required**: Adapt models to new tasks without fine-tuning\n",
    "2. **Rapid Iteration**: Quickly test and refine approaches\n",
    "3. **Cost Effective**: Lower computational cost compared to fine-tuning\n",
    "4. **Flexibility**: Easy to modify for different use cases\n",
    "5. **Accessibility**: Works with pre-trained models via APIs\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Context Window Constraints**: Limited by model's maximum context length\n",
    "2. **No External Knowledge**: Models can't access real-time or proprietary information\n",
    "3. **Inconsistency**: Results may vary between runs\n",
    "4. **Hallucinations**: Models may generate plausible but incorrect information\n",
    "5. **Limited Control**: Less control compared to fine-tuned models\n",
    "\n",
    "### Addressing Limitations: The Need for RAG\n",
    "\n",
    "**Key Limitation**: For external knowledge, real-time data, or domain-specific information, we need **Retrieval-Augmented Generation (RAG)**.\n",
    "\n",
    "RAG combines:\n",
    "- **Retrieval**: Finding relevant information from external sources\n",
    "- **Augmentation**: Adding retrieved context to prompts\n",
    "- **Generation**: Using the augmented prompt to generate responses\n",
    "\n",
    "This is the focus of the next module!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a5f64a-01f6-45af-bfcc-abae58ab00bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Practical Examples\n",
    "\n",
    "### Example 1: Simple Classification\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Classify the following customer review as positive, negative, or neutral:\n",
    "\n",
    "Review: \"The product arrived quickly and works perfectly!\"\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Example 2: Structured Output\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Extract information from the following text and format as JSON:\n",
    "\n",
    "Text: \"John Smith, a software engineer at Google, can be reached at john.smith@email.com.\"\n",
    "\n",
    "Output format:\n",
    "{\n",
    "  \"name\": \"\",\n",
    "  \"role\": \"\",\n",
    "  \"company\": \"\",\n",
    "  \"email\": \"\"\n",
    "}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Example 3: Multi-Step Task\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "You are a data analyst. Perform the following analysis:\n",
    "\n",
    "Step 1: Identify the main trends in the following data\n",
    "Step 2: Highlight any anomalies or outliers\n",
    "Step 3: Provide actionable insights\n",
    "\n",
    "Data: [your data here]\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61fb1d07-7c28-40c1-95cc-05aabbb05e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Prompts are the interface** between humans and AI models\n",
    "2. **Well-structured prompts** include instruction, context, input, and output format\n",
    "3. **Different techniques** (zero-shot, few-shot, chaining, CoT) suit different tasks\n",
    "4. **Prompt engineering** is model-specific and requires iteration\n",
    "5. **Limitations exist** - especially for external knowledge, which leads us to RAG\n",
    "\n",
    "### Next Module: Introduction to RAG\n",
    "\n",
    "In the next module, we'll explore how **Retrieval-Augmented Generation (RAG)** addresses the limitations of prompt engineering by:\n",
    "- Incorporating external knowledge sources\n",
    "- Providing up-to-date information\n",
    "- Reducing hallucinations\n",
    "- Enabling domain-specific applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c353dd30-0c6e-4f54-9e0c-63a883d3bda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Exercise 1**: Create a zero-shot prompt for sentiment analysis\n",
    "2. **Exercise 2**: Convert the zero-shot prompt to a few-shot prompt with 3 examples\n",
    "3. **Exercise 3**: Design a prompt chain for analyzing a research paper (extract → summarize → critique)\n",
    "4. **Exercise 4**: Create a chain-of-thought prompt for solving a multi-step word problem\n",
    "5. **Exercise 5**: Refine a prompt using the tips and tricks discussed\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01_Prompt_Engineering_Primer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
