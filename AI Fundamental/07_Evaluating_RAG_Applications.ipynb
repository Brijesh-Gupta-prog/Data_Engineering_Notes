{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e1e104-0bc4-47b3-bb65-ff7609df5ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 7: Evaluating RAG Applications\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what components to evaluate in RAG systems\n",
    "- Learn key evaluation metrics for RAG\n",
    "- Use MLflow for RAG evaluation\n",
    "- Implement evaluation best practices\n",
    "- Interpret evaluation results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc98e48-731a-46da-81af-186c7bd83266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Components to Evaluate\n",
    "\n",
    "### 1.1 Chunking\n",
    "\n",
    "**What to evaluate**:\n",
    "- **Method**: Fixed-size, semantic, header-based, etc.\n",
    "- **Size**: Chunk size and overlap\n",
    "- **Quality**: Do chunks preserve semantic meaning?\n",
    "\n",
    "**Metrics**:\n",
    "- Chunk size distribution\n",
    "- Overlap percentage\n",
    "- Semantic coherence\n",
    "- Context preservation\n",
    "\n",
    "### 1.2 Embedding Model\n",
    "\n",
    "**What to evaluate**:\n",
    "- **Model selection**: Is the model appropriate for the domain?\n",
    "- **Quality**: Do embeddings capture semantic relationships?\n",
    "- **Performance**: Query and document embedding alignment\n",
    "\n",
    "**Metrics**:\n",
    "- Embedding quality (semantic similarity)\n",
    "- Query-document alignment\n",
    "- Domain-specific performance\n",
    "- Dimensionality and efficiency\n",
    "\n",
    "### 1.3 Vector Store\n",
    "\n",
    "**What to evaluate**:\n",
    "- **Retrieval**: How well does retrieval work?\n",
    "- **Performance**: Query latency and throughput\n",
    "- **Scalability**: Performance at scale\n",
    "\n",
    "**Metrics**:\n",
    "- Retrieval precision\n",
    "- Retrieval recall\n",
    "- Query latency\n",
    "- Throughput\n",
    "\n",
    "### 1.4 Retrieval and Re-ranker\n",
    "\n",
    "**What to evaluate**:\n",
    "- **Initial retrieval**: Quality of first-stage retrieval\n",
    "- **Re-ranking**: Improvement from re-ranking\n",
    "- **Filtering**: Effectiveness of metadata filters\n",
    "\n",
    "**Metrics**:\n",
    "- Top-k accuracy\n",
    "- Re-ranking improvement\n",
    "- Filter effectiveness\n",
    "- Overall retrieval quality\n",
    "\n",
    "### 1.5 Generator\n",
    "\n",
    "**What to evaluate**:\n",
    "- **Response quality**: Accuracy, relevance, completeness\n",
    "- **Faithfulness**: Is the response grounded in context?\n",
    "- **Format**: Does it meet requirements?\n",
    "\n",
    "**Metrics**:\n",
    "- Answer accuracy\n",
    "- Answer relevancy\n",
    "- Faithfulness\n",
    "- Completeness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc8acf35-2139-4341-b921-fbfb5a105833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Key Evaluation Metrics\n",
    "\n",
    "### 2.1 Context Precision\n",
    "\n",
    "**Definition**: Proportion of retrieved chunks that are relevant to the query\n",
    "\n",
    "**Formula**: Relevant chunks retrieved / Total chunks retrieved\n",
    "\n",
    "**Example**:\n",
    "- Query: \"What is RAG?\"\n",
    "- Retrieved: 5 chunks\n",
    "- Relevant: 4 chunks\n",
    "- Context Precision: 4/5 = 0.8\n",
    "\n",
    "**Interpretation**: Higher is better (1.0 = all retrieved chunks are relevant)\n",
    "\n",
    "### 2.2 Context Relevancy\n",
    "\n",
    "**Definition**: How relevant are the retrieved chunks to the query?\n",
    "\n",
    "**Measures**: \n",
    "- Semantic relevance\n",
    "- Topic alignment\n",
    "- Information usefulness\n",
    "\n",
    "**Evaluation**: \n",
    "- Human evaluation (0-1 scale)\n",
    "- LLM-based evaluation\n",
    "- Semantic similarity scores\n",
    "\n",
    "**Interpretation**: Higher is better\n",
    "\n",
    "### 2.3 Context Recall\n",
    "\n",
    "**Definition**: Proportion of all relevant chunks that were retrieved\n",
    "\n",
    "**Formula**: Relevant chunks retrieved / Total relevant chunks available\n",
    "\n",
    "**Example**:\n",
    "- Total relevant chunks in corpus: 10\n",
    "- Retrieved relevant chunks: 7\n",
    "- Context Recall: 7/10 = 0.7\n",
    "\n",
    "**Interpretation**: Higher is better (1.0 = all relevant chunks retrieved)\n",
    "\n",
    "**Trade-off**: Higher recall may lower precision\n",
    "\n",
    "### 2.4 Faithfulness\n",
    "\n",
    "**Definition**: Is the generated answer faithful to the retrieved context?\n",
    "\n",
    "**Measures**:\n",
    "- No hallucinations\n",
    "- Grounded in provided context\n",
    "- No contradictions\n",
    "\n",
    "**Evaluation**:\n",
    "- Check if answer can be derived from context\n",
    "- Verify claims against context\n",
    "- Detect hallucinations\n",
    "\n",
    "**Interpretation**: Higher is better (1.0 = completely faithful)\n",
    "\n",
    "**Critical**: Low faithfulness = hallucinations = unreliable system\n",
    "\n",
    "### 2.5 Answer Relevancy\n",
    "\n",
    "**Definition**: How relevant is the generated answer to the query?\n",
    "\n",
    "**Measures**:\n",
    "- Does it answer the question?\n",
    "- Is it on-topic?\n",
    "- Is it useful?\n",
    "\n",
    "**Evaluation**:\n",
    "- Human evaluation\n",
    "- LLM-based evaluation\n",
    "- Semantic similarity to expected answer\n",
    "\n",
    "**Interpretation**: Higher is better\n",
    "\n",
    "### 2.6 Answer Correctness\n",
    "\n",
    "**Definition**: Is the generated answer factually correct?\n",
    "\n",
    "**Measures**:\n",
    "- Factual accuracy\n",
    "- Correct information\n",
    "- No errors\n",
    "\n",
    "**Evaluation**:\n",
    "- Compare to ground truth\n",
    "- Fact-checking\n",
    "- Expert review\n",
    "\n",
    "**Interpretation**: Higher is better\n",
    "\n",
    "**Note**: Requires ground truth or reference answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ead8bf0-1bc3-437d-b43e-05d84f20eae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. MLflow Evaluation for RAG\n",
    "\n",
    "### 3.1 Setting Up Evaluation\n",
    "\n",
    "**Create evaluation dataset**:\n",
    "\n",
    "```python\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"query\": \"What is RAG?\",\n",
    "        \"expected_answer\": \"RAG is Retrieval-Augmented Generation...\",\n",
    "        \"context\": [\"chunk1\", \"chunk2\", ...]\n",
    "    },\n",
    "    # ... more examples\n",
    "]\n",
    "```\n",
    "\n",
    "### 3.2 Running Evaluation\n",
    "\n",
    "**Evaluate RAG chain**:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Load model\n",
    "rag_model = mlflow.pyfunc.load_model(\"models:/rag-qa-system/Production\")\n",
    "\n",
    "# Evaluate\n",
    "results = mlflow.evaluate(\n",
    "    model=rag_model,\n",
    "    data=eval_dataset,\n",
    "    evaluators=\"default\",\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"query\",\n",
    "            \"targets\": \"expected_answer\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.3 Custom RAG Evaluators\n",
    "\n",
    "**Create custom evaluator**:\n",
    "\n",
    "```python\n",
    "from mlflow.models import EvaluationMetric\n",
    "\n",
    "def faithfulness_metric(eval_df, builtin_metrics):\n",
    "    # Calculate faithfulness\n",
    "    # Check if answers are grounded in context\n",
    "    return faithfulness_score\n",
    "\n",
    "mlflow.evaluate(\n",
    "    model=rag_model,\n",
    "    data=eval_dataset,\n",
    "    evaluators=\"custom\",\n",
    "    custom_metrics=[\n",
    "        EvaluationMetric(\n",
    "            name=\"faithfulness\",\n",
    "            function=faithfulness_metric\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.4 Logging Evaluation Results\n",
    "\n",
    "**Log to MLflow**:\n",
    "\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    # Run evaluation\n",
    "    results = evaluate_rag(rag_model, eval_dataset)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"context_precision\", results[\"context_precision\"])\n",
    "    mlflow.log_metric(\"context_recall\", results[\"context_recall\"])\n",
    "    mlflow.log_metric(\"faithfulness\", results[\"faithfulness\"])\n",
    "    mlflow.log_metric(\"answer_relevancy\", results[\"answer_relevancy\"])\n",
    "    \n",
    "    # Log evaluation dataset\n",
    "    mlflow.log_table(eval_dataset, \"eval_dataset.json\")\n",
    "    \n",
    "    # Log results\n",
    "    mlflow.log_table(results, \"evaluation_results.json\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5124f8e1-ed8e-48eb-9ffe-9f72feffc67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Evaluation Best Practices\n",
    "\n",
    "### 4.1 Create Comprehensive Test Sets\n",
    "\n",
    "**Include**:\n",
    "- Diverse query types\n",
    "- Different difficulty levels\n",
    "- Edge cases\n",
    "- Domain-specific queries\n",
    "\n",
    "**Size**: \n",
    "- Minimum: 50-100 examples\n",
    "- Recommended: 200+ examples\n",
    "- Production: 1000+ examples\n",
    "\n",
    "### 4.2 Evaluate End-to-End\n",
    "\n",
    "**Don't just evaluate components in isolation**:\n",
    "- Evaluate full RAG chain\n",
    "- Test real-world scenarios\n",
    "- Measure user-facing metrics\n",
    "\n",
    "### 4.3 Use Multiple Metrics\n",
    "\n",
    "**Don't rely on a single metric**:\n",
    "- Combine retrieval and generation metrics\n",
    "- Balance precision and recall\n",
    "- Consider faithfulness and relevancy\n",
    "\n",
    "### 4.4 Regular Evaluation\n",
    "\n",
    "**Evaluate regularly**:\n",
    "- After model updates\n",
    "- After data updates\n",
    "- After configuration changes\n",
    "- Periodic production evaluation\n",
    "\n",
    "### 4.5 Human Evaluation\n",
    "\n",
    "**Combine automated and human evaluation**:\n",
    "- Automated: Scale, consistency\n",
    "- Human: Quality, nuance\n",
    "- Use both for comprehensive assessment\n",
    "\n",
    "### 4.6 Track Over Time\n",
    "\n",
    "**Monitor metrics over time**:\n",
    "- Track degradation\n",
    "- Identify regressions\n",
    "- Measure improvements\n",
    "\n",
    "## 5. Interpreting Results\n",
    "\n",
    "### 5.1 Low Context Precision\n",
    "\n",
    "**Symptom**: Retrieved chunks are not relevant\n",
    "\n",
    "**Possible Causes**:\n",
    "- Poor embedding model\n",
    "- Wrong chunking strategy\n",
    "- Mismatched query-document embeddings\n",
    "\n",
    "**Solutions**:\n",
    "- Try different embedding model\n",
    "- Adjust chunking\n",
    "- Improve query processing\n",
    "\n",
    "### 5.2 Low Context Recall\n",
    "\n",
    "**Symptom**: Missing relevant chunks\n",
    "\n",
    "**Possible Causes**:\n",
    "- Top-k too small\n",
    "- Poor retrieval\n",
    "- Chunks too granular\n",
    "\n",
    "**Solutions**:\n",
    "- Increase top-k\n",
    "- Improve retrieval\n",
    "- Adjust chunking\n",
    "\n",
    "### 5.3 Low Faithfulness\n",
    "\n",
    "**Symptom**: Answers not grounded in context\n",
    "\n",
    "**Possible Causes**:\n",
    "- Poor prompt engineering\n",
    "- Model ignoring context\n",
    "- Irrelevant context\n",
    "\n",
    "**Solutions**:\n",
    "- Improve prompts\n",
    "- Use better context\n",
    "- Add instructions to use context\n",
    "\n",
    "### 5.4 Low Answer Relevancy\n",
    "\n",
    "**Symptom**: Answers don't address the query\n",
    "\n",
    "**Possible Causes**:\n",
    "- Poor retrieval\n",
    "- Wrong context\n",
    "- Model issues\n",
    "\n",
    "**Solutions**:\n",
    "- Improve retrieval\n",
    "- Better context selection\n",
    "- Model fine-tuning\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Evaluate all components** - chunking, embeddings, retrieval, generation\n",
    "2. **Use multiple metrics** - precision, recall, faithfulness, relevancy\n",
    "3. **MLflow helps** - track experiments, log metrics, compare versions\n",
    "4. **Regular evaluation** - catch issues early, track improvements\n",
    "5. **Interpret results** - understand what metrics mean and how to improve\n",
    "\n",
    "### Continuous Improvement\n",
    "\n",
    "RAG evaluation is an ongoing process:\n",
    "- Start with basic metrics\n",
    "- Refine evaluation over time\n",
    "- Add domain-specific metrics\n",
    "- Monitor in production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "553011da-2aeb-4afb-802c-790380f1c387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Exercise 1**: Create an evaluation dataset for a RAG application\n",
    "2. **Exercise 2**: Calculate context precision and recall for a retrieval system\n",
    "3. **Exercise 3**: Evaluate faithfulness of generated answers\n",
    "4. **Exercise 4**: Use MLflow to track and compare RAG evaluation results\n",
    "5. **Exercise 5**: Interpret evaluation results and suggest improvements\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "07_Evaluating_RAG_Applications",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
