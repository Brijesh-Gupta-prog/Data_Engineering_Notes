{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a857b5ff-02a1-4eca-a666-c3cff3ea6248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Assembling a RAG Application\n",
    "\n",
    "In this lab, we will assemble a Retrieval-augmented Generation (RAG) application using the components we previously created. The primary goal is to create a seamless pipeline where users can ask questions, and our system retrieves relevant documents from a Vector Search index to generate informative responses.\n",
    "\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "In this lab, you will need to complete the following tasks;\n",
    "\n",
    "* **Task 1 :** Setup the Retriever Component\n",
    "\n",
    "* **Task 2 :** Setup the Foundation Model\n",
    "\n",
    "* **Task 3 :** Assemble the Complete RAG Solution\n",
    "\n",
    "* **Task 4 :** Save the Model to Model Registry in Unity Catalog\n",
    "\n",
    "**üìù Your task:** Complete the **`<FILL_IN>`** sections in the code blocks and follow the other steps as instructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af46143e-1154-47a1-b860-cf1268b507c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a270fc9-0f53-417e-b8de-ee25d49ee300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**\n",
    "\n",
    "**üö® Important:** This lab relies on the resources established in the previous one. Please ensure you have completed the prior lab before starting this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ab4bbd-cf3e-4a5d-af0b-42f308d10339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c7fae8-3f1f-475c-b8c8-398975a4ecf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq databricks-sdk databricks-vectorsearch 'mlflow-skinny[databricks]==3.4.0' langchain==0.3.26 databricks-langchain==0.8.0 PyPDF2==3.0.0 flashrank\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06643fb5-bb01-430d-bdc9-af0347f68b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf77c1cf-7321-45bb-ad9e-77f223baf092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f826a0-1684-47a4-9ee8-c02f315ddda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1351b636-ab3b-466e-8760-e147f83be9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Setup the Retriever Component\n",
    "**Steps:**\n",
    "1. Define the embedding model.\n",
    "1. Get the vector search index that was created in the previous lab.\n",
    "1. Generate a **retriever** from the vector store. The retriever should return **three results.**\n",
    "1. Write a test prompt and show the returned search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0bdb858-e9df-4b18-8acd-6f1705550917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Components we created before\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "vs_endpoint_name = vs_endpoint_prefix+str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Assigned Vector Search endpoint name: {vs_endpoint_name}.\")\n",
    "\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.lab_pdf_text_managed_vs_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7f0c16-ea5e-4751-b195-c90538c5a699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.docstore.document import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "\n",
    "def get_retriever(cache_dir=f\"{DA.paths.working_dir}/opt\"):\n",
    "\n",
    "    def retrieve(query, k: int=10):\n",
    "        if isinstance(query, dict):\n",
    "            query = next(iter(query.values()))\n",
    "\n",
    "        ## get the vector search index\n",
    "        vsc = VectorSearchClient(disable_notice=True)\n",
    "        vs_index = <FILL_IN>\n",
    "\n",
    "        # get the query vector\n",
    "        embeddings = <FILL_IN>\n",
    "        query_vector = <FILL_IN>\n",
    "        \n",
    "        ## get similar k documents\n",
    "        return <FILL_IN>\n",
    "\n",
    "\n",
    "    def rerank(query, retrieved, cache_dir, k: int=2):\n",
    "        ## format result to align with reranker lib format \n",
    "        passages = []\n",
    "        for doc in retrieved.get(\"result\", {}).get(\"data_array\", []):\n",
    "            new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "            passages.append(new_doc)       \n",
    "        ## Load the flashrank ranker\n",
    "        ranker = <FILL_IN>\n",
    "\n",
    "        ## rerank the retrieved documents\n",
    "        rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "        results = ranker.<FILL_IN>\n",
    "\n",
    "        ## format the results of rerank to be ready for prompt\n",
    "        return [Document(page_content=r.get(\"text\"), metadata={\"source\": r.get(\"file\")}) for r in results]\n",
    "\n",
    "    ## the retriever is a runnable sequence of retrieving and reranking.\n",
    "    return <FILL_IN>\n",
    "\n",
    "\n",
    "## test your retriever\n",
    "question = <FILL_IN>\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.<FILL_IN>\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de2bee49-4fa9-47a7-8da1-a900001c6133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.docstore.document import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "def get_retriever(cache_dir=f\"{DA.paths.working_dir}/opt\"):\n",
    "\n",
    "    def retrieve(query, k: int=10):\n",
    "        if isinstance(query, dict):\n",
    "            query = next(iter(query.values()))\n",
    "\n",
    "        ## get the vector search index\n",
    "        vsc = VectorSearchClient(disable_notice=True)\n",
    "        vs_index = vsc.get_index(endpoint_name=vs_endpoint_name, index_name=vs_index_fullname)\n",
    "        \n",
    "        # get the query vector\n",
    "        embeddings = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")\n",
    "        query_vector = embeddings.embed_query(query)\n",
    "\n",
    "        ## get similar k documents\n",
    "        return query, vs_index.similarity_search(\n",
    "            query_vector=query_vector,\n",
    "            columns=[\"pdf_name\", \"content\"],\n",
    "            num_results=k)\n",
    "\n",
    "    def rerank(query, retrieved, cache_dir, k: int=2):\n",
    "        ## format result to align with reranker lib format \n",
    "        passages = []\n",
    "        for doc in retrieved.get(\"result\", {}).get(\"data_array\", []):\n",
    "            new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "            passages.append(new_doc)       \n",
    "        ## Load the flashrank ranker\n",
    "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=cache_dir)\n",
    "\n",
    "        ## rerank the retrieved documents\n",
    "        rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "        results = ranker.rerank(rerankrequest)[:k]\n",
    "\n",
    "        ## format the results of rerank to be ready for prompt\n",
    "        return [Document(page_content=r.get(\"text\"), metadata={\"source\": r.get(\"file\")}) for r in results]\n",
    "\n",
    "    ## the retriever is a runnable sequence of retrieving and reranking.\n",
    "    return RunnableLambda(retrieve) | RunnableLambda(lambda x: rerank(x[0], x[1], cache_dir))\n",
    "\n",
    "## test our retriever\n",
    "question = {\"input\": \"How does Generative AI impact humans?\"}\n",
    "vectorstore = get_retriever(cache_dir = f\"{DA.paths.working_dir}/opt\")\n",
    "similar_documents = vectorstore.invoke(question)\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b59694ec-2267-47ff-8d0a-530761e7202d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Setup the Foundation Model\n",
    "**Steps:**\n",
    "1. Define the foundation model for generating responses. Use `llama-3.3` as foundation model. \n",
    "2. Test the foundation model to ensure it provides accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4936aeaf-3c97-4671-95fb-3ba8a382ae6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "## define foundation model for generating responses\n",
    "chat_model = <FILL_IN>\n",
    "\n",
    "## test foundation model\n",
    "print(f\"Test chat model: {<FILL_IN>('What is Generative AI?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55fa3ed6-8a03-4a42-9d50-d766e994d948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## import necessary libraries\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "## define foundation model for generating responses\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 300)\n",
    "\n",
    "## test foundation model\n",
    "print(f\"Test chat model: {chat_model.invoke('What is Generative AI?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2068548-d2c8-4c54-898d-8d8382b12a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 3: Assemble the Complete RAG Solution\n",
    "**Steps:**\n",
    "1. Merge the retriever and foundation model into a single Langchain chain.\n",
    "2. Configure the Langchain chain with proper templates and context for generating responses.\n",
    "3. Test the complete RAG solution with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c30ffe8-ea53-4938-82d9-eecc5ee77241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "## Prompt template\n",
    "TEMPLATE = \"\"\"You are an assistant for GENAI teaching class. You are answering questions related to Generative AI and how it impacts humans life. If the question is not related to one of these topics, kindly decline to answer. \n",
    "Use the following pieces of context to answer the question at the end:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(TEMPLATE)  \n",
    "\n",
    "## Helper functions\n",
    "def format_docs(docs):\n",
    "    ## what the model sees in {context}\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "def unwrap(payload):\n",
    "    ## return both answer and normalized context (dicts) like you wanted\n",
    "    docs = payload[\"docs\"]\n",
    "    return {\n",
    "        \"answer\": payload[\"answer\"],\n",
    "        \"context\": [{\"metadata\": getattr(d, \"metadata\", {}), \"page_content\": getattr(d, \"page_content\", \"\")}\n",
    "                    for d in docs],\n",
    "    }\n",
    "\n",
    "## ---- build the chain ----\n",
    "## Step 1: retrieve docs\n",
    "retrieve = RunnableParallel(input=RunnablePassthrough(), docs=get_retriever())\n",
    "\n",
    "## Step 2: pass formatted context + input to the model\n",
    "rag = <FILL_IN>\n",
    "\n",
    "## Keep docs for postprocessing\n",
    "chain = <FILL_IN>\n",
    "\n",
    "## Test the complete RAG solution with sample query\n",
    "question = {\"input\": \"What are the generative AI's economical implications?\"}\n",
    "response = <FILL_IN>\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4da5f782-6351-4fc5-87a3-168d84b81317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "## Prompt template\n",
    "TEMPLATE = \"\"\"You are an assistant for GENAI teaching class. You are answering questions related to Generative AI and how it impacts humans life. If the question is not related to one of these topics, kindly decline to answer. \n",
    "Use the following pieces of context to answer the question at the end:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(TEMPLATE)      \n",
    "\n",
    "## Helper functions\n",
    "def format_docs(docs):\n",
    "    # what the model sees in {context}\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "def unwrap(payload):\n",
    "    # return both answer and normalized context (dicts) like you wanted\n",
    "    docs = payload[\"docs\"]\n",
    "    return {\n",
    "        \"answer\": payload[\"answer\"],\n",
    "        \"context\": [{\"metadata\": getattr(d, \"metadata\", {}), \"page_content\": getattr(d, \"page_content\", \"\")}\n",
    "                    for d in docs],\n",
    "    }\n",
    "\n",
    "## ---- build the chain ----\n",
    "## Step 1: retrieve docs\n",
    "retrieve = RunnableParallel(input=RunnablePassthrough(), docs=get_retriever())\n",
    "\n",
    "## Step 2: pass formatted context + input to the model\n",
    "rag = retrieve | {\n",
    "    \"input\": itemgetter(\"input\"),\n",
    "    \"context\": RunnableLambda(lambda x: format_docs(x[\"docs\"]))\n",
    "} | prompt | chat_model | StrOutputParser()\n",
    "\n",
    "## Keep docs for postprocessing\n",
    "chain = retrieve | {\n",
    "    \"answer\": ({\"input\": itemgetter(\"input\"), \"context\": RunnableLambda(lambda x: format_docs(x[\"docs\"]))}\n",
    "               | prompt | chat_model | StrOutputParser()),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "} | RunnableLambda(unwrap)\n",
    "\n",
    "## Test the complete RAG solution with sample query\n",
    "question = {\"input\": \"What are the generative AI's economical implications?\"}\n",
    "response = chain.invoke(question)\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80dcf04-e803-487e-89eb-66db8db72189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 4: Save the Model to Model Registry in Unity Catalog\n",
    "**Steps:**\n",
    "1. Register the assembled RAG model in the Model Registry with Unity Catalog.\n",
    "2. Ensure that all necessary dependencies and requirements are included.\n",
    "3. Provide an input example and infer the signature for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86173d68-38a9-44f3-afcd-c0b91293e0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "## set Model Registry URI to Unity Catalog\n",
    "mlflow.<FILL_IN>\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_demo4\"\n",
    "\n",
    "## register the assembled RAG model in Model Registry with Unity Catalog\n",
    "with mlflow.start_run(run_name=\"rag_app_demo4\") as run:\n",
    "    signature = <FILL_IN>\n",
    "    model_info = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87de7d36-3a7e-486f-87c8-0c6bac56842f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## import necessary libraries\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "## set Model Registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_lab_4\"\n",
    "\n",
    "## register the assembled RAG model in Model Registry with Unity Catalog\n",
    "with mlflow.start_run(run_name=\"rag_app_lab_4\") as run:\n",
    "    signature = infer_signature(question, response)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever,\n",
    "        name=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47524b2a-ac4f-4ac3-a064-9a27648b5cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Resources\n",
    "\n",
    "This was the final lab. You can delete all resources created in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88df3837-73e4-4887-9481-2266c6a912c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, you learned how to assemble a Retrieval-augmented Generation (RAG) application using Databricks components. By integrating Vector Search for document retrieval and a foundational model for response generation, you created a powerful tool for answering user queries. This lab provided hands-on experience in building end-to-end AI applications and demonstrated the capabilities of Databricks for natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d994ea-ecec-4136-88b6-e2247aa35cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "4.LAB - Assembling a RAG Application",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
