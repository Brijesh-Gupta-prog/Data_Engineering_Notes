{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0682cd34-2b66-407b-bdd9-2868beb63f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab: Preparing Data for RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b63fe55-48f2-43c5-a280-49532a60cedc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The objective of this lab is to demonstrate the process of ingesting and processing documents for a Retrieval-Augmented Generation (RAG) application. This involves extracting text from PDF documents, computing embeddings using a foundation model, and storing the embeddings in a Delta table.\n",
    "\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "In this lab, you will need to complete the following tasks:\n",
    "\n",
    "* **Task 1 :** Read the PDF files and load them into a DataFrame.\n",
    "\n",
    "* **Task 2 :** Extract the text content from the PDFs and split it into manageable chunks.\n",
    "\n",
    "* **Task 3 :** Compute embeddings for each text chunk using a foundation model endpoint.\n",
    "\n",
    "* **Task 4 :** Create a Delta table to store the computed embeddings.\n",
    "\n",
    "**üìù Your task:** Complete the **`<FILL_IN>`** sections in the code blocks and follow the other steps as instructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e796178-f309-40dd-ad38-8a29bdf9a74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24721db3-5d56-4c60-a92c-073755c5402c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5b90842-6a9b-40b3-993e-3d86f69b36de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9c9192-4ae1-420f-8548-7cda94d96b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qqqq -U mlflow==3.4.0 llama-index==0.14.5 PyPDF2==3.0.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc9bdd2-564c-444f-bf45-e5546ea04b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef08688-2181-4f7e-8eb6-537e61e91800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7331c66d-f61d-4d74-8b80-9dfe6c9d7f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08196178-7aae-41ae-a306-bf87e017632e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Read the PDF files and load them into a DataFrame.\n",
    "\n",
    "To start, you need to load the PDF files into a DataFrame.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use Spark to load the binary PDFs into a DataFrame.\n",
    "\n",
    "2. Ensure that each PDF file is represented as a separate record in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba1eb91-840a-4f2c-a7ee-909c2599eb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## run this cell to import the required libraries\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.utils import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf, explode\n",
    "import os\n",
    "import pandas as pd \n",
    "import io\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6ac674-d04a-4b0b-a70d-47f244ff7cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## use Spark to load the PDF files into a DataFrame\n",
    "## reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "articles_path = f\"{DA.paths.datasets.arxiv}/arxiv-articles/\"\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.lab_pdf_raw_text\"\n",
    "\n",
    "## read pdf files\n",
    "df = <FILL_IN>\n",
    "\n",
    "## save list of the files to table\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "399976f3-db90-4122-8126-2dfced3d4101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## use Spark to load the PDF files into a DataFrame\n",
    "## reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "articles_path = f\"{DA.paths.datasets.arxiv}/arxiv-articles/\"\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.lab_pdf_raw_text\"\n",
    "\n",
    "## read pdf files\n",
    "df = (\n",
    "        spark.read.format(\"binaryfile\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(articles_path)\n",
    "        )\n",
    "\n",
    "## save list of the files to table\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "599d1b96-3d9a-4d6e-9a6d-50eeeb5a33f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Extract the text content from the PDFs and split it into manageable chunks\n",
    "\n",
    "Next, extract the text content from the PDFs and split it into manageable chunks.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define a function to split the text content into chunks.\n",
    "\n",
    "    * Split the text content into manageable chunks.\n",
    "\n",
    "    * Ensure each chunk contains a reasonable amount of text for processing.\n",
    "\n",
    "2. Apply the function to the DataFrame to create a new DataFrame with the text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d53833dd-c6c6-450e-9814-1b5895ca8c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define a function to split the text content into chunks\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # set llama2 as tokenizer\n",
    "    <FILL_IN>\n",
    "    # sentence splitter from llama_index to split on sentences\n",
    "    <FILL_IN>\n",
    "      return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        <FILL_IN>\n",
    "\n",
    "df_chunks = (df\n",
    "                .withColumn(\"content\", explode(read_as_chunk(\"content\")))\n",
    "                .selectExpr('path as pdf_name', 'content')\n",
    "                )\n",
    "display(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66bdf493-46f9-4174-88a9-a4c540293ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## Define a function to split the text content into chunks\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    ## Set llama2 as tokenizer\n",
    "    set_global_tokenizer(\n",
    "      AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    ## Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    def extract_and_split(b):\n",
    "        txt = parse_bytes_pypdf(b)\n",
    "        if txt is None:\n",
    "            return []\n",
    "        nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "        return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)\n",
    "\n",
    "df_chunks = (df\n",
    "              .withColumn(\"content\", explode(read_as_chunk(\"content\")))\n",
    "              .selectExpr(\"path as pdf_name\", \"content\")\n",
    "            )\n",
    "display(df_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9a5963-304a-4302-85e5-b3b966cf8724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Compute embeddings for each text chunk using a foundation model endpoint\n",
    "Now, compute embeddings for each text chunk using a foundation model endpoint.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define a function to compute embeddings for text chunks.\n",
    "    + Use a foundation model endpoint to compute embeddings for each text chunk.\n",
    "    + Ensure that the embeddings are computed efficiently, considering the limitations of the model.  \n",
    "\n",
    "2. Apply the function to the DataFrame containing the text chunks to compute embeddings for each chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ac9afe-b7fd-4a68-ad24-05b4f52fc838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define a function to compute embeddings for text chunks\n",
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    ## define deployment client\n",
    "    <FILL_IN>\n",
    " \n",
    "    def get_embeddings(batch):\n",
    "        ## calculate embeddings using the deployment client's predict function \n",
    "        response = <FILL_IN>\n",
    "        return [e['embedding'] for e in response.data]\n",
    "\n",
    "    ## splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    ## process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        <FILL_IN>\n",
    "\n",
    "    return <FILL_IN>\n",
    "    \n",
    "df_chunk_emd = (df_chunks\n",
    "                .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "                .selectExpr(\"pdf_name\", \"content\", \"embedding\")\n",
    "               )\n",
    "display(df_chunk_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fdc483-1599-4078-a38d-76211fcdb0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## define a function to compute embeddings for text chunks\n",
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    \n",
    "    def get_embeddings(batch):\n",
    "        ## calculate embeddings using the deployment client's predict function \n",
    "        ## NOTE: this will fail if an exception is thrown during embedding creation (add try/except if needed) \n",
    "        response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": batch})\n",
    "        return [e[\"embedding\"] for e in response.data]\n",
    "\n",
    "    ## splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    ## process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "    return pd.Series(all_embeddings)\n",
    "    \n",
    "df_chunk_emd = (df_chunks\n",
    "                .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "                .selectExpr(\"pdf_name\", \"content\", \"embedding\")\n",
    "                )\n",
    "display(df_chunk_emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc893241-7827-4b49-bd41-5cde9f2d680a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Create a Delta table to store the computed embeddings\n",
    "\n",
    "Finally, create a Delta table to store the computed embeddings.\n",
    "\n",
    "Steps:\n",
    "\n",
    "  1. Define the schema for the Delta table.\n",
    "\n",
    "  2. Save the DataFrame containing the computed embeddings as a Delta table.\n",
    "\n",
    "\n",
    "**Note:** Ensure that the Delta table is properly structured to facilitate efficient querying and retrieval of the embeddings.\n",
    "\n",
    "**üìå Instructions:** \n",
    "\n",
    "- Please execute the following SQL code block to create the Delta table. This table will store the computed embeddings along with other relevant information. \n",
    "\n",
    "**Important:** Storing the computed embeddings in a structured format like a Delta table ensures efficient querying and retrieval of the embeddings when needed for various downstream tasks such as retrieval-augmented generation. Additionally, setting the `delta.enableChangeDataFeed` property to true enables Change Data Feed (CDC), which is required for VectorSearch to efficiently process changes in the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ec86ab5-8b2c-4e28-b06d-642d692c1e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS lab_pdf_text_embeddings (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  pdf_name STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    "  ---- NOTE: the table has to be CDC because VectorSearch is using DLT that is requiring CDC state\n",
    "  ) TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ffd5a9-4224-44b7-a399-f2ec4df9feca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define the schema for the Delta table\n",
    "embedding_table_name = f\"{DA.catalog_name}.{DA.schema_name}.lab_pdf_text_embeddings\"\n",
    "## save the DataFrame as a Delta table\n",
    "df_chunk_emd.<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb48e03-571a-48f4-8332-197163240e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "## define the schema for the Delta table\n",
    "embedding_table_name = f\"{DA.catalog_name}.{DA.schema_name}.lab_pdf_text_embeddings\"\n",
    "## save the DataFrame as a Delta table\n",
    "df_chunk_emd.write.mode(\"append\").saveAsTable(embedding_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8fdfa5e-bdd2-40db-8243-521783701a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "**üö® Warning:** Please refrain from deleting tables created in this lab, as they are required for upcoming labs. To clean up the classroom assets, execute the classroom clean-up script provided in the final lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2f9143-82cd-4151-af1d-889dc719227b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, you learned how to prepare data for Retrieval-Augmented Generation (RAG) applications. By extracting text from PDF documents, computing embeddings, and storing them in a Delta table, you can enhance the capabilities of language models to generate more accurate and relevant responses."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.LAB - Preparing Data for RAG",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
