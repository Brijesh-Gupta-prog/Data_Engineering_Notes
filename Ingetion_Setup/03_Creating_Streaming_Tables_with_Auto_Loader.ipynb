{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9957de93-51a8-4a33-8a0d-015a8a14587c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating Streaming Tables with SQL using Auto Loader\n",
    "\n",
    "In this demonstration we will create a streaming table to incrementally ingest files from a volume using Auto Loader with SQL. \n",
    "\n",
    "When you create a streaming table using the CREATE OR REFRESH STREAMING TABLE statement, the initial data refresh and population begin immediately. These operations do not consume DBSQL warehouse compute. Instead, streaming tables rely on serverless DLT for both creation and refresh. A dedicated serverless DLT pipeline is automatically created and managed by the system for each streaming table.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "- Create streaming tables in Databricks SQL for incremental data ingestion.\n",
    "- Refresh streaming tables using the REFRESH statement.\n",
    "- Understand how streaming tables automatically track and ingest only new files.\n",
    "\n",
    "### RECOMMENDATION\n",
    "\n",
    "The CREATE STREAMING TABLE SQL command is the recommended alternative to the legacy COPY INTO SQL command for incremental ingestion from cloud object storage. Databricks recommends using streaming tables to ingest data using Databricks SQL. \n",
    "\n",
    "A streaming table is a table registered to Unity Catalog with extra support for streaming or incremental data processing. A DLT pipeline is automatically created for each streaming table. You can use streaming tables for incremental data loading from Kafka and cloud object storage.\n",
    "\n",
    "---\n",
    "\n",
    "**Environment Setup:**\n",
    "- **Catalog:** `lakeflow_demo`\n",
    "- **Schema:** `lakeflow_schema`\n",
    "- **Volumes:** \n",
    "  - `csv_files_autoloader_source` (source volume for streaming ingestion)\n",
    "  - `autoloader_staging_files` (staging volume with additional files for incremental ingestion demo)\n",
    "\n",
    "**Note:** Make sure to run the `00_Setup_Environment.ipynb` notebook first to create the catalog, schema, volumes, and sample data files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b83eb9f-fd76-486e-a1a8-e5c289639474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ecc89b-1f38-4cea-8877-ffb3366bf262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Set default catalog and schema\n",
    "USE CATALOG lakeflow_demo;\n",
    "USE SCHEMA lakeflow_schema;\n",
    "\n",
    "-- View current catalog and schema\n",
    "SELECT \n",
    "  current_catalog(), \n",
    "  current_schema();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dce9755-98c9-4a73-ae1f-d9855a04b214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. SQL Warehouse Requirement\n",
    "\n",
    "**REQUIRED - SELECT YOUR SERVERLESS SQL WAREHOUSE**\n",
    "\n",
    "**NOTE: Creating streaming tables with Databricks SQL requires a SQL warehouse.**\n",
    "\n",
    "Before executing cells in this notebook, please select a **SQL WAREHOUSE** in the lab. Follow these steps:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down to select compute (it might say **Connect**).\n",
    "2. Select **More**.\n",
    "3. Then select the **SQL Warehouse** button.\n",
    "4. Select or create a SQL warehouse.\n",
    "5. Then, at the bottom of the pop-up, select **Start and attach**.\n",
    "\n",
    "**Important:** Without a SQL warehouse, the streaming table creation commands will fail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f925a712-8224-4802-a4f1-90cbefbc85a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Create Streaming Tables for Incremental Processing\n",
    "\n",
    "### C1. Explore the Data Source\n",
    "\n",
    "1. Explore the volume `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source` and confirm it contains CSV file(s).\n",
    "\n",
    "   Use the `LIST` statement to view the files in this volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e59b3138-a721-432e-bbdc-44de0e4912ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47dcaa98-140b-4b59-ab7b-8ec011f8be6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the query below to view the data in the CSV file(s) in your cloud storage location. Notice that it was returned in tabular format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d249166c-9392-4953-b6cb-a6010b844f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "  format => 'CSV',\n",
    "  sep => '|',\n",
    "  header => true\n",
    ")\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b0e3f52-14bd-47db-8fbb-1b6f27d6f657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Create a STREAMING TABLE using Databricks SQL\n",
    "\n",
    "3. Your goal is to create an incremental pipeline that only ingests new files (instead of using traditional batch ingestion). You can achieve this by using [streaming tables in Databricks SQL](https://docs.databricks.com/aws/en/dlt/dbsql/streaming) (Auto Loader).\n",
    "\n",
    "   - The SQL code below creates a streaming table that will incrementally ingest only new data.\n",
    "   \n",
    "   - A pipeline is automatically created for each streaming table. You can use streaming tables for incremental data loading from Kafka and cloud object storage.\n",
    "\n",
    "   **NOTE:** Incremental batch ingestion automatically detects new records in the data source and ignores records that have already been ingested. This reduces the amount of data processed, making ingestion jobs faster and more efficient in their use of compute resources.\n",
    "\n",
    "   **REQUIRED: This process will take about a minute to run and set up the incremental ingestion pipeline.**\n",
    "\n",
    "   **IMPORTANT:** If you encounter schema errors, make sure to drop any existing table first, or use the explicit schema option shown in the alternative approach below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea1fb8f-469e-44ed-8f2a-e23bf660af8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop existing table if it exists (to avoid schema conflicts)\n",
    "DROP TABLE IF EXISTS sql_csv_autoloader;\n",
    "\n",
    "-- Create streaming table with automatic schema inference\n",
    "CREATE STREAMING TABLE sql_csv_autoloader\n",
    "AS\n",
    "SELECT *\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "  format => 'CSV',\n",
    "  sep => '|',\n",
    "  header => true\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a78d32-13bd-4911-8933-4a32d81851e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Alternative: If you encounter schema errors, you can explicitly define the schema:**\n",
    "\n",
    "```sql\n",
    "-- Drop existing table if it exists\n",
    "DROP TABLE IF EXISTS sql_csv_autoloader;\n",
    "\n",
    "-- Create streaming table with explicit schema\n",
    "CREATE STREAMING TABLE sql_csv_autoloader\n",
    "AS\n",
    "SELECT *\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "  format => 'CSV',\n",
    "  sep => '|',\n",
    "  header => true,\n",
    "  schema => 'order_id STRING, product STRING, quantity INT, price DOUBLE, sale_date TIMESTAMP'\n",
    ");\n",
    "```\n",
    "\n",
    "**Note:** The explicit schema approach is useful when:\n",
    "- Schema inference fails\n",
    "- You want to ensure consistent column types\n",
    "- You're working with files that might have inconsistent schemas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c390a70-310d-4e57-9827-20d655ed970c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. View and Inspect the Streaming Table\n",
    "\n",
    "4. Run the cell below to view the streaming table. Confirm that the results contain the expected number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e7b654-08ee-4468-a749-ca977dd000e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2318e31-8fac-4466-8a86-53a0e14bfe88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Describe the STREAMING TABLE and view the results. Notice the following:\n",
    "\n",
    "- Under **Detailed Table Information**, notice the following rows:\n",
    "  - **View Text**: The query that created the table.\n",
    "  - **Type**: Specifies that it is a STREAMING TABLE.\n",
    "  - **Provider**: Indicates that it is a Delta table.\n",
    "\n",
    "- Under **Refresh Information**, you can see specific refresh details including Last Refreshed, Last Refresh Type, Latest Refresh Status, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c28f825-99dc-4192-88ef-40a7a9407e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "669c97f6-a241-4339-b2a4-58bc7873b056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. The `DESCRIBE HISTORY` statement displays a detailed list of all changes, versions, and metadata associated with a Delta streaming table, including information on updates, deletions, and schema changes.\n",
    "\n",
    "   Run the cell below and view the results. Notice the following:\n",
    "\n",
    "   - In the **operation** column, you can see that a streaming table performs operations: **CREATE TABLE**, **DLT SETUP** and **STREAMING UPDATE**.\n",
    "   \n",
    "   - Scroll to the right and find the **operationMetrics** column to see the number of rows processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8ccef5-1c60-49d6-bbc7-0eb5303c7347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21467576-8375-4d59-9e51-28f6ad8f0656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Demonstrating Incremental Ingestion\n",
    "\n",
    "### D1. Add New Files for Incremental Ingestion\n",
    "\n",
    "7. To demonstrate incremental ingestion, manually add another file to your cloud storage location: `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source`.\n",
    "\n",
    "   **Option 1 - Using Python:**\n",
    "   - Copy a file from the staging volume to the source volume\n",
    "\n",
    "   **Option 2 - Using UI:**\n",
    "   - Click the catalog icon on the left\n",
    "   - Expand the **lakeflow_demo** catalog\n",
    "   - Expand your **lakeflow_schema** schema\n",
    "   - Expand **Volumes**\n",
    "   - Open the **autoloader_staging_files** volume\n",
    "   - Copy a file from there to the **csv_files_autoloader_source** volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394627e9-a8bd-40fe-af89-36917ae506cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Option 1: Copy a clean CSV file from staging to source volume using Python\n",
    "# CRITICAL: Only copy CSV files, NOT Spark metadata files (_SUCCESS, _committed_*, etc.)\n",
    "\n",
    "def copy_clean_csv_files(copy_from, copy_to, n=1):\n",
    "    \"\"\"\n",
    "    Copy only CSV files from staging to source, excluding Spark metadata.\n",
    "    This ensures Autoloader sees clean file arrivals.\n",
    "    \"\"\"\n",
    "    all_files = dbutils.fs.ls(copy_from)\n",
    "    \n",
    "    # Filter to ONLY CSV files (exclude Spark metadata and directories)\n",
    "    csv_files = [f for f in all_files if f.name.endswith('.csv') and not f.isDir()]\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        print(f\"⚠ WARNING: No CSV files found in {copy_from}\")\n",
    "        return\n",
    "    \n",
    "    # Take first n CSV files\n",
    "    files_to_copy = csv_files[:n]\n",
    "    \n",
    "    print(f\"Copying {len(files_to_copy)} clean CSV file(s) from staging to source...\")\n",
    "    for f in files_to_copy:\n",
    "        # Use timestamp to make filename unique for Autoloader\n",
    "        import time\n",
    "        timestamp = int(time.time())\n",
    "        dest_name = f\"sales_incremental_{timestamp}_{f.name}\"\n",
    "        dest_path = f\"{copy_to}/{dest_name}\"\n",
    "        dbutils.fs.cp(f.path, dest_path)\n",
    "        print(f\"  ✓ Copied: {f.name} → {dest_name} ({f.size:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\n✓ Successfully copied {len(files_to_copy)} file(s) for incremental ingestion\")\n",
    "    print(\"  You can now refresh the streaming table to see incremental ingestion.\")\n",
    "\n",
    "# Copy one additional CSV file for incremental ingestion demo\n",
    "copy_clean_csv_files(\n",
    "    copy_from=\"/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files\",\n",
    "    copy_to=\"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source\",\n",
    "    n=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eacfbd6a-9310-4e25-8a43-6fc32d09f6ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Refresh the Streaming Table\n",
    "\n",
    "8. Next, manually refresh the STREAMING TABLE using `REFRESH STREAMING TABLE table-name`. \n",
    "\n",
    "   - [Refresh a streaming table](https://docs.databricks.com/aws/en/dlt/dbsql/streaming#refresh-a-streaming-table) documentation\n",
    "\n",
    "   **NOTE:** You can also rerun the CREATE STREAMING TABLE cell to incrementally ingest only new files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd3bada-9379-4b2f-a245-42776046caf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "REFRESH STREAMING TABLE lakeflow_demo.lakeflow_schema.sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcca268b-c47f-498b-ab09-f80239926126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Verify Incremental Ingestion\n",
    "\n",
    "9. Run the cell below to view the data in the **sql_csv_autoloader** table. Notice that the table now contains additional rows from the newly added file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcee6a6-3f0c-4845-adca-472ad952da8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af37904d-12db-4644-8581-cb930e7a5a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Describe the history of the **sql_csv_autoloader** table. Observe the following:\n",
    "\n",
    "  - Additional versions of the streaming table include **STREAMING UPDATE** operations.\n",
    "\n",
    "  - Expand the **operationMetrics** column and note the number of rows that were incrementally ingested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707114d3-c403-4f49-ac38-e3ae16a9a3a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68cc5633-2dd6-444b-9d03-a6badaa7e30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Key Concepts and Benefits\n",
    "\n",
    "### Key Benefits of Streaming Tables\n",
    "\n",
    "1. **Automatic Incremental Processing**: Streaming tables automatically track which files have been processed and only ingest new files on each refresh.\n",
    "\n",
    "2. **Serverless DLT Pipeline**: Each streaming table has its own automatically managed DLT (Delta Live Tables) pipeline that runs serverlessly.\n",
    "\n",
    "3. **Idempotent Operations**: Running the same refresh multiple times won't duplicate data - already processed files are skipped.\n",
    "\n",
    "4. **No Compute Consumption**: The initial data refresh and population do not consume DBSQL warehouse compute.\n",
    "\n",
    "5. **Unified Catalog Integration**: Streaming tables are registered in Unity Catalog, making them accessible like any other table.\n",
    "\n",
    "### When to Use Streaming Tables\n",
    "\n",
    "- **Incremental Batch Ingestion**: When you need to periodically ingest new files from cloud storage\n",
    "- **Real-time Data Pipelines**: When combined with streaming sources like Kafka\n",
    "- **Automated Data Refresh**: When you want automatic tracking of processed files\n",
    "- **Production Workloads**: When you need reliable, managed data ingestion pipelines\n",
    "\n",
    "### Comparison: Streaming Tables vs COPY INTO\n",
    "\n",
    "| Feature | Streaming Tables | COPY INTO |\n",
    "|---------|------------------|-----------|\n",
    "| Incremental Processing | Automatic | Manual tracking |\n",
    "| DLT Pipeline | Automatic | Not required |\n",
    "| Serverless Execution | Yes | No (uses compute) |\n",
    "| Recommended for New Projects | Yes | Legacy approach |\n",
    "| Complexity | Lower | Higher |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e863c692-080c-4d78-9f5d-9b7fa84bd332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Additional Resources\n",
    "\n",
    "- [Streaming Tables Documentation](https://docs.databricks.com/aws/en/dlt/dbsql/streaming)\n",
    "- [CREATE STREAMING TABLE Syntax](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table)\n",
    "- [Using Streaming Tables in Databricks SQL](https://docs.databricks.com/aws/en/dlt/dbsql/streaming)\n",
    "- [REFRESH (MATERIALIZED VIEW or STREAMING TABLE)](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-refresh-full)\n",
    "- [Auto Loader Documentation](https://docs.databricks.com/ingestion/auto-loader/index.html)\n",
    "- [Delta Live Tables (DLT) Overview](https://docs.databricks.com/dlt/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8942730893017918,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Creating_Streaming_Tables_with_Auto_Loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
