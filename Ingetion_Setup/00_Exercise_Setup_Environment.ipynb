{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf08f9f-7283-4d9c-a80f-ccd8a0bfff3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise Setup Environment\n",
    "\n",
    "This notebook sets up the environment for the Data Ingestion with Lakeflow exercises.\n",
    "\n",
    "It will create:\n",
    "- Catalog: `lakeflow_exercise`\n",
    "- Schema: `exercise_schema`\n",
    "- Volume: `exercise_raw` (within exercise_schema)\n",
    "- Sample data files in the exercise_raw volume for practice exercises\n",
    "\n",
    "**Note:** This setup creates different datasets than the main notebooks to provide fresh practice scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55cc9176-c494-4e29-b6df-8ede0be9ce3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create Catalog and Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15dc098-c1ae-4adf-a5b9-9785e14f96a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create catalog\n",
    "CREATE CATALOG IF NOT EXISTS lakeflow_exercise;\n",
    "\n",
    "-- Create schema within the catalog\n",
    "CREATE SCHEMA IF NOT EXISTS lakeflow_exercise.exercise_schema;\n",
    "\n",
    "-- Set default catalog and schema\n",
    "USE CATALOG lakeflow_exercise;\n",
    "USE SCHEMA exercise_schema;\n",
    "\n",
    "-- Verify current catalog and schema\n",
    "SELECT current_catalog(), current_schema();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a0ea0e4-ac8e-46be-a60a-fe8f58caf3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced7aa31-677a-466e-9fa1-37fedda009fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create volume for raw data files\n",
    "CREATE VOLUME IF NOT EXISTS lakeflow_exercise.exercise_schema.exercise_raw;\n",
    "\n",
    "-- Verify volume creation\n",
    "DESCRIBE VOLUME lakeflow_exercise.exercise_schema.exercise_raw;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec73ade9-73b9-4638-ae6b-0b9566055c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Create Sample Data Files\n",
    "\n",
    "We'll create sample data files in different formats (Parquet, CSV, JSON) for the exercise.\n",
    "\n",
    "### 3.1: Create Sample Parquet Files (Customer Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd459c2-46bf-437e-8bee-a0468954ed72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "# Generate sample customer data\n",
    "n_records = 15000\n",
    "customer_ids = [f\"CUST{str(i).zfill(8)}\" for i in range(1, n_records + 1)]\n",
    "customer_names = [f\"Customer_{i}\" for i in range(1, n_records + 1)]\n",
    "registration_timestamps = []\n",
    "\n",
    "# Generate Unix timestamps (in microseconds)\n",
    "base_timestamp = int(datetime(2019, 1, 1).timestamp() * 1_000_000)\n",
    "for _ in range(n_records):\n",
    "    registration_timestamps.append(base_timestamp + random.randint(0, 730*24*60*60*1_000_000))\n",
    "\n",
    "# Create DataFrame\n",
    "customers_df = pd.DataFrame({\n",
    "    'customer_id': customer_ids,\n",
    "    'customer_name': customer_names,\n",
    "    'registration_timestamp': registration_timestamps,\n",
    "    'country': [random.choice(['USA', 'UK', 'Canada', 'Australia', 'Germany']) for _ in range(n_records)]\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(customers_df)\n",
    "\n",
    "# Write to Parquet files in the raw volume\n",
    "output_path = \"/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/\"\n",
    "spark_df.coalesce(6).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Created {n_records} customer records in Parquet format at: {output_path}\")\n",
    "print(f\"Files created:\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file in files:\n",
    "    if file.name.startswith('part-'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67de4159-b9f8-4aee-be81-e300b50365ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2: Create Sample CSV Files (Transactions Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765dada8-4af9-4548-b924-d29e634ceca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "# Generate sample transaction data\n",
    "n_records = 8000\n",
    "transaction_ids = [f\"TXN{str(i).zfill(7)}\" for i in range(1, n_records + 1)]\n",
    "categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Home\"]\n",
    "amounts = np.random.uniform(5.0, 2000.0, n_records).round(2)\n",
    "\n",
    "transactions_df = pd.DataFrame({\n",
    "    'transaction_id': transaction_ids,\n",
    "    'category': [random.choice(categories) for _ in range(n_records)],\n",
    "    'amount': amounts,\n",
    "    'transaction_date': [datetime.now() - timedelta(days=random.randint(0, 180)) for _ in range(n_records)]\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(transactions_df)\n",
    "\n",
    "# Write to CSV files in the raw volume (pipe-delimited)\n",
    "output_path = \"/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/transactions-csv/\"\n",
    "spark_df.coalesce(4).write.mode(\"overwrite\").option(\"sep\", \"|\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "print(f\"Created {n_records} transaction records in CSV format at: {output_path}\")\n",
    "print(f\"Files created:\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file in files:\n",
    "    if file.name.endswith('.csv'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95768dbc-9a8e-4f14-83f0-c8793f72cef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3: Create Sample CSV File with Malformed Data (for Rescued Data Exercise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166cb323-74a9-4f46-aaf2-94c6008431ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create sample inventory data with malformed rows\n",
    "inventory_data = [\n",
    "    {\"product_id\": \"P001\", \"product_name\": \"Laptop\", \"stock_quantity\": 50},\n",
    "    {\"product_id\": \"P002\", \"product_name\": \"Mouse\", \"stock_quantity\": 200},\n",
    "    {\"product_id\": \"P003\", \"product_name\": \"Keyboard\", \"stock_quantity\": \"150 units\"},  # Malformed: quantity with text\n",
    "    {\"product_id\": \"P004\", \"product_name\": \"Monitor\", \"stock_quantity\": 75},\n",
    "    {\"product_id\": \"P005\", \"product_name\": \"Headphones\", \"stock_quantity\": \"N/A\"},  # Malformed: text instead of number\n",
    "    {\"product_id\": \"P006\", \"product_name\": \"Webcam\", \"stock_quantity\": 30}\n",
    "]\n",
    "\n",
    "inventory_df = pd.DataFrame(inventory_data)\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/inventory-csv\"\n",
    "dbutils.fs.mkdirs(output_dir)\n",
    "\n",
    "# Write to CSV file (comma-delimited)\n",
    "output_path = f\"{output_dir}/exercise_malformed_data.csv\"\n",
    "inventory_df.to_csv(output_path, index=False, sep=\",\")\n",
    "\n",
    "print(f\"Created malformed CSV file at: {output_path}\")\n",
    "print(f\"File contains {len(inventory_df)} rows, including malformed rows with stock_quantity containing text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ff40a9-f3d2-40f2-b440-ab2c20003488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4: Create Sample JSON Files (Web Events Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5700639d-c8fc-485b-ab96-5a7362871351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import base64\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed\n",
    "random.seed(123)\n",
    "\n",
    "# Generate sample web event data\n",
    "n_records = 3000\n",
    "browsers = [\"Chrome\", \"Firefox\", \"Safari\", \"Edge\", \"Opera\"]\n",
    "pages = [\"home\", \"products\", \"cart\", \"checkout\", \"about\"]\n",
    "actions = [\"view\", \"click\", \"add_to_cart\", \"purchase\", \"search\"]\n",
    "cities = [\"San Francisco\", \"New York\", \"London\", \"Toronto\", \"Sydney\"]\n",
    "countries = [\"US\", \"GB\", \"CA\", \"AU\", \"DE\"]\n",
    "\n",
    "events = []\n",
    "for i in range(n_records):\n",
    "    customer_id = f\"CUST{str(i+1).zfill(8)}\"\n",
    "    \n",
    "    # Create event value (JSON string)\n",
    "    event_value = {\n",
    "        \"browser\": random.choice(browsers),\n",
    "        \"page\": random.choice(pages),\n",
    "        \"action\": random.choice(actions),\n",
    "        \"event_timestamp\": int(datetime.now().timestamp() * 1000) + i,\n",
    "        \"location\": {\n",
    "            \"city\": random.choice(cities),\n",
    "            \"country\": random.choice(countries)\n",
    "        },\n",
    "        \"session_id\": f\"SESS{str(i+1).zfill(8)}\",\n",
    "        \"customer_id\": customer_id\n",
    "    }\n",
    "    \n",
    "    # Encode key and value in base64\n",
    "    key_encoded = base64.b64encode(customer_id.encode()).decode()\n",
    "    value_encoded = base64.b64encode(json.dumps(event_value).encode()).decode()\n",
    "    \n",
    "    # Create web event record\n",
    "    web_event = {\n",
    "        \"key\": key_encoded,\n",
    "        \"offset\": 500000 + i,\n",
    "        \"partition\": i % 4,\n",
    "        \"timestamp\": int(datetime.now().timestamp() * 1000) + i,\n",
    "        \"topic\": \"web_events\",\n",
    "        \"value\": value_encoded\n",
    "    }\n",
    "    \n",
    "    events.append(web_event)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(events)\n",
    "\n",
    "# Write to JSON files in the raw volume\n",
    "output_path = \"/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/web-events-json/\"\n",
    "spark_df.coalesce(6).write.mode(\"overwrite\").json(output_path)\n",
    "\n",
    "print(f\"Created {n_records} web event records in JSON format at: {output_path}\")\n",
    "print(f\"Files created:\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file in files:\n",
    "    if file.name.startswith('part-'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9d094b-a133-4b06-a1a3-e0189bbf5f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Verify Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9745355d-8c33-4aff-bf95-b9ffc41ff6c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify catalog and schema\n",
    "SHOW SCHEMAS IN lakeflow_exercise;\n",
    "\n",
    "-- Verify volume\n",
    "DESCRIBE VOLUME lakeflow_exercise.exercise_schema.exercise_raw;\n",
    "\n",
    "-- List files in raw volume\n",
    "LIST '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0edd095-fd4d-4ae3-8ea8-48ef31761320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Complete!\n",
    "\n",
    "Your environment is now ready for the Data Ingestion exercises.\n",
    "\n",
    "**Catalog:** `lakeflow_exercise`  \n",
    "**Schema:** `exercise_schema`  \n",
    "**Volume:** `exercise_raw` (located at `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/`)\n",
    "\n",
    "**Sample data files created:**\n",
    "- Parquet files: `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/`\n",
    "- CSV files: `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/transactions-csv/`\n",
    "- JSON files: `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/web-events-json/`\n",
    "- Malformed CSV: `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/inventory-csv/exercise_malformed_data.csv`\n",
    "\n",
    "You can now proceed with the exercise notebook!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6201146797106217,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Exercise_Setup_Environment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
