{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67f9a3b-7bc5-4269-850e-a3aeb84915ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion with Lakeflow - Exercise\n",
    "\n",
    "This exercise notebook tests your understanding of data ingestion techniques covered in the practice notebooks.\n",
    "\n",
    "**Instructions:**\n",
    "1. Make sure you have run the `00_Exercise_Setup_Environment.ipynb` notebook first\n",
    "2. Complete each exercise below\n",
    "3. You can use either SQL or Python/PySpark (or both) to solve the exercises\n",
    "4. All tables should be created in the `lakeflow_exercise.exercise_schema` catalog and schema\n",
    "\n",
    "---\n",
    "\n",
    "**Environment Setup:**\n",
    "- **Catalog:** `lakeflow_exercise`\n",
    "- **Schema:** `exercise_schema`\n",
    "- **Volume:** `exercise_raw` (located at `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/`)\n",
    "\n",
    "**Note:** Make sure to run the `00_Exercise_Setup_Environment.ipynb` notebook first to create the catalog, schema, volume, and sample data files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad91f2bf-039d-496c-a01e-8405556bca94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482f75db-0de5-434c-9d90-6ab25f59499a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Set default catalog and schema\n",
    "USE CATALOG lakeflow_exercise;\n",
    "USE SCHEMA exercise_schema;\n",
    "\n",
    "-- View current catalog and schema\n",
    "SELECT \n",
    "  current_catalog(), \n",
    "  current_schema();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b010b92-7fe0-44b3-8d90-9b4ce153d32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 1: Parquet File Ingestion with CTAS\n",
    "\n",
    "**Objective:** Ingest customer data from Parquet files into a Delta table.\n",
    "\n",
    "**Tasks:**\n",
    "1. Explore the Parquet files located at `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/`\n",
    "2. Use `read_files()` with a CTAS statement to create a table named `customers_bronze_ctas`\n",
    "3. Verify the table was created successfully and contains the expected data\n",
    "\n",
    "**Hints:**\n",
    "- Use `LIST` to explore the files\n",
    "- Use `read_files()` with `format => 'parquet'`\n",
    "- Use `CREATE TABLE ... AS SELECT` syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1adac087-77c0-497f-9aab-74b98e574545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4835cce2-c419-4cfc-92e0-32e972d66521",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List files in the volume"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1f7f74-0f5e-4998-8e3d-22feb304b3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE customers_bronze_ctas\n",
    "USING DELTA\n",
    "AS SELECT * FROM read_files('/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/', format => 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f0d82d-0f07-456a-ba32-981ce3c936d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify table: sample data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM customers_bronze_ctas LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed99309-01e0-4820-80a9-c0d0ace7d803",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify table: describe"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE customers_bronze_ctas;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff604b9-7b1e-4d3e-8ef0-2d8709bfb981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 2: Incremental Ingestion with COPY INTO\n",
    "\n",
    "**Objective:** Use `COPY INTO` to incrementally load data into an existing Delta table.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create an empty table named `customers_bronze_copy` with only the `customer_id` and `customer_name` columns\n",
    "2. Use `COPY INTO` to load data from the Parquet files\n",
    "3. Handle the schema mismatch error by using `COPY_OPTIONS` with `mergeSchema = 'true'`\n",
    "4. Verify that the data was loaded successfully\n",
    "\n",
    "**Hints:**\n",
    "- The Parquet files contain more columns than initially defined in the table\n",
    "- Use `COPY_OPTIONS ('mergeSchema' = 'true')` to handle schema evolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f2c960-1011-4eb0-8fc6-d80c4efb6d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4059c30-182f-4b10-be3b-497e3b294f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Step 1: Create empty table with partial schema\n",
    "CREATE TABLE customers_bronze_copy (\n",
    "  customer_id STRING,\n",
    "  customer_name STRING\n",
    ");\n",
    "\n",
    "-- Step 2: Use COPY INTO with mergeSchema option\n",
    "\n",
    "COPY INTO customers_bronze_copy\n",
    "  FROM '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "-- Step 3: Verify the data\n",
    "\n",
    "SELECT * FROM customers_bronze_copy LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "686bf805-fa2b-490c-9356-ca609c8d1949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 3: Adding Metadata Columns During Ingestion\n",
    "\n",
    "**Objective:** Create a bronze table with metadata columns from customer Parquet files.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a table named `customers_bronze_metadata` that includes:\n",
    "   - All original columns from the Parquet files\n",
    "   - A `registration_date` column (convert `registration_timestamp` from Unix microseconds to DATE)\n",
    "   - A `file_modification_time` column (from `_metadata`)\n",
    "   - A `source_file` column (from `_metadata`)\n",
    "   - An `ingestion_time` column (current timestamp)\n",
    "\n",
    "**Hints:**\n",
    "- Use `from_unixtime()` to convert Unix timestamp (divide by 1,000,000 to convert microseconds to seconds)\n",
    "- Use `_metadata.file_modification_time` and `_metadata.file_name`\n",
    "- Use `current_timestamp()` for ingestion time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aa0fcbe-0afb-4c61-a476-e08588e8fd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0db539-799f-46f7-8786-827b8869a6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Create table with metadata columns using CTAS and read_files()\n",
    "CREATE TABLE customers_bronze_metadata\n",
    "USING DELTA\n",
    "AS SELECT *,\n",
    "    DATE(from_unixtime(registration_timestamp/1000000)) AS registration_date,\n",
    "    _metadata.file_modification_time AS file_modification_time,\n",
    "    _metadata.file_name   AS source_file,\n",
    "    current_timestamp()   AS ingestion_time\n",
    "\n",
    "FROM read_files('/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/', format => 'parquet')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf7a55c-b904-4758-a134-9a9a143fcce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 4: CSV Ingestion with Rescued Data Column\n",
    "\n",
    "**Objective:** Ingest a CSV file with malformed data and handle rescued data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Inspect the malformed CSV file at `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/inventory-csv/exercise_malformed_data.csv`\n",
    "2. Create a table named `inventory_bronze` that:\n",
    "   - Uses the schema: `product_id STRING, product_name STRING, stock_quantity INT`\n",
    "   - Includes the `_rescued_data` column to capture malformed rows\n",
    "   - Includes metadata columns: `file_modification_time`, `source_file`, and `ingestion_time`\n",
    "3. Query the table to identify which rows have rescued data\n",
    "\n",
    "**Hints:**\n",
    "- The CSV file is comma-delimited\n",
    "- Use `read_files()` with `rescuedDataColumn => '_rescued_data'`\n",
    "- Some `stock_quantity` values contain text instead of numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57c3f47-99d6-45f8-a184-96fa18587c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64be04a-a482-49d0-8583-74ed3fa77902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/inventory-csv/exercise_malformed_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ddf379-faae-4a5f-8eb1-166df4f0484f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Step 1: Inspect the CSV file (optional)\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/inventory-csv/exercise_malformed_data.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ");\n",
    "\n",
    "-- Step 2: Create table with rescued_data column\n",
    "CREATE TABLE inventory_bronze\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  product_id,\n",
    "  product_name,\n",
    "  stock_quantity,\n",
    "  _rescued_data,\n",
    "  _metadata.file_modification_time AS file_modification_time,\n",
    "  _metadata.file_path AS source_file,\n",
    "  current_timestamp() AS ingestion_time\n",
    "FROM read_files(\n",
    "  '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/inventory-csv/exercise_malformed_data.csv',\n",
    "  format => 'csv',\n",
    "  header => true,\n",
    "  schema => 'product_id STRING, product_name STRING, stock_quantity INT',\n",
    "  rescuedDataColumn => '_rescued_data'\n",
    ");\n",
    "\n",
    "-- Step 3: Query to find rows with rescued data\n",
    "SELECT *\n",
    "FROM inventory_bronze\n",
    "WHERE _rescued_data IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b83620-714b-4531-83bc-356ba1fb9e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 5: JSON File Ingestion and Decoding\n",
    "\n",
    "**Objective:** Ingest JSON files containing base64-encoded web event data and decode the fields.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a table named `web_events_bronze_raw` from the JSON files at `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/web-events-json/`\n",
    "2. Create a second table named `web_events_bronze_decoded` that:\n",
    "   - Decodes the base64-encoded `key` column to STRING (name it `decoded_key`)\n",
    "   - Decodes the base64-encoded `value` column to STRING (name it `decoded_value`)\n",
    "   - Includes all other columns: `offset`, `partition`, `timestamp`, `topic`\n",
    "3. Verify the decoded data is readable\n",
    "\n",
    "**Hints:**\n",
    "- Use `unbase64()` function to decode base64 strings\n",
    "- Cast the decoded BINARY to STRING using `cast(unbase64(...) AS STRING)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d55b25-0685-4e12-9630-d1dd04a18e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4317b82a-05cf-4717-b3af-4144b868e8bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Step 1: Create raw bronze table from JSON files\n",
    "CREATE TABLE web_events_bronze_raw\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/web-events-json/',\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "SELECT * FROM web_events_bronze_raw LIMIT 10\n",
    "  \n",
    "-- Step 2: Create decoded bronze table with unbase64()\n",
    "CREATE TABLE web_events_bronze_decoded\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  CAST(unbase64(key) AS STRING)   AS decoded_key,\n",
    "  CAST(unbase64(value) AS STRING) AS decoded_value,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic\n",
    "FROM web_events_bronze_raw;\n",
    "\n",
    "\n",
    "-- Step 3: Verify the decoded data\n",
    "\n",
    "SELECT\n",
    "  decoded_key,\n",
    "  decoded_value\n",
    "FROM web_events_bronze_decoded\n",
    "LIMIT 10;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b7c04a8-c77f-463e-878a-e78e7aa21671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 6: Flattening JSON String Columns\n",
    "\n",
    "**Objective:** Extract and flatten fields from JSON-formatted strings.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a table named `web_events_bronze_flattened` from `web_events_bronze_decoded` that extracts:\n",
    "   - `decoded_value:browser` as `browser`\n",
    "   - `decoded_value:page` as `page`\n",
    "   - `decoded_value:action` as `action`\n",
    "   - `decoded_value:location` as `location` (this will be a JSON string)\n",
    "   - `decoded_value:customer_id` as `customer_id`\n",
    "   - Include `decoded_key`, `offset`, `partition`, `timestamp`, `topic`\n",
    "2. Query the table to verify the flattened structure\n",
    "\n",
    "**Hints:**\n",
    "- Use SQL-style JSON path extraction: `decoded_value:field_name`\n",
    "- The `location` field contains nested JSON, so it will remain as a JSON string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b21f3783-d7c8-47cc-a3ad-9387b015ff05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ab6d04-d558-49c9-b40a-6acd69d42f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Create flattened table using JSON path extraction\n",
    "CREATE TABLE web_events_bronze_flattened\n",
    "USING DELTA\n",
    "AS \n",
    "  SELECT \n",
    "    decoded_key,\n",
    "    decoded_value:browser As browser,\n",
    "    decoded_value:page As page,\n",
    "    decoded_value:action As action,\n",
    "    decoded_value:location As location,\n",
    "    decoded_value:customer_id As customer_id,\n",
    "    offset,\n",
    "    partition,\n",
    "    timestamp,\n",
    "    topic\n",
    "FROM \n",
    "    web_events_bronze_decoded\n",
    "\n",
    "SELECT * FROM web_events_bronze_flattened;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22a55cdb-e355-4524-a826-76917137a59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 7: CSV Ingestion with Different Delimiter (Challenge)\n",
    "\n",
    "**Objective:** Ingest CSV files with a pipe delimiter and create a bronze table.\n",
    "\n",
    "**Tasks:**\n",
    "1. Explore the CSV files at `/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/transactions-csv/`\n",
    "2. Create a table named `transactions_bronze` that:\n",
    "   - Reads the pipe-delimited CSV files\n",
    "   - Includes metadata columns: `file_modification_time`, `source_file`, and `ingestion_time`\n",
    "3. Verify the table contains the expected number of rows\n",
    "\n",
    "**Hints:**\n",
    "- The CSV files use pipe (`|`) as delimiter\n",
    "- Use `read_files()` with `sep => '|'` option\n",
    "- Don't forget to set `header => true`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f29664-b2da-422a-89f7-7d5dbd58f735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee2e5ee-6659-45d8-8fd3-61dafd59ccb9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768477197354}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Step 1: List files in the volume\n",
    "\n",
    "SELECT * FROM read_files('/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/transactions-csv/',\n",
    " format => 'csv',\n",
    " sep => '|',\n",
    " header => true)\n",
    "-- Step 2: Create table with pipe delimiter\n",
    "\n",
    "CREATE TABLE transactions_bronze\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT *,\n",
    "    _metadata.file_modification_time AS file_modification_time,\n",
    "    _metadata.file_name AS source_file,\n",
    "    current_timestamp() AS ingestion_time\n",
    "FROM read_files('/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/transactions-csv/',\n",
    " format => 'csv',\n",
    " sep => '|',\n",
    " header => true)\n",
    "-- Step 3: Verify row count\n",
    "\n",
    "SELECT COUNT(*) AS total_rows\n",
    "FROM transactions_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0efa5fb-c96d-4560-8a0a-669d0915a736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 8: Python/PySpark Alternative (Optional)\n",
    "\n",
    "**Objective:** Complete Exercise 3 using Python/PySpark instead of SQL.\n",
    "\n",
    "**Tasks:**\n",
    "1. Recreate the `customers_bronze_metadata` table using PySpark\n",
    "2. Use PySpark functions to:\n",
    "   - Convert Unix timestamp to DATE\n",
    "   - Add file name using `input_file_name()` or `read_files()` SQL function\n",
    "   - Add ingestion timestamp using `current_timestamp()`\n",
    "\n",
    "**Hints:**\n",
    "- You can use `spark.read.format(\"parquet\")` or `spark.sql()` with `read_files()`\n",
    "- Use `from_unixtime()` from `pyspark.sql.functions`\n",
    "- Use `df.write.saveAsTable()` to create the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7af0c6c-fc94-4307-801c-969f89ebae78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a681a6-d129-4345-833b-014393dfcfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# TODO: Write your solution here\n",
    "# Option 1: Using native PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_unixtime, col, current_timestamp, input_file_name\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Exercise3\").getOrCreate()\n",
    "\n",
    "# Read the Parquet files\n",
    "df = spark.read.format(\"parquet\").load(\"/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/\")\n",
    "\n",
    "# Add the necessary transformations:\n",
    "# - Convert the Unix timestamp to a DATE format using from_unixtime\n",
    "# - Add the file name using input_file_name\n",
    "# - Add the ingestion timestamp using current_timestamp\n",
    "df_transformed = df.withColumn(\n",
    "    \"registration_date\", from_unixtime(col(\"registration_timestamp\") / 1000000).cast(\"date\")\n",
    ").withColumn(\n",
    "    \"file_modification_time\", input_file_name()  # Capture the file name\n",
    ").withColumn(\n",
    "    \"source_file\", input_file_name()  # Same as file_modification_time in this case\n",
    ").withColumn(\n",
    "    \"ingestion_time\", current_timestamp()  # Add ingestion time\n",
    ")\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customers_bronze_metadata\")\n",
    "\n",
    "# Option 2: Using spark.sql() with read_files() to access _metadata\n",
    "\n",
    "# Using SQL-style read_files to access _metadata\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE customers_bronze_metadata\n",
    "    USING DELTA\n",
    "    AS\n",
    "    SELECT\n",
    "        *,\n",
    "        DATE(from_unixtime(registration_timestamp / 1000000)) AS registration_date,\n",
    "        _metadata.file_modification_time AS file_modification_time,\n",
    "        _metadata.file_path AS source_file,\n",
    "        current_timestamp() AS ingestion_time\n",
    "    FROM read_files(\n",
    "        '/Volumes/lakeflow_exercise/exercise_schema/exercise_raw/customers-parquet/',\n",
    "        format => 'parquet'\n",
    "    )\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2606c31-10bf-44eb-bc91-5b539f0940e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 9: Data Quality Check (Challenge)\n",
    "\n",
    "**Objective:** Analyze the rescued data and create a cleaned version.\n",
    "\n",
    "**Tasks:**\n",
    "1. From the `inventory_bronze` table created in Exercise 4, create a new table `inventory_bronze_cleaned` that:\n",
    "   - Extracts numeric values from the `_rescued_data` column for malformed `stock_quantity` values\n",
    "   - For rows where `stock_quantity` is NULL but `_rescued_data` contains a quantity, extract and use that value\n",
    "   - Sets `stock_quantity` to NULL for rows where no valid numeric value can be extracted (e.g., \"N/A\")\n",
    "   - Includes all other columns: `product_id`, `product_name`, and metadata columns\n",
    "\n",
    "**Hints:**\n",
    "- Use `COALESCE()` to prefer the original `stock_quantity` value\n",
    "- Use JSON path extraction on `_rescued_data` to get rescued values: `_rescued_data:stock_quantity`\n",
    "- Use `REPLACE()` or string functions to clean extracted values if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e3689ff-3ffc-4088-9efa-26eb4ca5b317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0727730-f89b-45c1-b6b6-4b33647b344a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your solution here\n",
    "-- Create cleaned table by extracting values from _rescued_data\n",
    "CREATE TABLE inventory_bronze_cleaned\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "    product_id,\n",
    "    product_name,\n",
    "    COALESCE(\n",
    "        stock_quantity,\n",
    "        CASE \n",
    "            WHEN REGEXP_REPLACE(_rescued_data:stock_quantity, '[^0-9]', '') != '' \n",
    "            THEN TRY_CAST(REGEXP_REPLACE(_rescued_data:stock_quantity, '[^0-9]', '') AS INT)\n",
    "            ELSE NULL\n",
    "        END\n",
    "    ) AS stock_quantity,\n",
    "    _metadata.file_modification_time AS file_modification_time,\n",
    "    _metadata.file_path AS source_file,\n",
    "    current_timestamp() AS ingestion_time\n",
    "FROM inventory_bronze;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97166165-369a-458e-9d02-3e928f67d348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM inventory_bronze_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ee48ff1-b93f-4760-98e3-bf8330515180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise 10: Summary Query (Verification)\n",
    "\n",
    "**Objective:** Create summary queries to verify all your work.\n",
    "\n",
    "**Tasks:**\n",
    "1. Write a query that shows the row count for each bronze table you created\n",
    "2. Write a query that shows how many rows have rescued data in the `inventory_bronze` table\n",
    "3. Write a query that shows the distribution of browsers in the `web_events_bronze_flattened` table\n",
    "\n",
    "**Hints:**\n",
    "- Use `COUNT(*)` for row counts\n",
    "- Use `WHERE _rescued_data IS NOT NULL` to find rescued rows\n",
    "- Use `GROUP BY` for distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce8b95e-0ca4-4aff-ae08-70e797bf3248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Your Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d012f100-91a8-47e3-b392-ab0fbe8e7db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Write your summary queries here\n",
    "-- Query 1: Row counts for all bronze tables\n",
    "\n",
    "SELECT 'inventory_bronze' AS table_name, COUNT(*) AS row_count\n",
    "FROM inventory_bronze\n",
    "UNION ALL\n",
    "SELECT 'web_events_bronze_flattened' AS table_name, COUNT(*) AS row_count\n",
    "FROM web_events_bronze_flattened\n",
    "UNION ALL\n",
    "SELECT 'customers_bronze_metadata' AS table_name, COUNT(*) AS row_count\n",
    "FROM customers_bronze_metadata\n",
    "UNION ALL\n",
    "SELECT 'transactions_bronze' AS table_name, COUNT(*) AS row_count\n",
    "FROM transactions_bronze;\n",
    "\n",
    "-- Query 2: Count of rows with rescued data\n",
    "\n",
    "SELECT COUNT(*) AS rows_with_rescued_data\n",
    "FROM inventory_bronze\n",
    "WHERE _rescued_data IS NOT NULL;\n",
    "\n",
    "-- Query 3: Browser distribution\n",
    "\n",
    "\n",
    "SELECT browser, COUNT(*) AS browser_count\n",
    "FROM web_events_bronze_flattened\n",
    "GROUP BY browser\n",
    "ORDER BY browser_count DESC;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bbd2111-58ae-431b-85cc-ce768993d690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise Complete!\n",
    "\n",
    "Congratulations on completing the Data Ingestion exercises!\n",
    "\n",
    "**Expected Tables Created:**\n",
    "1. `customers_bronze_ctas` - Customer data ingested with CTAS\n",
    "2. `customers_bronze_copy` - Customer data ingested with COPY INTO\n",
    "3. `customers_bronze_metadata` - Customer data with metadata columns\n",
    "4. `inventory_bronze` - Inventory data with rescued data column\n",
    "5. `inventory_bronze_cleaned` - Cleaned inventory data (Challenge)\n",
    "6. `web_events_bronze_raw` - Raw web events JSON data\n",
    "7. `web_events_bronze_decoded` - Decoded web events data\n",
    "8. `web_events_bronze_flattened` - Flattened web events data\n",
    "9. `transactions_bronze` - Transaction data from CSV files\n",
    "\n",
    "**Key Skills Demonstrated:**\n",
    "- ✅ CTAS with `read_files()`\n",
    "- ✅ COPY INTO with schema evolution\n",
    "- ✅ Adding metadata columns during ingestion\n",
    "- ✅ Handling rescued data columns\n",
    "- ✅ JSON ingestion and base64 decoding\n",
    "- ✅ JSON string flattening\n",
    "- ✅ CSV ingestion with different delimiters\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6049106472219204,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Ingestion_Exercise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
