{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32dad41-6aa7-44f9-ad71-132fd18c28d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup Environment for Lakeflow Data Ingestion\n",
    "\n",
    "This notebook sets up the environment for the Data Ingestion with Lakeflow exercises.\n",
    "\n",
    "It will create:\n",
    "- Catalog: `lakeflow_demo`\n",
    "- Schema: `lakeflow_schema`\n",
    "- Volume: `raw` (within lakeflow_schema)\n",
    "- Sample data files in the raw volume for ingestion exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c749aed-c613-43b2-9c66-bf2780d4bf90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create Catalog and Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9112ca5-17bd-4bad-9f3e-1130b3a9728f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create catalog\n",
    "CREATE CATALOG IF NOT EXISTS lakeflow_demo;\n",
    "\n",
    "-- Create schema within the catalog\n",
    "CREATE SCHEMA IF NOT EXISTS lakeflow_demo.lakeflow_schema;\n",
    "\n",
    "-- Set default catalog and schema\n",
    "USE CATALOG lakeflow_demo;\n",
    "USE SCHEMA lakeflow_schema;\n",
    "\n",
    "-- Verify current catalog and schema\n",
    "SELECT current_catalog(), current_schema();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a09e77-59bd-4dcf-9320-c0d60873eb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3798ef3-40a8-437e-bf56-6e65c63b2eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create volume for raw data files\n",
    "CREATE VOLUME IF NOT EXISTS lakeflow_demo.lakeflow_schema.raw;\n",
    "\n",
    "-- Verify volume creation\n",
    "DESCRIBE VOLUME lakeflow_demo.lakeflow_schema.raw;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839ac59d-f5ba-4b1f-bd23-d7a494de04e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Create Sample Data Files\n",
    "\n",
    "We'll create sample data files in different formats (Parquet, CSV, JSON) for the ingestion exercises.\n",
    "\n",
    "### 3.1: Create Sample Parquet Files (Users Historical Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4dee9c2-5b1e-468d-bb49-302a85006a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate sample user data\n",
    "n_records = 10000\n",
    "user_ids = [f\"UA{str(i).zfill(12)}\" for i in range(1, n_records + 1)]\n",
    "emails = [f\"user{i}@example.com\" for i in range(1, n_records + 1)]\n",
    "\n",
    "# Generate Unix timestamps (in microseconds)\n",
    "base_timestamp = int(datetime(2020, 1, 1).timestamp() * 1_000_000)\n",
    "timestamps = [base_timestamp + random.randint(0, 365*24*60*60*1_000_000) for _ in range(n_records)]\n",
    "\n",
    "# Create DataFrame\n",
    "users_df = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'user_first_touch_timestamp': timestamps,\n",
    "    'email': emails\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(users_df)\n",
    "\n",
    "# Write to Parquet files in the raw volume\n",
    "output_path = \"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\"\n",
    "spark_df.coalesce(5).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Created {n_records} user records in Parquet format at: {output_path}\")\n",
    "print(f\"Files created:\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file in files:\n",
    "    if file.name.startswith('part-'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1382225-6a4c-4ec9-a273-55df1d8b91a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2: Create Sample CSV Files (Sales Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d3a082-c63c-44a7-af2f-7ebc98ab7f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate sample sales data\n",
    "n_records = 5000\n",
    "order_ids = [f\"ORD{str(i).zfill(6)}\" for i in range(1, n_records + 1)]\n",
    "products = [\"Product A\", \"Product B\", \"Product C\", \"Product D\", \"Product E\"]\n",
    "quantities = np.random.randint(1, 10, n_records)\n",
    "prices = np.random.uniform(10.0, 1000.0, n_records).round(2)\n",
    "\n",
    "sales_df = pd.DataFrame({\n",
    "    'order_id': order_ids,\n",
    "    'product': [random.choice(products) for _ in range(n_records)],\n",
    "    'quantity': quantities,\n",
    "    'price': prices,\n",
    "    'sale_date': [datetime.now() - timedelta(days=random.randint(0, 365)) for _ in range(n_records)]\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(sales_df)\n",
    "\n",
    "# Write to CSV files in the raw volume (pipe-delimited)\n",
    "output_path = \"/Volumes/lakeflow_demo/lakeflow_schema/raw/sales-csv/\"\n",
    "spark_df.coalesce(3).write.mode(\"overwrite\").option(\"sep\", \"|\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "print(f\"Created {n_records} sales records in CSV format at: {output_path}\")\n",
    "print(f\"Files created:\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file in files:\n",
    "    if file.name.endswith('.csv'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ec5f3b-c198-41f2-9e10-482914648541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3: Create Sample JSON Files (Kafka Events Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2d1b0e-864c-4d32-8331-1abc330e6163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import base64\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Generate sample Kafka event data\n",
    "n_records = 2000\n",
    "devices = [\"iOS\", \"Android\", \"Linux\", \"Windows\", \"Mac\"]\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"]\n",
    "states = [\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\"]\n",
    "traffic_sources = [\"google\", \"email\", \"direct\", \"social\", \"organic\"]\n",
    "event_names = [\"main\", \"add_item\", \"finalize\", \"purchase\", \"view_item\"]\n",
    "\n",
    "events = []\n",
    "for i in range(n_records):\n",
    "    user_id = f\"UA{str(i+1).zfill(12)}\"\n",
    "    \n",
    "    # Create event value (JSON string)\n",
    "    event_value = {\n",
    "        \"device\": random.choice(devices),\n",
    "        \"ecommerce\": {},\n",
    "        \"event_name\": random.choice(event_names),\n",
    "        \"event_timestamp\": int(datetime.now().timestamp() * 1000) + i,\n",
    "        \"geo\": {\n",
    "            \"city\": random.choice(cities),\n",
    "            \"state\": random.choice(states)\n",
    "        },\n",
    "        \"items\": [],\n",
    "        \"traffic_source\": random.choice(traffic_sources),\n",
    "        \"user_first_touch_timestamp\": int(datetime.now().timestamp() * 1000) - random.randint(0, 86400000),\n",
    "        \"user_id\": user_id\n",
    "    }\n",
    "    \n",
    "    # Encode key and value in base64\n",
    "    key_encoded = base64.b64encode(user_id.encode()).decode()\n",
    "    value_encoded = base64.b64encode(json.dumps(event_value).encode()).decode()\n",
    "    \n",
    "    # Create Kafka event record\n",
    "    kafka_event = {\n",
    "        \"key\": key_encoded,\n",
    "        \"offset\": 219255000 + i,\n",
    "        \"partition\": i % 3,\n",
    "        \"timestamp\": int(datetime.now().timestamp() * 1000) + i,\n",
    "        \"topic\": \"clickstream\",\n",
    "        \"value\": value_encoded\n",
    "    }\n",
    "    \n",
    "    events.append(kafka_event)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(events)\n",
    "\n",
    "# Write to JSON files in the raw volume\n",
    "output_path = \"/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/\"\n",
    "spark_df.coalesce(5).write.mode(\"overwrite\").json(output_path)\n",
    "\n",
    "print(f\"Created {n_records} Kafka event records in JSON format at: {output_path}\")\n",
    "print(f\"Files created:\")\n",
    "files = dbutils.fs.ls(output_path)\n",
    "for file in files:\n",
    "    if file.name.startswith('part-'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7792ea3-b98d-4743-a7db-fbda7dc5576b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4: Create Sample CSV File with Malformed Data (for Rescued Data Exercise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33134f1-85b6-422f-b3de-4db706118650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create sample product data with one malformed row\n",
    "products_data = [\n",
    "    {\"item_id\": \"M_PREM_Q\", \"name\": \"Premium Queen Mattress\", \"price\": 1795.0},\n",
    "    {\"item_id\": \"M_STAN_F\", \"name\": \"Standard Full Mattress\", \"price\": 945.0},\n",
    "    {\"item_id\": \"M_PREM_A\", \"name\": \"Premium Queen Mattress\", \"price\": \"$100.00\"},  # Malformed: price with $ sign\n",
    "    {\"item_id\": \"M_STAN_T\", \"name\": \"Standard Twin Mattress\", \"price\": 595.0}\n",
    "]\n",
    "\n",
    "products_df = pd.DataFrame(products_data)\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv\"\n",
    "dbutils.fs.mkdirs(output_dir)\n",
    "\n",
    "# Write to CSV file (comma-delimited)\n",
    "output_path = f\"{output_dir}/lab_malformed_data.csv\"\n",
    "products_df.to_csv(output_path, index=False, sep=\",\")\n",
    "\n",
    "print(f\"Created malformed CSV file at: {output_path}\")\n",
    "print(f\"File contains {len(products_df)} rows, including one malformed row with price='$100.00'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a2a637-4f0c-4423-86d0-6d0e3a980d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.5: Create Volumes and Data for Streaming Tables (Auto Loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff362ea-2e8a-486d-aa6d-afb9f9354d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Create volumes for streaming table exercises\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS lakeflow_demo.lakeflow_schema.autoloader_staging_files\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS lakeflow_demo.lakeflow_schema.csv_files_autoloader_source\")\n",
    "\n",
    "print(\"Created volumes for streaming table exercises\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abe2410-7d26-43aa-aae5-bc9dc5ebef9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Creating clean CSV files for streaming table exercises...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# CRITICAL: We need to create clean CSV files WITHOUT Spark metadata\n",
    "# Autoloader expects raw file drop zones, not Spark output directories\n",
    "\n",
    "# Step 1: Generate sample sales data\n",
    "n_records_per_file = 2000\n",
    "products = [\"Product A\", \"Product B\", \"Product C\", \"Product D\", \"Product E\"]\n",
    "\n",
    "def create_csv_file(output_path, n_records, file_number):\n",
    "    \"\"\"Create a clean CSV file without Spark metadata\"\"\"\n",
    "    order_ids = [f\"ORD{str(i + file_number * n_records).zfill(6)}\" for i in range(1, n_records + 1)]\n",
    "    quantities = np.random.randint(1, 10, n_records)\n",
    "    prices = np.random.uniform(10.0, 1000.0, n_records).round(2)\n",
    "    \n",
    "    sales_df = pd.DataFrame({\n",
    "        'order_id': order_ids,\n",
    "        'product': [random.choice(products) for _ in range(n_records)],\n",
    "        'quantity': quantities,\n",
    "        'price': prices,\n",
    "        'sale_date': [datetime.now() - timedelta(days=random.randint(0, 365)) for _ in range(n_records)]\n",
    "    })\n",
    "    \n",
    "    # Write CSV content to a string buffer (avoids local filesystem access)\n",
    "    from io import StringIO\n",
    "    csv_buffer = StringIO()\n",
    "    sales_df.to_csv(csv_buffer, index=False, sep=\"|\", lineterminator='\\n')\n",
    "    csv_content = csv_buffer.getvalue()\n",
    "    csv_buffer.close()\n",
    "    \n",
    "    # Write directly to volume using dbutils.fs.put()\n",
    "    # This avoids local filesystem access and Spark metadata files\n",
    "    dbutils.fs.put(output_path, csv_content, overwrite=True)\n",
    "    \n",
    "    return len(sales_df)\n",
    "\n",
    "# Step 2: Clean and prepare Autoloader volumes (remove any existing Spark artifacts)\n",
    "print(\"\\n1. Cleaning Autoloader volumes (removing any Spark artifacts)...\")\n",
    "try:\n",
    "    # Remove all files from autoloader volumes to start clean\n",
    "    for path in [\n",
    "        \"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source\",\n",
    "        \"/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files\"\n",
    "    ]:\n",
    "        try:\n",
    "            existing_files = dbutils.fs.ls(path)\n",
    "            for f in existing_files:\n",
    "                if not f.isDir():\n",
    "                    dbutils.fs.rm(f.path)\n",
    "            print(f\"  Cleaned: {path}\")\n",
    "        except Exception as e:\n",
    "            # Directory might not exist or be empty\n",
    "            dbutils.fs.mkdirs(path)\n",
    "            print(f\"  Created: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Note: {e}\")\n",
    "\n",
    "# Step 3: Create initial CSV file for autoloader source (file drop zone)\n",
    "print(\"\\n2. Creating initial CSV file for csv_files_autoloader_source...\")\n",
    "initial_file_path = \"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source/sales_initial.csv\"\n",
    "rows_created = create_csv_file(initial_file_path, n_records_per_file, 0)\n",
    "print(f\"  ✓ Created: sales_initial.csv with {rows_created} rows\")\n",
    "\n",
    "# Step 4: Create staging CSV files for incremental ingestion demo\n",
    "print(\"\\n3. Creating staging CSV files for autoloader_staging_files...\")\n",
    "staging_files = []\n",
    "for i in range(1, 4):  # Create 3 files\n",
    "    staging_file_path = f\"/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files/sales_staging_{i:03d}.csv\"\n",
    "    rows_created = create_csv_file(staging_file_path, n_records_per_file, i)\n",
    "    staging_files.append(f\"sales_staging_{i:03d}.csv\")\n",
    "    print(f\"  ✓ Created: sales_staging_{i:03d}.csv with {rows_created} rows\")\n",
    "\n",
    "# Step 5: Verify files (should ONLY see CSV files, no Spark metadata)\n",
    "print(\"\\n4. Verifying Autoloader volumes (should contain ONLY CSV files)...\")\n",
    "print(\"\\n   csv_files_autoloader_source:\")\n",
    "source_files = dbutils.fs.ls(\"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source\")\n",
    "csv_files = [f for f in source_files if f.name.endswith('.csv') and not f.isDir()]\n",
    "non_csv = [f for f in source_files if not f.name.endswith('.csv') and not f.isDir()]\n",
    "print(f\"     CSV files: {len(csv_files)}\")\n",
    "for f in csv_files:\n",
    "    print(f\"       - {f.name} ({f.size:,} bytes)\")\n",
    "if non_csv:\n",
    "    print(f\"     ⚠ WARNING: Found {len(non_csv)} non-CSV files (should be 0):\")\n",
    "    for f in non_csv:\n",
    "        print(f\"       - {f.name}\")\n",
    "\n",
    "print(\"\\n   autoloader_staging_files:\")\n",
    "staging_files_list = dbutils.fs.ls(\"/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files\")\n",
    "staging_csv = [f for f in staging_files_list if f.name.endswith('.csv') and not f.isDir()]\n",
    "staging_non_csv = [f for f in staging_files_list if not f.name.endswith('.csv') and not f.isDir()]\n",
    "print(f\"     CSV files: {len(staging_csv)}\")\n",
    "for f in staging_csv:\n",
    "    print(f\"       - {f.name} ({f.size:,} bytes)\")\n",
    "if staging_non_csv:\n",
    "    print(f\"     ⚠ WARNING: Found {len(staging_non_csv)} non-CSV files (should be 0):\")\n",
    "    for f in staging_non_csv:\n",
    "        print(f\"       - {f.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Streaming table data setup complete!\")\n",
    "print(\"\\nThese volumes are now clean file drop zones suitable for Autoloader:\")\n",
    "print(f\"  • Initial file: /Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source/\")\n",
    "print(f\"  • Staging files: /Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files/\")\n",
    "print(\"\\nNote: Files were created using pandas (not Spark) to avoid metadata artifacts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3d946e-a7f6-41d9-a771-e6fddc96ab88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Verify Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85f9637-feaa-480a-ab3d-82459c7d8773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify catalog and schema\n",
    "SHOW SCHEMAS IN lakeflow_demo;\n",
    "\n",
    "-- Verify volume\n",
    "DESCRIBE VOLUME lakeflow_demo.lakeflow_schema.raw;\n",
    "\n",
    "-- List files in raw volume\n",
    "LIST '/Volumes/lakeflow_demo/lakeflow_schema/raw';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecefd2e3-a71d-4c09-9d9f-19ff8e1ea296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Complete!\n",
    "\n",
    "Your environment is now ready for the Data Ingestion with Lakeflow exercises.\n",
    "\n",
    "**Catalog:** `lakeflow_demo`  \n",
    "**Schema:** `lakeflow_schema`  \n",
    "**Volumes:**\n",
    "- `raw` (located at `/Volumes/lakeflow_demo/lakeflow_schema/raw/`)\n",
    "- `csv_files_autoloader_source` (for streaming table exercises)\n",
    "- `autoloader_staging_files` (for incremental ingestion demo)\n",
    "\n",
    "**Sample data files created:**\n",
    "- Parquet files: `/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/`\n",
    "- CSV files: `/Volumes/lakeflow_demo/lakeflow_schema/raw/sales-csv/`\n",
    "- JSON files: `/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/`\n",
    "- Malformed CSV: `/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv`\n",
    "\n",
    "**Streaming table data:**\n",
    "- Initial CSV file: `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source/` (1 file)\n",
    "- Staging CSV files: `/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files/` (3 files)\n",
    "\n",
    "You can now proceed with the Data Ingestion exercises!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5462007867612629,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Setup_Environment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
