{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9891fa-96a2-44ea-8861-58310da72c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion with Lakeflow Connect\n",
    "\n",
    "This module introduces **Lakeflow Connect** as a scalable, production-ready framework for ingesting data into Databricks from diverse source systems.\n",
    "\n",
    "You will start by understanding the two connector types:\n",
    "- **Standard Connectors** (object storage, files, event-based ingestion)\n",
    "- **Managed Connectors** (enterprise SaaS and database sources)\n",
    "\n",
    "Next, you will learn common ingestion patterns:\n",
    "- Batch ingestion  \n",
    "- Incremental batch ingestion  \n",
    "- Streaming ingestion  \n",
    "\n",
    "The module reinforces why **Delta tables** and the **Medallion Architecture (Bronze → Silver → Gold)** are foundational for reliable ingestion pipelines.\n",
    "\n",
    "You will then gain hands-on exposure to ingesting data from cloud object storage using **Lakeflow Connect Standard Connectors**, covering:\n",
    "- `CREATE TABLE AS SELECT (CTAS)`\n",
    "- `COPY INTO`\n",
    "- **Auto Loader**\n",
    "\n",
    "Each approach is discussed with its benefits, trade-offs, and when to use it in real production scenarios.\n",
    "\n",
    "You will also learn how to:\n",
    "- Append ingestion metadata columns at the **Bronze** layer\n",
    "- Use and manage the **rescued data column** to safely handle schema drift and malformed records\n",
    "\n",
    "The module further covers:\n",
    "- Ingesting and flattening **semi-structured JSON data**\n",
    "- Enterprise-grade ingestion using **Lakeflow Connect Managed Connectors**\n",
    "\n",
    "Finally, you will explore alternative ingestion strategies such as:\n",
    "- `MERGE INTO`–based ingestion patterns\n",
    "- Leveraging datasets from the **Databricks Marketplace**\n",
    "\n",
    "By the end of this module, you will have a solid foundation to design and implement robust, scalable ingestion pipelines in modern data engineering architectures.\n",
    "\n",
    "---\n",
    "\n",
    "**Environment Setup:**\n",
    "- **Catalog:** `lakeflow_demo`\n",
    "- **Schema:** `lakeflow_schema`\n",
    "- **Volume:** `raw` (located at `/Volumes/lakeflow_demo/lakeflow_schema/raw/`)\n",
    "\n",
    "**Note:** Make sure to run the `00_Setup_Environment.ipynb` notebook first to create the catalog, schema, volume, and sample data files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46c8f0c-8996-45da-abac-d35ad850adb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog Overview\n",
    "\n",
    "Unity Catalog = unified governance, not a single namespace.\n",
    "\n",
    "- \"Unity\" does NOT mean \"one catalog.\"\n",
    "- It means one control plane.\n",
    "\n",
    "It unifies:\n",
    "\n",
    "- Security (GRANT/REVOKE once, enforced everywhere)\n",
    "- Identity (users, groups, service principals)\n",
    "- Lineage\n",
    "- Auditing\n",
    "- Metastore\n",
    "\n",
    "All of that lives in one metastore per region, governed centrally by Databricks.\n",
    "\n",
    "That's the \"Unity.\"\n",
    "\n",
    "### Unity Catalog Hierarchy\n",
    "\n",
    "- **Cloud Storage** (S3 / ADLS / GCS)\n",
    "  - ↓\n",
    "  - **Unity Catalog Metastore** (one per region)\n",
    "    - ↓\n",
    "    - **Catalog** (e.g., `lakeflow_demo`)\n",
    "      - ↓\n",
    "      - **Schema** (e.g., `lakeflow_schema`)\n",
    "        - ↓\n",
    "        - **Table / View / Volumes / Function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e209f6-d51a-4afd-8800-9fd5a92ec56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270ce73b-d56c-4bd4-9690-3e3c2a8bdd89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Set default catalog and schema\n",
    "USE CATALOG lakeflow_demo;\n",
    "USE SCHEMA lakeflow_schema;\n",
    "\n",
    "-- View current catalog and schema\n",
    "SELECT \n",
    "  current_catalog(), \n",
    "  current_schema();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf097d8-b75f-400d-9528-c4c363fc933f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Data Ingestion with CREATE TABLE AS and COPY INTO\n",
    "\n",
    "In this demonstration, we'll explore ingesting data from cloud storage into Delta tables with the `CREATE TABLE AS (CTAS)` AND `COPY INTO` statements.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "- Use the CTAS statement with `read_files()` to ingest Parquet files into a Delta table.\n",
    "- Use `COPY INTO` to incrementally load Parquet files from cloud object storage into a Delta table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b07532-b912-45dc-8304-405c45b9a28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Data Source Files\n",
    "\n",
    "1. We'll create a table containing historical user data from Parquet files stored in the volume  \n",
    "   `'/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/'` within Unity Catalog.\n",
    "\n",
    "   Use the `LIST` statement to view the files in this volume. Run the cell and review the results.\n",
    "\n",
    "   Notice the files in the **name** column begin with **part-**. This shows that this volume contains multiple **Parquet** files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea970760-032c-4cd3-b355-e160db1c2e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc60b882-77f0-4f8c-b39c-fd10604b7394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Query the Parquet files by path in the `/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/` directory to view the raw data in tabular format to quickly preview the files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa66f205-b68f-4b46-9bdd-6f34c50415f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM parquet.`/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/`\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90bf99c-2bc3-4e18-b6aa-5ab89b5454e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Batch Data Ingestion with CTAS and read_files()\n",
    "\n",
    "The `CREATE TABLE AS` (CTAS) statement is used to create and populate tables using the results of a query. This allows you to create a table and load it with data in a single step, streamlining data ingestion workflows.\n",
    "\n",
    "#### Automatic Schema Inference for Parquet Files\n",
    "\n",
    "Apache Parquet is a columnar storage format optimized for analytical queries. It includes embedded schema metadata (e.g., column names and data types), which enables automatic schema inference when creating tables from Parquet files. This eliminates the need for manual schema definitions and simplifies the process of converting Parquet files into Delta format by leveraging the built-in schema metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486090a4-338b-476a-803c-a396da23f8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. CTAS with the `read_files()` Function\n",
    "\n",
    "The code in the next cell creates a table using CTAS with the `read_files()` function.\n",
    "\n",
    "The `read_files()` table-valued function (TVF) enables reading a variety of file formats and provides additional options for data ingestion.\n",
    "\n",
    "1. Use the `read_files()` function to query the same Parquet files located in `/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/`. The `LIMIT` clause limits the amount of rows during exploration and development.\n",
    "\n",
    "   - The first parameter in `read_files` is the path to the data.\n",
    "\n",
    "   - The `format => \"parquet\"` option specifies the file format.\n",
    "\n",
    "   The `read_files` function automatically detects the file format and infers a unified schema across all files. It also supports explicit schema definitions and `schemaHints`. For more details on schema inference capabilities, refer to the [Schema inference](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files#schema-inference) documentation.\n",
    "\n",
    "**NOTE:** A **_rescued_data** column is automatically included by default to capture any data that doesn't match the inferred schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c12ed02-fe3b-4265-8d5f-d24ca8271aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "  '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/',\n",
    "  format => 'parquet'\n",
    ")\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a6224c-d286-4790-bd44-1d546e73cc25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Next, let's use `read_files()` with a CTAS statement to create the table **historical_users_bronze_ctas_rf**, then display the table.\n",
    "\n",
    "   Notice that the Parquet files were ingested to create a table (Delta by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b568e194-b91b-4938-a1e5-87ed4c7161f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze_ctas_rf;\n",
    "\n",
    "-- Create the Delta table\n",
    "CREATE TABLE historical_users_bronze_ctas_rf \n",
    "AS\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "        '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/',\n",
    "        format => 'parquet'\n",
    "      );\n",
    "\n",
    "-- Preview the Delta table\n",
    "SELECT * \n",
    "FROM historical_users_bronze_ctas_rf \n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9bfd9b0-60c4-498f-a1e3-483c1bfac07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the `DESCRIBE TABLE EXTENDED` statement to view column names, data types, and additional table metadata.  \n",
    "\n",
    "   Review the results and notice the following:\n",
    "   \n",
    "   - The table was created in your schema within the catalog **lakeflow_demo**.\n",
    "\n",
    "   - The *Type* row indicates that the table is *MANAGED*.\n",
    "   \n",
    "   - The *Provider* row specifies that the table is a Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a839973-d177-4976-823f-f0a6b19a82b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED historical_users_bronze_ctas_rf;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0c393e-7d54-42ca-bdfd-a5dfc8160bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Incremental Data Ingestion with `COPY INTO`\n",
    "\n",
    "`COPY INTO` is a Databricks SQL command that allows you to load data from a file location into a Delta table. This operation is re-triable and idempotent, i.e. files in the source location that have already been loaded are skipped. This command is useful for when you need to load data into an existing Delta table. \n",
    "\n",
    "[COPY INTO](https://docs.databricks.com/aws/en/sql/language-manual/delta-copy-into)\n",
    "\n",
    "### D1. Ingesting Parquet Files with COPY INTO\n",
    "\n",
    "Using the same set of Parquet files as before, let's use `COPY INTO` to create our Bronze table again.\n",
    "\n",
    "We will look at two examples:\n",
    "\n",
    "1. Example 1: Common Schema Mismatch Error\n",
    "\n",
    "2. Example 2: Preemptively Handling Schema Evolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1371c3a-72ce-4f77-a9fa-f68436c046ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 1: Common Schema Mismatch Error\n",
    "\n",
    "1. The cell below creates an empty table named **historical_users_bronze_ci** with a defined schema for only the **user_id** and **user_first_touch_timestamp** columns.\n",
    "\n",
    "   However, the Parquet files in `'/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/'` contain three columns: \n",
    "    - **user_id**\n",
    "    - **user_first_touch_timestamp** \n",
    "    - **email**\n",
    "\n",
    "   Run the cell below and review the error. You should see the `[COPY_INTO_SCHEMA_MISMATCH_WITH_TARGET_TABLE]` error. This error occurs because there is a schema mismatch: the Parquet files contain 3 columns, but the target table **historical_users_bronze_ci** only has 2 columns.\n",
    "\n",
    "   How can you handle this error?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbf9370-ef25-4b61-82a6-ca9a130a852c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--------------------------------------------\n",
    "-- This cell returns an error\n",
    "--------------------------------------------\n",
    "\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze_ci;\n",
    "\n",
    "-- Create an empty table with the specified table schema (only 2 out of the 3 columns)\n",
    "CREATE TABLE historical_users_bronze_ci (\n",
    "  user_id STRING,\n",
    "  user_first_touch_timestamp BIGINT\n",
    ");\n",
    "\n",
    "-- Use COPY INTO to populate Delta table\n",
    "COPY INTO historical_users_bronze_ci\n",
    "  FROM '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    " SELECT * FROM historical_users_bronze_ci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d7d4fb-defc-4858-9e26-a14162e3b341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. We can fix this error by adding `COPY_OPTIONS` with the `mergeSchema = 'true'` option. When set to `true`, this option allows the schema to evolve based on the incoming data.\n",
    "\n",
    "   Run the next cell with the `COPY_OPTIONS` option added. You should notice that the Parquet files were successfully ingested into the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52174795-7b8d-4e6a-b26e-9f3ba4898583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO historical_users_bronze_ci\n",
    "  FROM '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');     -- Merge the schema of each file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c4d65a-ef6f-44d1-b279-56e126f055b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Preview the data in the **historical_users_bronze_ci** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14e6f95-6532-439b-8fc0-aab3667bb22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM historical_users_bronze_ci\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab51829-25ff-41f9-bb05-cbfa7e899008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Idempotency (Incremental Ingestion)\n",
    "\n",
    "`COPY INTO` tracks the files it has previously ingested. If the command is run again, no additional data is ingested because the files in the source directory haven't changed.\n",
    "\n",
    "1. Let's run the `COPY INTO` command again and check if any data is re-ingested into the table.\n",
    "\n",
    "   Run the cell and view the results. Notice that the values for **num_affected_rows**, **num_inserted_rows**, and **num_skipped_corrupt_files** are all 0 because the data has already been ingested into the Delta table.\n",
    "\n",
    "**NOTE**: If new files are added to the cloud storage location, `COPY INTO` will only ingest those files. Using `COPY INTO` is a great option if you want to run a job for incremental batch ingestion from cloud storage location without re-reading files that have already been loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cccfc42-812f-421b-9378-1b84676cc5d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO historical_users_bronze_ci\n",
    "  FROM '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/'\n",
    "  FILEFORMAT = parquet\n",
    "  COPY_OPTIONS ('mergeSchema' = 'true');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788e25cc-f4c3-4bbd-8d5b-8617d3e643bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Adding Metadata Columns During Ingestion\n",
    "\n",
    "In this demonstration, we'll explore how to add metadata columns during data ingestion. \n",
    "\n",
    "This process will include adding metadata, converting Unix timestamps to standard `DATE` format, and row ingestion times.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "- Modify columns during data ingestion from cloud storage to your bronze table.\n",
    "- Add the current ingestion timestamp to the bronze.\n",
    "- Use the `_metadata` column to extract file-level metadata (e.g., file name, modification time) during ingestion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d06ab4-81ac-4f08-8bce-607d184b0012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Data Source Files\n",
    "\n",
    "1. We'll create a table containing historical user data from Parquet files stored in the volume  \n",
    "   `'/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/'` within Unity Catalog.\n",
    "\n",
    "   Use the `LIST` statement to view the files in this volume. Run the cell and review the results.\n",
    "\n",
    "   View the values in the **name** column that begin with **part-**. This shows that this volume contains multiple **Parquet** files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28642227-fb8b-4278-a19a-3387cb44c819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefaaede-549e-45a8-8659-b030a5fb9a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Adding Metadata Columns to the Bronze Table During Ingestion\n",
    "\n",
    "When ingesting data into the Bronze layer, you can apply transformations during ingestion and also retrieve metadata about the input files using the **_metadata** column.\n",
    "\n",
    "The **_metadata** column is a hidden column available for all supported file formats. To include it in the returned data, you must explicitly select it in the read query that specifies the source.\n",
    "\n",
    "### Ingestion Requirements\n",
    "\n",
    "During data ingestion, we'll perform the following actions:\n",
    "\n",
    "1. Convert the parquet Unix timestamp to a `DATE` column.\n",
    "\n",
    "2. Include the **input file name** to indicate the data raw source.\n",
    "\n",
    "3. Include the **last modification** timestamp of the input file.\n",
    "\n",
    "4. Add the **file ingestion time** to the Bronze table.\n",
    "\n",
    "**Note:** The `_metadata` column is available across all supported input file formats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5070588a-9fe0-47ed-ae01-17ac6ea0332d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to display the parquet data in the `\"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\"` volume and view the results.\n",
    "\n",
    "    Notice that the **user_first_touch_timestamp** column has a Unix timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d6d850-c3ad-47d7-9dd5-1844fca2ce4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\",\n",
    "  format => 'parquet')\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e87bd2f-fa6a-4ef9-aa03-87364f982ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Convert the Unix Time on Ingestion to Bronze\n",
    "\n",
    "The Unix timestamp column **user_first_touch_timestamp** values represent the time in microseconds since the Unix epoch (January 1, 1970).\n",
    "\n",
    "To create a readable date column, use the [`from_unixtime()`](https://docs.databricks.com/en/sql/language-manual/functions/from_unixtime.html) function, converting the **user_first_touch_timestamp** from microseconds to seconds by dividing by 1,000,000.\n",
    "\n",
    "1. Run the query and review the results. The query generates a new column, **first_touch_date**, by converting the Unix timestamp into a human-readable date column.\n",
    "\n",
    "   Run the cell and view the **first_touch_date** column. Notice the **first_touch_date** column is cast to a data type of **DATE**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d989ad2e-0361-4a5d-876b-6d6271ea2bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *,\n",
    "  cast(from_unixtime(user_first_touch_timestamp/1000000) AS DATE) AS first_touch_date\n",
    "FROM read_files(\n",
    "  \"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\",\n",
    "  format => 'parquet')\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ff1866-181f-496b-9669-210be166ab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Adding Column Metadata on Ingestion\n",
    "\n",
    "The following metadata can be added to the bronze table:\n",
    "\n",
    "- `_metadata.file_modification_time`: Adds the last modification time of the input file.\n",
    "\n",
    "- `_metadata.file_name`: Adds the input file name.\n",
    "\n",
    "- [`current_timestamp()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/current_timestamp): Returns the current timestamp (`TIMESTAMP` data type) when the query starts, useful for tracking ingestion time.\n",
    "\n",
    "You can read more about the `_metadata` column in the [Databricks documentation](https://docs.databricks.com/en/ingestion/file-metadata-column.html).\n",
    "\n",
    "1. Run the query below to add the following columns:\n",
    "\n",
    "   - **file_modification_time** and **file_name**, using the **_metadata** column to capture input file details.  \n",
    "   \n",
    "   - **ingestion_time**, which records the exact time the data was ingested.\n",
    "\n",
    "   Review the results. You should see the new columns **file_modification_time**, **source_file**, and **ingestion_time** added to the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2497a387-2ee0-4411-8f1a-4f6cf746cc39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *,\n",
    "  cast(from_unixtime(user_first_touch_timestamp / 1000000) AS DATE) AS first_touch_date,\n",
    "  _metadata.file_modification_time AS file_modification_time,      -- Last data source file modification time\n",
    "  _metadata.file_name AS source_file,                              -- Ingest data source file name\n",
    "  current_timestamp() as ingestion_time                            -- Ingestion timestamp\n",
    "FROM read_files(\n",
    "  \"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\",\n",
    "  format => 'parquet')\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c57042-437f-4b04-8232-56632d29b3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Creating the Final Bronze Table\n",
    "\n",
    "1. Put it all together with the `CTAS` statement to create the Delta table.\n",
    "\n",
    "    Run the cell to create and view the new table **historical_users_bronze**.\n",
    "    \n",
    "    Confirm that the new columns **first_touch_date**, **file_modification_time**, **source_file** and **ingestion_time** were created successfully in the bronze table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07de85a-c2bf-4168-976d-0d6756326462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze;\n",
    "\n",
    "-- Create an empty table\n",
    "CREATE TABLE historical_users_bronze AS\n",
    "SELECT\n",
    "  *,\n",
    "  cast(from_unixtime(user_first_touch_timestamp / 1000000) AS DATE) AS first_touch_date,\n",
    "  _metadata.file_modification_time AS file_modification_time,      -- Last data source file modification time\n",
    "  _metadata.file_name AS source_file,                              -- Ingest data source file name\n",
    "  current_timestamp() as ingestion_time                            -- Ingestion timestamp\n",
    "FROM read_files(\n",
    "  \"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\",\n",
    "  format => 'parquet');\n",
    "\n",
    "\n",
    "-- View the final bronze table\n",
    "SELECT * \n",
    "FROM historical_users_bronze\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea09bb0-4f02-4919-bf5e-c510c1b96195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Handling CSV Ingestion with the Rescued Data Column\n",
    "\n",
    "In this demonstration, we will focus on ingesting CSV files into Delta Lake using the `CTAS` (`CREATE TABLE AS SELECT`) pattern with the `read_files()` method and exploring the rescued data column. \n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Ingest CSV files as Delta tables using the `CREATE TABLE AS SELECT` (CTAS) statement with the `read_files()` function.\n",
    "- Define and apply an explicit schema with `read_files()` to ensure consistent and reliable data ingestion.\n",
    "- Handle and inspect rescued data that does not conform to the defined schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440a1db5-c627-49bb-a823-fcebc55b1113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Inspect the Dataset\n",
    "\n",
    "1. Let's take a look at our CSV file with malformed data. \n",
    "\n",
    "    The query should use `text.<path>` to return the headers and rows from the CSV file. \n",
    "\n",
    "    Run the cell and view row 4. Notice that the value for the price contains a `$`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15756b5-e594-41d0-8e95-288829eb2480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM text.`/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv`;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d553eb75-bb4c-4fce-9cae-ab45cfb8c480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Ingesting and Rescuing Malformed Data\n",
    "\n",
    "Begin developing your query to ingest the CSV file in the specified path and view malformed records using the **_rescued_data** column.\n",
    "\n",
    "#### Requirements\n",
    "Your final SQL query should ingest the CSV file using CTAS and `read_files`. **In the cell below, do not create a table yet. Simply start developing your query to ingest and create the table**:\n",
    "\n",
    "1. Select all columns from the raw CSV file.\n",
    "\n",
    "2. Use the `read_files()` function with appropriate options to read the CSV file. \n",
    "   - **HINT:** Note that the delimiter is a comma (`,`) not a pipe (`|`).\n",
    "\n",
    "3. Explicitly define the schema for ingestion. The schema is defined as follows:  \n",
    "   - `item_id` (STRING)  \n",
    "   - `name` (STRING)  \n",
    "   - `price` (DOUBLE)\n",
    "\n",
    "4. Use the correct option to include the rescued data column and name it **_rescued_data** to capture malformed rows.\n",
    "\n",
    "   - **HINT**: If you define a schema you must [use the rescuedDataColumn option](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files#csv-options) to add the **_rescued_data** column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f79cc2-dd0a-491b-ad41-179d1ebf728d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "        '/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \",\",\n",
    "        header => true,\n",
    "        schema => '''\n",
    "              item_id STRING, \n",
    "              name STRING, \n",
    "              price DOUBLE\n",
    "        ''',\n",
    "        rescueddatacolumn => \"_rescued_data\"\n",
    "      );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eaa923c-13bd-49b9-a6b1-097704b1c8cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Add Additional Metadata Columns During Ingestion\n",
    "\n",
    "Next, you can create the final bronze table named **products_bronze** that contains the additional metadata columns. Use the query you created above as the starting point.\n",
    "\n",
    "### Final Table Requirements\n",
    "\n",
    "Incorporate the SQL query you created in the previous section and complete the following:\n",
    "\n",
    "1. Use a CTAS statement to create the final bronze Delta table named **products_bronze**. \n",
    "\n",
    "2. Ingest the same CSV file `/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv`\n",
    "\n",
    "3. Use the same defined schema:  \n",
    "   - `item_id` (STRING)  \n",
    "   - `name` (STRING)  \n",
    "   - `price` (DOUBLE)\n",
    "\n",
    "4. Use the `_metadata` column to create two new columns named **file_modification_time** and **source_file**  within your SELECT statement.\n",
    "   - **HINT:** [_metadata](https://docs.databricks.com/en/ingestion/file-metadata-column.html)\n",
    "\n",
    "5. Add a column named **ingestion_time** that provides a timestamp for ingestion. \n",
    "   - **HINT:** Use the [current_timestamp()](https://docs.databricks.com/aws/en/sql/language-manual/functions/current_timestamp) to record the current timestamp at the start of the query evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df493f97-947f-4298-a959-33cdcf3528a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS products_bronze;\n",
    "\n",
    "-- Create the Delta table\n",
    "CREATE TABLE products_bronze \n",
    "AS\n",
    "SELECT\n",
    "  *,\n",
    "  _metadata.file_modification_time AS file_modification_time,\n",
    "  _metadata.file_name AS source_file, \n",
    "  current_timestamp() as ingestion_time\n",
    "FROM read_files(\n",
    "        '/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \",\",\n",
    "        header => true,\n",
    "        schema => 'item_id STRING, name STRING, price DOUBLE', \n",
    "        rescueddatacolumn => \"_rescued_data\"\n",
    "      );\n",
    "\n",
    "-- View the final table\n",
    "SELECT * \n",
    "FROM products_bronze;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d34884b-dbd5-4dd2-a86a-de1863bfc795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Ingesting JSON Files with Databricks\n",
    "\n",
    "In this demonstration, we'll explore how to ingest JSON files and perform foundational JSON-specific transformations during ingestion, including decoding encoded fields and flattening nested JSON strings. We'll be working with simulated Kafka event data.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Ingest raw JSON data into Unity Catalog using CTAS and `read_files()`.\n",
    "- Apply multiple techniques to flatten JSON string columns with and without converting to a STRUCT type.\n",
    "- Understand the difference between `explode()` and `explode_outer()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf90f5e-5e09-4afd-9036-a42edd39417c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Overview of CTAS with `read_files()` for Ingestion of JSON files\n",
    "\n",
    "### B1. Inspect JSON files\n",
    "\n",
    "1. Run the next cell to verify that there are JSON files located at `/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8f50e9-892f-40a4-ad7d-36ae45f8d49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56a9f76-a0cc-4000-b6e7-f6c9ee28fb20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to view the raw JSON data in the output. Note the following:\n",
    "\n",
    "   - Each row contains JSON with 6 key/value pairs.\n",
    "\n",
    "   - The **key** and **value** fields are encoded in base64. Base64 is an encoding scheme that converts binary data into a readable ASCII string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd70b060-2bcb-4c71-8ea4-be69edcffeb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM text.`/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/`\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a17cd8-5f0b-4569-925e-206e0414e1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to see how to use `read_files()` to read the JSON data. Notice the following:\n",
    "\n",
    "   - The JSON file is cleanly read into a tabular format with 6 columns.\n",
    "\n",
    "   - The **key** and **value** columns are base64-encoded and returned as STRING data type.\n",
    "   \n",
    "   - There are no rows in the **_rescued_data** column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe07e57-8956-4c89-8cff-4fd5042e2934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/\",\n",
    "  format => \"json\"\n",
    ")\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6caf39-8201-43a7-a3d6-1534e5ef1937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Using CTAS and `read_files()` with JSON\n",
    "\n",
    "Ingesting JSON files using `read_files()` is as straightforward as reading CSV files.\n",
    "\n",
    "1. Run the cell below to store this raw data in the **kafka_events_bronze_raw** table and view the table. When inspecting the results, you'll notice that:\n",
    "\n",
    "   - The **key** and **value** columns are of type STRING and contain data that is **base64-encoded**.\n",
    "\n",
    "   - This means the actual content has been encoded into base64 format and stored as a string. \n",
    "   \n",
    "   - They have not yet been transformed into a readable string in the first bronze table we create.\n",
    "\n",
    "**NOTE:** Base64 encoding is commonly used when ingesting data from sources like message queues or streaming platforms, where preserving formatting and avoiding data corruption is important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217fcfe0-88e6-4422-8bf4-70d7dfec704d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS kafka_events_bronze_raw;\n",
    "\n",
    "-- Create the Delta table\n",
    "CREATE TABLE kafka_events_bronze_raw AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/\",\n",
    "  format => \"json\"\n",
    ");\n",
    "\n",
    "-- Display the table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_raw\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8584f5-4d38-409c-9ed2-9e3e2f44bf04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Decoding base64 Strings for the Bronze Table\n",
    "\n",
    "1. Let's take a look at decoding the **key** and **value** columns by inspecting their data types after applying the `unbase64()` function. The `unbase64` function returns a decoded base64 string as binary.\n",
    "\n",
    "    - **encoded_key**: The original encoded **key** column as a base64 string.\n",
    "\n",
    "    - **decoded_key**: A new column created by decoding **key** from a base64 string to BINARY.\n",
    "\n",
    "    - **encoded_value**: The original encoded **value** column as a base64 string.\n",
    "\n",
    "    - **decoded_value**: A new column created by decoding **value** from a base64 string to BINARY.\n",
    "\n",
    "    Run the cell and view the results. Notice that the **decoded_key** and **decoded_value** columns are now BINARY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dba4218-f941-498e-8db4-c4999dc197f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  key AS encoded_key,\n",
    "  unbase64(key) AS decoded_key,\n",
    "  value AS encoded_value,\n",
    "  unbase64(value) AS decoded_value\n",
    "FROM kafka_events_bronze_raw\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205afee0-1d04-4491-9207-1b0e2a6da912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the next cell to convert the BINARY columns to STRING columns using the `CAST` function. Notice the following in the results:\n",
    "\n",
    "    - The **decoded_key** and **decoded_value** columns are now of type STRING and readable.\n",
    "\n",
    "    - The **decoded_value** column is a JSON-formatted string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4498a992-43bd-4fd6-be50-47978bd12300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  key AS encoded_key,\n",
    "  cast(unbase64(key) AS STRING) AS decoded_key,\n",
    "  value AS encoded_value,\n",
    "  cast(unbase64(value) AS STRING) AS decoded_value\n",
    "FROM kafka_events_bronze_raw\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e5e497-2cb4-40f6-8737-2b9a1973a5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Now, let's put it all together to create another bronze-level table named **kafka_events_bronze_decoded**. This table will store the STRING values for the **key** and **value** columns from the original **kafka_events_bronze_raw** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1080d823-e047-403a-a410-43eed1667bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE kafka_events_bronze_decoded AS\n",
    "SELECT\n",
    "  cast(unbase64(key) AS STRING) AS decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  cast(unbase64(value) AS STRING) AS decoded_value\n",
    "FROM kafka_events_bronze_raw;\n",
    "\n",
    "-- View the new table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_decoded\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41117c19-3d00-490e-b394-53e77108d694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Working with JSON Formatted Strings in a Table\n",
    "\n",
    "### C1. Flattening JSON String Columns\n",
    "\n",
    "Next, we will explore how extract a column from a column containing a JSON formatted string. \n",
    "\n",
    "**BENEFITS**\n",
    "- **Simple** - Easy to implement and store JSON as plain text.\n",
    "- **Flexible** - Can hold any JSON structure without schema constraints.\n",
    "\n",
    "**CONSIDERATIONS**\n",
    "- **Performance** - STRING columns are slower when querying and processing complex data.\n",
    "- **No Schema** - The lack of a defined schema for STRING columns can lead to data integrity issues.\n",
    "- **Complex to Query** - Requires additional code to parse and retrieve data, which can be complex.\n",
    "\n",
    "#### C1.1 Query JSON strings\n",
    "\n",
    "You can extract a column from fields containing JSON strings using the syntax: `<column-name>:<extraction-path>`, where `<column-name>` is the string column name and `<extraction-path>` is the path to the field to extract. The returned results are strings. You can also do this with nested fields by using either `.` or `[]`.\n",
    "\n",
    "This utilizes Spark SQL's built-in functionality to interact directly with nested data stored as JSON strings.\n",
    "\n",
    "[Query JSON strings](https://docs.databricks.com/aws/en/semi-structured/json)\n",
    "\n",
    "1. For example, let's extract the following values from the JSON-formatted string:\n",
    "    - `decoded_value:device`\n",
    "    - `decoded_value:traffic_source`\n",
    "    - `decoded_value:geo`\n",
    "    - `decoded_value:items`\n",
    "\n",
    "    Run the cell and view the results. Notice that we have successfully extracted the values from the JSON formatted string.\n",
    "\n",
    "    - **device** is a STRING\n",
    "\n",
    "    - **traffic_source** is a STRING\n",
    "\n",
    "    - **geo** is a STRING containing another JSON formatted string\n",
    "    \n",
    "    - **item** is a STRING contain an array of JSON formatted strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa148b2-b4a6-4c34-bdb3-422a5f7cc466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  decoded_value,\n",
    "  decoded_value:device,\n",
    "  decoded_value:traffic_source,\n",
    "  decoded_value:geo,       -- Contains another JSON formatted string\n",
    "  decoded_value:items      -- Contains a nested-array of JSON formatted strings\n",
    "FROM kafka_events_bronze_decoded\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5561b7-4c7c-42ba-99b1-a82ce770562b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. We can then begin to parse out the necessary JSON formatted string values to create another bronze table to flatten the JSON formatted string column for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2f226a-718e-49f0-b6ee-c221e2f18939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE kafka_events_bronze_string_flattened AS\n",
    "SELECT\n",
    "  decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  decoded_value:device,\n",
    "  decoded_value:traffic_source,\n",
    "  decoded_value:geo,       -- Contains another JSON formatted string\n",
    "  decoded_value:items      -- Contains a nested-array of JSON formatted strings\n",
    "FROM kafka_events_bronze_decoded;\n",
    "\n",
    "-- Display the table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_string_flattened\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc22049b-acd9-48d9-89b2-b53e940bdeac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Flattening JSON Formatting Strings via STRUCT Conversion\n",
    "\n",
    "Similar to the previous section, we will discuss how to flatten our JSON STRING column **decoded_value** using a STRUCT column.\n",
    "\n",
    "#### Benefits and Considerations of STRUCT Columns\n",
    "\n",
    "**Benefits**\n",
    "- **Schema Enforcement** – STRUCT columns define and enforce a schema, helping maintain data integrity.\n",
    "- **Improved Performance** – STRUCTs are generally more efficient for querying and processing than plain strings.\n",
    "\n",
    "**Considerations**\n",
    "- **Schema Enforcement** – Because the schema is enforced, issues can arise if the JSON structure changes over time.\n",
    "- **Reduced Flexibility** – The data must consistently match the defined schema, leaving less room for structural variation.\n",
    "\n",
    "#### C2.1 Converting a JSON STRING to a STRUCT Column\n",
    "\n",
    "To convert a JSON-formatted STRING column to a STRUCT column, you will need to derive the schema of the JSON-formatted string and then parse each row into a STRUCT type.\n",
    "\n",
    "We can do this in two steps:\n",
    "1. Get the STRUCT type of the JSON formatted string.\n",
    "2. Apply the STRUCT to the JSON formatted string column.\n",
    "\n",
    "1. Determine the derived schema using the [`schema_of_json()`](https://docs.databricks.com/en/sql/language-manual/functions/schema_of_json.html) function, which returns the schema inferred from a JSON-formatted string.\n",
    "\n",
    "   Run the cell and view the results. Notice that the output displays the structure of the JSON string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0c42b4-7471-408f-9762-1dbaa435d888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- First, get a sample JSON string from the decoded table to determine schema\n",
    "SELECT decoded_value \n",
    "FROM kafka_events_bronze_decoded \n",
    "LIMIT 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9618957-c440-42af-8f2c-780679f36699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use `schema_of_json()` with a sample JSON string to get the schema. Then use `from_json()` to convert the JSON string column to a STRUCT.\n",
    "\n",
    "   **Note:** For this exercise, we'll use a simplified schema based on the web events structure. Copy the output from `schema_of_json` into the `from_json()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9ce8bf-50cd-4b52-9318-d0fac5f61712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get schema from a sample JSON string\n",
    "SELECT schema_of_json('{\"browser\":\"Chrome\",\"page\":\"home\",\"action\":\"view\",\"event_timestamp\":1234567890,\"location\":{\"city\":\"San Francisco\",\"country\":\"US\"},\"session_id\":\"SESS00000001\",\"customer_id\":\"CUST00000001\"}')\n",
    "AS schema;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e8eea9-ce77-464a-abf1-12a6095d90a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Use `from_json()` to parse the JSON string column into a STRUCT type and create a new table named **kafka_events_bronze_struct**.\n",
    "\n",
    "   Run the cell and view the results. Notice that the **value** column has been transformed into a nested STRUCT that includes scalar fields, nested structs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5561f073-efe0-41a0-b6fc-877791fdf24b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE kafka_events_bronze_struct AS\n",
    "SELECT \n",
    "  * EXCEPT (decoded_value),\n",
    "  from_json(\n",
    "      decoded_value,    -- JSON formatted string column\n",
    "      'STRUCT<browser: STRING, page: STRING, action: STRING, event_timestamp: BIGINT, location: STRUCT<city: STRING, country: STRING>, session_id: STRING, customer_id: STRING>'\n",
    "  ) AS value\n",
    "FROM kafka_events_bronze_decoded;\n",
    "\n",
    "-- View the new table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_struct\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36576072-41be-4315-b6be-3dfcaa5187ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.2 Extract fields, nested fields from STRUCT columns\n",
    "\n",
    "We can query the STRUCT column using `value.browser` or `value.location` in our SELECT statement.\n",
    "\n",
    "1. Using this syntax, we can obtain values from the **value** struct column. Run the cell and view the results. Notice the following:\n",
    "\n",
    "   - We obtained values from the STRUCT column for **browser** and **city** (nested field from location)\n",
    "   \n",
    "   - The STRUCT provides better performance and type safety than JSON string extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d25b5f-f1cf-4074-9b2e-2228f04d5cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  decoded_key,\n",
    "  value.browser as browser,           -- Field\n",
    "  value.page as page,                 -- Field\n",
    "  value.location.city as city,        -- Nested-field from location field\n",
    "  value.location.country as country,  -- Nested-field from location field\n",
    "  value.customer_id as customer_id    -- Field\n",
    "FROM kafka_events_bronze_struct\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc94c308-e01a-49d8-85d1-9e341a1c7076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Working with a VARIANT Column (Public Preview)\n",
    "\n",
    "#### VARIANT Column Benefits and Considerations:\n",
    "\n",
    "**BENEFITS**\n",
    "- **Open** - Fully open-sourced, no proprietary data lock-in.\n",
    "- **Flexible** - No strict schema. You can put any type of semi-structured data into VARIANT.\n",
    "- **Performant** - Improved performance over existing methods.\n",
    "\n",
    "**CONSIDERATIONS**\n",
    "- Currently in public preview as of 2025 Q2.\n",
    "- [Variant support in Delta Lake](https://docs.databricks.com/aws/en/delta/variant)\n",
    "\n",
    "**RESOURCES**:\n",
    "- [Introducing the Open Variant Data Type in Delta Lake and Apache Spark](https://www.databricks.com/blog/introducing-open-variant-data-type-delta-lake-and-apache-spark)\n",
    "- [Say goodbye to messy JSON headaches with VARIANT](https://www.youtube.com/watch?v=fWdxF7nL3YI)\n",
    "- [Variant Data Type - Making Semi-Structured Data Fast and Simple](https://www.youtube.com/watch?v=jtjOfggD4YY)\n",
    "\n",
    "**NOTE:** Variant data type will not work on Serverless Version 1.\n",
    "\n",
    "1. View the **kafka_events_bronze_decoded** table. Confirm the **decoded_value** column contains a JSON formatted string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3fb8ca-dca3-4a6b-801a-26885405ce04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_decoded\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43bfb2c7-9f64-4e59-a382-1b846eae7acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use the [`parse_json`](https://docs.databricks.com/aws/en/sql/language-manual/functions/parse_json) function to return a VARIANT value from the JSON formatted string.\n",
    "\n",
    "   Run the cell and view the results. Notice that the **json_variant_value** column is of type VARIANT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1d8243-0669-4123-bf71-209825a5a6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE kafka_events_bronze_variant AS\n",
    "SELECT\n",
    "  decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  parse_json(decoded_value) AS json_variant_value   -- Convert the decoded_value column to a variant data type\n",
    "FROM kafka_events_bronze_decoded;\n",
    "\n",
    "-- View the table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_variant\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de3c0d7-6a90-431b-9ed7-d2696e2d0897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can parse the VARIANT data type column using `:` to create your desired table.\n",
    "\n",
    "   [VARIANT type](https://docs.databricks.com/aws/en/sql/language-manual/data-types/variant-type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47762ee7-c6c3-472f-9a53-d95184151e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  json_variant_value,\n",
    "  json_variant_value:browser :: STRING,  -- Obtain the value of browser and cast to a string\n",
    "  json_variant_value:page :: STRING,\n",
    "  json_variant_value:location\n",
    "FROM kafka_events_bronze_variant\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f1b985-5a3a-4994-9ff9-f07c14528f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Creating Streaming Tables with SQL using Auto Loader\n",
    "\n",
    "In this demonstration we will create a streaming table to incrementally ingest files from a volume using Auto Loader with SQL. \n",
    "\n",
    "When you create a streaming table using the CREATE OR REFRESH STREAMING TABLE statement, the initial data refresh and population begin immediately. These operations do not consume DBSQL warehouse compute. Instead, streaming tables rely on serverless DLT for both creation and refresh. A dedicated serverless DLT pipeline is automatically created and managed by the system for each streaming table.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "- Create streaming tables in Databricks SQL for incremental data ingestion.\n",
    "- Refresh streaming tables using the REFRESH statement.\n",
    "\n",
    "### RECOMMENDATION\n",
    "\n",
    "The CREATE STREAMING TABLE SQL command is the recommended alternative to the legacy COPY INTO SQL command for incremental ingestion from cloud object storage. Databricks recommends using streaming tables to ingest data using Databricks SQL. \n",
    "\n",
    "A streaming table is a table registered to Unity Catalog with extra support for streaming or incremental data processing. A DLT pipeline is automatically created for each streaming table. You can use streaming tables for incremental data loading from Kafka and cloud object storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0df357b-950a-4a4b-9a75-7dc431558ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup for Streaming Tables\n",
    "\n",
    "**REQUIRED - SELECT YOUR SERVERLESS SQL WAREHOUSE**\n",
    "\n",
    "**NOTE: Creating streaming tables with Databricks SQL requires a SQL warehouse.**\n",
    "\n",
    "Before executing cells in this notebook, please select a **SQL WAREHOUSE** in the lab. Follow these steps:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down to select compute (it might say **Connect**).\n",
    "2. Select **More**.\n",
    "3. Then select the **SQL Warehouse** button.\n",
    "4. Select or create a SQL warehouse.\n",
    "5. Then, at the bottom of the pop-up, select **Start and attach**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688f5fa1-1262-4490-b648-de73a3a918e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create Streaming Tables for Incremental Processing\n",
    "\n",
    "1. Explore the volume `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source` and confirm it contains CSV file(s).\n",
    "\n",
    "   Use the `LIST` statement to view the files in this volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5894609-c694-4aa4-b428-d988adf94bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2071964-1577-40dc-bb13-f585ceb4549f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the query below to view the data in the CSV file(s) in your cloud storage location. Notice that it was returned in tabular format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac691256-d41c-47c0-b3cb-b532bd70e29f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "  format => 'CSV',\n",
    "  sep => '|',\n",
    "  header => true\n",
    ")\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8251ef-80e6-44f3-9aa9-15c0f509c05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create a STREAMING TABLE using Databricks SQL\n",
    "\n",
    "3. Your goal is to create an incremental pipeline that only ingests new files (instead of using traditional batch ingestion). You can achieve this by using [streaming tables in Databricks SQL](https://docs.databricks.com/aws/en/dlt/dbsql/streaming) (Auto Loader).\n",
    "\n",
    "   - The SQL code below creates a streaming table that will incrementally ingest only new data.\n",
    "   \n",
    "   - A pipeline is automatically created for each streaming table. You can use streaming tables for incremental data loading from Kafka and cloud object storage.\n",
    "\n",
    "   **NOTE:** Incremental batch ingestion automatically detects new records in the data source and ignores records that have already been ingested. This reduces the amount of data processed, making ingestion jobs faster and more efficient in their use of compute resources.\n",
    "\n",
    "   **REQUIRED: This process will take about a minute to run and set up the incremental ingestion pipeline.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65edcb47-014f-4cfe-bdcb-81f69046cd08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create streaming table\n",
    "CREATE STREAMING TABLE sql_csv_autoloader\n",
    "AS\n",
    "SELECT *\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "  format => 'CSV',\n",
    "  sep => '|',\n",
    "  header => true\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca404eb-07ee-42b2-974a-25b437633390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the cell below to view the streaming table. Confirm that the results contain the expected number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb8c92b-47ff-4490-893d-930f71546f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf644c81-fe30-4dc6-b8f8-a520097315f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Describe the STREAMING TABLE and view the results. Notice the following:\n",
    "\n",
    "- Under **Detailed Table Information**, notice the following rows:\n",
    "  - **View Text**: The query that created the table.\n",
    "  - **Type**: Specifies that it is a STREAMING TABLE.\n",
    "  - **Provider**: Indicates that it is a Delta table.\n",
    "\n",
    "- Under **Refresh Information**, you can see specific refresh details including Last Refreshed, Last Refresh Type, Latest Refresh Status, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b741db3-cd10-4013-96c9-6f33c4d878db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b55781-58d9-4144-a16e-9079e74ad3e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. The `DESCRIBE HISTORY` statement displays a detailed list of all changes, versions, and metadata associated with a Delta streaming table, including information on updates, deletions, and schema changes.\n",
    "\n",
    "   Run the cell below and view the results. Notice the following:\n",
    "\n",
    "   - In the **operation** column, you can see that a streaming table performs operations: **CREATE TABLE**, **DLT SETUP** and **STREAMING UPDATE**.\n",
    "   \n",
    "   - Scroll to the right and find the **operationMetrics** column to see the number of rows processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2080c46-706c-4755-8839-fb93144d67d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99c3473-c9af-49c6-9f7c-30cd8a0e5419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. To demonstrate incremental ingestion, manually add another file to your cloud storage location: `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source`.\n",
    "\n",
    "   **Option 1 - Using Python:**\n",
    "   - Copy a file from the staging volume to the source volume\n",
    "\n",
    "   **Option 2 - Using UI:**\n",
    "   - Click the catalog icon on the left\n",
    "   - Expand the **lakeflow_demo** catalog\n",
    "   - Expand your **lakeflow_schema** schema\n",
    "   - Expand **Volumes**\n",
    "   - Open the **autoloader_staging_files** volume\n",
    "   - Copy a file from there to the **csv_files_autoloader_source** volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549e9c78-cf18-4b31-b54c-0b983af1af2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Option 1: Copy a file from staging to source volume using Python\n",
    "def copy_files(copy_from, copy_to, n=1):\n",
    "    files = dbutils.fs.ls(copy_from)\n",
    "    for f in files[:n]:\n",
    "        dbutils.fs.cp(f.path, f\"{copy_to}/{f.name}\")\n",
    "    print(f\"Copied {min(n, len(files))} file(s) from {copy_from} to {copy_to}\")\n",
    "\n",
    "# Copy one additional file for incremental ingestion demo\n",
    "copy_files(\n",
    "    copy_from=\"/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files\",\n",
    "    copy_to=\"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "print(\"File copied. You can now refresh the streaming table to see incremental ingestion.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcaf56a0-e884-4762-835c-3ffb203d9c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Next, manually refresh the STREAMING TABLE using `REFRESH STREAMING TABLE table-name`. \n",
    "\n",
    "   - [Refresh a streaming table](https://docs.databricks.com/aws/en/dlt/dbsql/streaming#refresh-a-streaming-table) documentation\n",
    "\n",
    "   **NOTE:** You can also rerun the CREATE STREAMING TABLE cell to incrementally ingest only new files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6e190c-5e36-4e84-a26d-6e8f8dc5d314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "REFRESH STREAMING TABLE lakeflow_demo.lakeflow_schema.sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558517f0-1e43-4712-bca4-fdd3ac0c2c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Run the cell below to view the data in the **sql_csv_autoloader** table. Notice that the table now contains additional rows from the newly added file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1db85e-46a1-4e7e-a522-39d9cf8e4ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b9060e-aa04-4941-bc8f-fbf03efb777f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Describe the history of the **sql_csv_autoloader** table. Observe the following:\n",
    "\n",
    "  - Additional versions of the streaming table include **STREAMING UPDATE** operations.\n",
    "\n",
    "  - Expand the **operationMetrics** column and note the number of rows that were incrementally ingestet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eca2d85-c968-4d18-aa69-7c490e57cbf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY sql_csv_autoloader;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d44115-a983-4d52-90d5-b788e8752b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Streaming Tables Documentation](https://docs.databricks.com/aws/en/dlt/dbsql/streaming)\n",
    "- [CREATE STREAMING TABLE Syntax](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table)\n",
    "- [Using Streaming Tables in Databricks SQL](https://docs.databricks.com/aws/en/dlt/dbsql/streaming)\n",
    "- [REFRESH (MATERIALIZED VIEW or STREAMING TABLE)](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-refresh-full)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5462007867612649,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Ingestion_with_Lakeflow_Connect",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
