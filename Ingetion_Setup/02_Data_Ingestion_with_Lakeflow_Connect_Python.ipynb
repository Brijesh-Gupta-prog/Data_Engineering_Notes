{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a14dd3-2468-4ea5-9c99-b62d085d5e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion with Lakeflow Connect - Python/PySpark Edition\n",
    "\n",
    "This notebook provides Python/PySpark implementations of the data ingestion exercises covered in the SQL version.\n",
    "\n",
    "You will learn how to:\n",
    "- Use PySpark to ingest Parquet, CSV, and JSON files into Delta tables\n",
    "- Add metadata columns during ingestion using PySpark\n",
    "- Handle rescued data columns for malformed records\n",
    "- Work with JSON data and decode base64-encoded fields\n",
    "- Perform incremental data ingestion patterns\n",
    "\n",
    "---\n",
    "\n",
    "**Environment Setup:**\n",
    "- **Catalog:** `lakeflow_demo`\n",
    "- **Schema:** `lakeflow_schema`\n",
    "- **Volume:** `raw` (located at `/Volumes/lakeflow_demo/lakeflow_schema/raw/`)\n",
    "\n",
    "**Note:** Make sure to run the `00_Setup_Environment.ipynb` notebook first to create the catalog, schema, volume, and sample data files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfed0a4f-303a-4d45-888d-0bf58a969876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8214982-de8b-477c-b10d-ecf102cbed23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Set default catalog and schema\n",
    "spark.sql(\"USE CATALOG lakeflow_demo\")\n",
    "spark.sql(\"USE SCHEMA lakeflow_schema\")\n",
    "\n",
    "# View current catalog and schema\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Current Catalog: {current_catalog}\")\n",
    "print(f\"Current Schema: {current_schema}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13cc779-350a-4732-bd3c-e84a4d0937b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Data Ingestion with PySpark - Parquet Files\n",
    "\n",
    "In this demonstration, we'll explore ingesting data from cloud storage into Delta tables using PySpark.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "- Use PySpark to read Parquet files and create Delta tables.\n",
    "- Use PySpark DataFrame operations to perform incremental data loads.\n",
    "- Understand the differences between SQL and PySpark approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef320043-e796-4a79-ba7c-8d457ba78464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Data Source Files\n",
    "\n",
    "1. Let's first explore the Parquet files stored in the volume `/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/` using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a13879d-2063-4fc5-a0f3-e71fada08c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List files in the volume\n",
    "files = dbutils.fs.ls('/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/')\n",
    "print(\"Files in the volume:\")\n",
    "for file in files:\n",
    "    if file.name.startswith('part-'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eef8d62-3296-4824-927a-d0b08ad0f129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Read and preview the Parquet files using PySpark to view the raw data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee06ec89-a513-4681-8a99-ab13a1ac454a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read Parquet files using PySpark\n",
    "users_df = spark.read.format(\"parquet\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\")\n",
    "\n",
    "# Display schema and preview data\n",
    "print(\"Schema:\")\n",
    "users_df.printSchema()\n",
    "\n",
    "print(\"\\nPreview (first 10 rows):\")\n",
    "users_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67dbf859-a6d3-4ef1-9ca5-873c54bc62d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Batch Data Ingestion with PySpark\n",
    "\n",
    "### C1. Creating Delta Tables with PySpark\n",
    "\n",
    "1. Create a Delta table from the Parquet files using PySpark. This is equivalent to the SQL `CREATE TABLE AS SELECT` statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5edc3c51-4b29-42d2-86cf-b28515fb5107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS historical_users_bronze_pyspark\")\n",
    "\n",
    "# Read Parquet files\n",
    "users_df = spark.read.format(\"parquet\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\")\n",
    "\n",
    "# Write to Delta table (equivalent to CREATE TABLE AS SELECT)\n",
    "users_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"historical_users_bronze_pyspark\")\n",
    "\n",
    "# Preview the Delta table\n",
    "print(\"Table created successfully!\")\n",
    "spark.table(\"historical_users_bronze_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf2d73c-8abb-4d74-b065-9abba2f2f6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Describe the table to view metadata information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6808a46f-23fd-4a18-be85-c5fd04b5db3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Describe table extended to view metadata\n",
    "table_info = spark.sql(\"DESCRIBE TABLE EXTENDED historical_users_bronze_pyspark\")\n",
    "table_info.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95b5e1b-28dd-43f9-8a01-3e93a9e2f992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Incremental Data Ingestion with PySpark\n",
    "\n",
    "### D1. Incremental Load Pattern\n",
    "\n",
    "In PySpark, we can implement incremental loading by:\n",
    "1. Reading new files from the source\n",
    "2. Appending to an existing Delta table\n",
    "3. Using merge operations for upserts\n",
    "\n",
    "1. Let's demonstrate an incremental append operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a8023f-1642-4d21-8f83-02dab5f8fe5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Create a table for incremental loading\n",
    "spark.sql(\"DROP TABLE IF EXISTS historical_users_bronze_incremental\")\n",
    "\n",
    "# First load - create the table\n",
    "users_df = spark.read.format(\"parquet\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\")\n",
    "users_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"historical_users_bronze_incremental\")\n",
    "\n",
    "print(\"Initial load complete. Row count:\", spark.table(\"historical_users_bronze_incremental\").count())\n",
    "\n",
    "# Simulate incremental load - append mode\n",
    "# In a real scenario, you would read only new files\n",
    "users_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"historical_users_bronze_incremental\")\n",
    "\n",
    "print(\"After incremental append. Row count:\", spark.table(\"historical_users_bronze_incremental\").count())\n",
    "print(\"Note: In production, you would filter for only new files to avoid duplicates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d7373d-e088-4706-bbfb-14b8c21e2afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Adding Metadata Columns During Ingestion - Python\n",
    "\n",
    "In this demonstration, we'll explore how to add metadata columns during data ingestion using PySpark.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "- Add metadata columns using PySpark DataFrame operations.\n",
    "- Convert Unix timestamps to readable dates.\n",
    "- Use PySpark functions to capture file-level metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab9140a-d294-4976-bd61-37e548972037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Read Data and Add Metadata Columns\n",
    "\n",
    "1. Read the Parquet files and add metadata columns using PySpark functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3621199e-1426-4da7-a9e3-155bb1ebb576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, from_unixtime, current_timestamp, input_file_name\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Read Parquet files\n",
    "users_df = spark.read.format(\"parquet\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\")\n",
    "\n",
    "# Add metadata columns\n",
    "users_with_metadata = (\n",
    "    users_df\n",
    "    .withColumn(\"first_touch_date\", from_unixtime(col(\"user_first_touch_timestamp\") / 1_000_000).cast(DateType()))\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"ingestion_time\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"Data with metadata columns:\")\n",
    "users_with_metadata.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdadd247-2355-43ff-9693-08a6f4c85649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Note: In PySpark, `_metadata` is available when using `read_files()` SQL function. For native PySpark, we use `input_file_name()` for file names. For file modification time, we can use Spark SQL functions or read metadata separately.\n",
    "\n",
    "Let's create the final bronze table with all metadata columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f501f8cd-4a38-40b6-856b-efa4c9c1691e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, from_unixtime, current_timestamp, input_file_name\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS historical_users_bronze_pyspark_metadata\")\n",
    "\n",
    "# Read Parquet files\n",
    "users_df = spark.read.format(\"parquet\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/\")\n",
    "\n",
    "# Add metadata columns\n",
    "users_with_metadata = (\n",
    "    users_df\n",
    "    .withColumn(\"first_touch_date\", from_unixtime(col(\"user_first_touch_timestamp\") / 1_000_000).cast(DateType()))\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"ingestion_time\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Write to Delta table\n",
    "users_with_metadata.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"historical_users_bronze_pyspark_metadata\")\n",
    "\n",
    "# View the final bronze table\n",
    "print(\"Final bronze table with metadata:\")\n",
    "spark.table(\"historical_users_bronze_pyspark_metadata\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3543897c-ae8c-4aa5-9297-f0d2bf8a294c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Alternative: Using `read_files()` SQL function from PySpark to access `_metadata` column directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dc1cadc-2b0f-4b4c-81bf-a7b0ff4d8b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, from_unixtime, current_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Using read_files() SQL function to access _metadata\n",
    "users_with_metadata_sql = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      *,\n",
    "      cast(from_unixtime(user_first_touch_timestamp / 1000000) AS DATE) AS first_touch_date,\n",
    "      _metadata.file_modification_time AS file_modification_time,\n",
    "      _metadata.file_name AS source_file,\n",
    "      current_timestamp() as ingestion_time\n",
    "    FROM read_files(\n",
    "      '/Volumes/lakeflow_demo/lakeflow_schema/raw/users-historical/',\n",
    "      format => 'parquet'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "print(\"Using read_files() with _metadata:\")\n",
    "users_with_metadata_sql.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "488a9c11-7112-48fc-980c-d52cf5eea06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Handling CSV Ingestion with the Rescued Data Column - Python\n",
    "\n",
    "In this demonstration, we'll focus on ingesting CSV files into Delta Lake using PySpark and exploring the rescued data column.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Ingest CSV files as Delta tables using PySpark.\n",
    "- Define and apply explicit schemas with PySpark.\n",
    "- Handle and inspect rescued data that does not conform to the defined schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41ea9329-f678-4959-b6e0-b8cece767e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Inspect the Dataset\n",
    "\n",
    "1. Let's first inspect the CSV file with malformed data using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cade24f-fac8-422c-b623-d7f882354af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read CSV file as text to inspect raw content\n",
    "text_df = spark.read.text(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv\")\n",
    "print(\"Raw CSV content:\")\n",
    "text_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "549a8d33-e4c3-48c9-8cd4-3f76434650bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Ingesting and Rescuing Malformed Data with PySpark\n",
    "\n",
    "1. Using `read_files()` SQL function from PySpark to read CSV with rescued data column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f1f609-9b90-4e9d-8d31-29d8defeb720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Using read_files() SQL function to read CSV with rescued data\n",
    "products_df = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_files(\n",
    "        '/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv',\n",
    "        format => 'csv',\n",
    "        sep => ',',\n",
    "        header => true,\n",
    "        schema => 'item_id STRING, name STRING, price DOUBLE',\n",
    "        rescuedDataColumn => '_rescued_data'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"CSV data with rescued data column:\")\n",
    "products_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3dc9a1-3ee7-4408-af0d-3f35119b4f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Alternative: Using native PySpark to read CSV with schema and handle errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f6c64e-bd79-4105-b527-dea38c56380d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV with schema and mode for handling malformed records\n",
    "# Note: PySpark's native CSV reader doesn't have rescued_data column\n",
    "# We need to use read_files() SQL function for that feature\n",
    "try:\n",
    "    products_df_native = spark.read.format(\"csv\") \\\n",
    "        .option(\"sep\", \",\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"PERMISSIVE\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv\")\n",
    "    \n",
    "    print(\"Native PySpark read (malformed rows will have null values):\")\n",
    "    products_df_native.display()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Note: For rescued_data column, use read_files() SQL function instead\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7426d3a-f2a1-44f3-b752-7b1580b1a80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Add Additional Metadata Columns During Ingestion\n",
    "\n",
    "1. Create the final bronze table with metadata columns using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5eadac-6fb8-4b9d-91ff-34c410bb62ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS products_bronze_pyspark\")\n",
    "\n",
    "# Read CSV with rescued data using read_files() SQL function\n",
    "products_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      *,\n",
    "      _metadata.file_modification_time AS file_modification_time,\n",
    "      _metadata.file_name AS source_file,\n",
    "      current_timestamp() as ingestion_time\n",
    "    FROM read_files(\n",
    "        '/Volumes/lakeflow_demo/lakeflow_schema/raw/products-csv/lab_malformed_data.csv',\n",
    "        format => 'csv',\n",
    "        sep => ',',\n",
    "        header => true,\n",
    "        schema => 'item_id STRING, name STRING, price DOUBLE',\n",
    "        rescuedDataColumn => '_rescued_data'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Write to Delta table\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products_bronze_pyspark\")\n",
    "\n",
    "# View the final table\n",
    "print(\"Final products bronze table:\")\n",
    "spark.table(\"products_bronze_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c2c646e-7acb-4878-91ae-b7bccebef553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Ingesting JSON Files with PySpark\n",
    "\n",
    "In this demonstration, we'll explore how to ingest JSON files and perform transformations using PySpark, including decoding encoded fields and flattening nested JSON strings.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Ingest raw JSON data into Unity Catalog using PySpark.\n",
    "- Apply techniques to flatten JSON string columns.\n",
    "- Decode base64-encoded fields using PySpark functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e98f1f-ce7d-4f6e-8cd6-21cf0676cf93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Overview of JSON Ingestion with PySpark\n",
    "\n",
    "### B1. Inspect JSON files\n",
    "\n",
    "1. List and preview the JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d37dbc5-6c05-4b49-8076-1dd3c5abb01a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List files in the volume\n",
    "files = dbutils.fs.ls('/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/')\n",
    "print(\"JSON files in the volume:\")\n",
    "for file in files:\n",
    "    if file.name.startswith('part-'):\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd762ed-c6b6-47ca-9f58-67ff11840bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Read JSON files using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f74330c-1e95-421c-940b-5a88fa2cc74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read JSON files using PySpark\n",
    "kafka_events_df = spark.read.format(\"json\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/\")\n",
    "\n",
    "print(\"Schema:\")\n",
    "kafka_events_df.printSchema()\n",
    "\n",
    "print(\"\\nPreview (first 5 rows):\")\n",
    "kafka_events_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2608a82-0e3c-4aab-a36b-1a36dce99089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Create Bronze Table with Raw JSON Data\n",
    "\n",
    "1. Store the raw JSON data in a Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50bada12-dabf-43dc-962c-09d9c3404ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS kafka_events_bronze_raw_pyspark\")\n",
    "\n",
    "# Read JSON files\n",
    "kafka_events_df = spark.read.format(\"json\").load(\"/Volumes/lakeflow_demo/lakeflow_schema/raw/events-kafka/\")\n",
    "\n",
    "# Write to Delta table\n",
    "kafka_events_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kafka_events_bronze_raw_pyspark\")\n",
    "\n",
    "# Display the table\n",
    "print(\"Raw Kafka events table:\")\n",
    "spark.table(\"kafka_events_bronze_raw_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6492e0-5c3f-44ca-97e4-342fffbfd3b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Decoding base64 Strings with PySpark\n",
    "\n",
    "1. Decode the base64-encoded key and value columns using PySpark functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92bfc009-bf85-4772-9701-bbd75fa1c640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, unbase64\n",
    "\n",
    "# Read from the bronze table\n",
    "kafka_events_df = spark.table(\"kafka_events_bronze_raw_pyspark\")\n",
    "\n",
    "# Decode base64 columns\n",
    "decoded_df = kafka_events_df.select(\n",
    "    col(\"key\").alias(\"encoded_key\"),\n",
    "    unbase64(col(\"key\")).alias(\"decoded_key_binary\"),\n",
    "    col(\"value\").alias(\"encoded_value\"),\n",
    "    unbase64(col(\"value\")).alias(\"decoded_value_binary\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"partition\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"topic\")\n",
    ")\n",
    "\n",
    "print(\"Decoded columns (as BINARY):\")\n",
    "decoded_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153d132c-3973-409b-a38d-b6a1dfe40757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Convert BINARY columns to STRING columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bf7c1e-a84a-46a2-be82-ef5f07f01d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, unbase64\n",
    "\n",
    "# Read from the bronze table\n",
    "kafka_events_df = spark.table(\"kafka_events_bronze_raw_pyspark\")\n",
    "\n",
    "# Decode and cast to STRING\n",
    "decoded_df = kafka_events_df.select(\n",
    "    col(\"key\").alias(\"encoded_key\"),\n",
    "    unbase64(col(\"key\")).cast(\"string\").alias(\"decoded_key\"),\n",
    "    col(\"value\").alias(\"encoded_value\"),\n",
    "    unbase64(col(\"value\")).cast(\"string\").alias(\"decoded_value\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"partition\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"topic\")\n",
    ")\n",
    "\n",
    "print(\"Decoded columns (as STRING):\")\n",
    "decoded_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c3890b3-49d0-4f32-9074-382ea259d59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Create a decoded bronze table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbe596b-79be-4cec-b4ad-7ba890fbb00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, unbase64\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS kafka_events_bronze_decoded_pyspark\")\n",
    "\n",
    "# Read from the bronze table\n",
    "kafka_events_df = spark.table(\"kafka_events_bronze_raw_pyspark\")\n",
    "\n",
    "# Decode and cast to STRING\n",
    "decoded_df = kafka_events_df.select(\n",
    "    unbase64(col(\"key\")).cast(\"string\").alias(\"decoded_key\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"partition\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"topic\"),\n",
    "    unbase64(col(\"value\")).cast(\"string\").alias(\"decoded_value\")\n",
    ")\n",
    "\n",
    "# Write to Delta table\n",
    "decoded_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kafka_events_bronze_decoded_pyspark\")\n",
    "\n",
    "# View the new table\n",
    "print(\"Decoded Kafka events table:\")\n",
    "spark.table(\"kafka_events_bronze_decoded_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "114236e1-472c-4d79-bbc7-31033e52578b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Working with JSON Formatted Strings in PySpark\n",
    "\n",
    "### C1. Flattening JSON String Columns\n",
    "\n",
    "1. Extract fields from JSON-formatted strings using PySpark's JSON functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93bd5ccf-ecd7-4d97-8e7c-fca8a5785a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, get_json_object\n",
    "\n",
    "# Read from decoded table\n",
    "decoded_df = spark.table(\"kafka_events_bronze_decoded_pyspark\")\n",
    "\n",
    "# Extract fields from JSON string using get_json_object\n",
    "flattened_df = decoded_df.select(\n",
    "    col(\"decoded_value\"),\n",
    "    get_json_object(col(\"decoded_value\"), \"$.device\").alias(\"device\"),\n",
    "    get_json_object(col(\"decoded_value\"), \"$.traffic_source\").alias(\"traffic_source\"),\n",
    "    get_json_object(col(\"decoded_value\"), \"$.geo\").alias(\"geo\"),\n",
    "    get_json_object(col(\"decoded_value\"), \"$.items\").alias(\"items\"),\n",
    "    col(\"decoded_key\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"partition\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"topic\")\n",
    ")\n",
    "\n",
    "print(\"Flattened JSON fields:\")\n",
    "flattened_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e23c9013-36cf-4845-818b-b9bd8197c1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Alternative: Using SQL-style JSON path extraction (column:field syntax) via Spark SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dade630-220b-49ca-82d4-342147562635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Using SQL-style JSON path extraction\n",
    "flattened_sql_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      decoded_value,\n",
    "      decoded_value:device,\n",
    "      decoded_value:traffic_source,\n",
    "      decoded_value:geo,\n",
    "      decoded_value:items,\n",
    "      decoded_key,\n",
    "      offset,\n",
    "      partition,\n",
    "      timestamp,\n",
    "      topic\n",
    "    FROM kafka_events_bronze_decoded_pyspark\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"Using SQL-style JSON path extraction:\")\n",
    "flattened_sql_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26fb8271-1db0-4d67-b5bb-e62e272d1d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Create a flattened bronze table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd7c4caf-e7af-4dd2-8f68-e861d38a998c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS kafka_events_bronze_string_flattened_pyspark\")\n",
    "\n",
    "# Create flattened table using SQL\n",
    "flattened_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      decoded_key,\n",
    "      offset,\n",
    "      partition,\n",
    "      timestamp,\n",
    "      topic,\n",
    "      decoded_value:device,\n",
    "      decoded_value:traffic_source,\n",
    "      decoded_value:geo,\n",
    "      decoded_value:items\n",
    "    FROM kafka_events_bronze_decoded_pyspark\n",
    "\"\"\")\n",
    "\n",
    "# Write to Delta table\n",
    "flattened_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kafka_events_bronze_string_flattened_pyspark\")\n",
    "\n",
    "# Display the table\n",
    "print(\"Flattened Kafka events table:\")\n",
    "spark.table(\"kafka_events_bronze_string_flattened_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cb3b631-2e12-4604-8ae7-83cd291085e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Flattening JSON Formatting Strings via STRUCT Conversion - Python\n",
    "\n",
    "Similar to the previous section, we will discuss how to flatten our JSON STRING column **decoded_value** using a STRUCT column.\n",
    "\n",
    "#### Benefits and Considerations of STRUCT Columns\n",
    "\n",
    "**Benefits**\n",
    "- **Schema Enforcement** – STRUCT columns define and enforce a schema, helping maintain data integrity.\n",
    "- **Improved Performance** – STRUCTs are generally more efficient for querying and processing than plain strings.\n",
    "\n",
    "**Considerations**\n",
    "- **Schema Enforcement** – Because the schema is enforced, issues can arise if the JSON structure changes over time.\n",
    "- **Reduced Flexibility** – The data must consistently match the defined schema, leaving less room for structural variation.\n",
    "\n",
    "#### C2.1 Converting a JSON STRING to a STRUCT Column\n",
    "\n",
    "To convert a JSON-formatted STRING column to a STRUCT column, you can use PySpark's `from_json()` function with a defined schema.\n",
    "\n",
    "1. First, get a sample JSON string to determine the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e900245-1bef-4f67-acea-d4e05e5c49ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Get a sample JSON string from the decoded table\n",
    "sample_json = spark.table(\"kafka_events_bronze_decoded_pyspark\").select(\"decoded_value\").first()[0]\n",
    "print(\"Sample JSON string:\")\n",
    "print(sample_json[:200] + \"...\" if len(sample_json) > 200 else sample_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5977ea-2a2f-4844-888d-36355f0a0e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use `from_json()` to convert the JSON string column to a STRUCT. You can define the schema manually or use `schema_of_json()` to infer it.\n",
    "\n",
    "   **Note:** For this exercise, we'll use a simplified schema. In practice, you can use `schema_of_json()` to get the schema from a sample JSON string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45801805-3359-4cc5-868b-921f3a7498b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, MapType\n",
    "\n",
    "# Define schema for the JSON structure\n",
    "json_schema = StructType([\n",
    "    StructField(\"device\", StringType(), True),\n",
    "    StructField(\"page\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"event_timestamp\", LongType(), True),\n",
    "    StructField(\"location\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Drop table if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS kafka_events_bronze_struct_pyspark\")\n",
    "\n",
    "# Read the decoded table\n",
    "decoded_df = spark.table(\"kafka_events_bronze_decoded_pyspark\")\n",
    "\n",
    "# Convert JSON string to STRUCT\n",
    "struct_df = decoded_df.select(\n",
    "    col(\"decoded_key\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"partition\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"topic\"),\n",
    "    from_json(col(\"decoded_value\"), json_schema).alias(\"value\")\n",
    ")\n",
    "\n",
    "# Write to Delta table\n",
    "struct_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kafka_events_bronze_struct_pyspark\")\n",
    "\n",
    "# Display the table\n",
    "print(\"Kafka events with STRUCT column:\")\n",
    "spark.table(\"kafka_events_bronze_struct_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8fa694f-8c30-41f7-86ec-dfb4b826876d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.2 Extract fields, nested fields from STRUCT columns\n",
    "\n",
    "We can query the STRUCT column using `value.browser` or `value.location` in our SELECT statement.\n",
    "\n",
    "1. Using this syntax, we can obtain values from the **value** struct column. Notice the following:\n",
    "\n",
    "   - We obtained values from the STRUCT column for **device** and **city** (nested field from location)\n",
    "   \n",
    "   - The STRUCT provides better performance and type safety than JSON string extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e750e15-a185-40f7-bb6e-a0c4140cf89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract fields from STRUCT column\n",
    "struct_df = spark.table(\"kafka_events_bronze_struct_pyspark\")\n",
    "\n",
    "extracted_df = struct_df.select(\n",
    "    col(\"decoded_key\"),\n",
    "    col(\"value.device\").alias(\"device\"),           # Field\n",
    "    col(\"value.page\").alias(\"page\"),               # Field\n",
    "    col(\"value.location\").alias(\"location\"),       # Nested struct\n",
    "    col(\"value.customer_id\").alias(\"customer_id\")  # Field\n",
    ")\n",
    "\n",
    "print(\"Extracted fields from STRUCT:\")\n",
    "extracted_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd8a75f2-6faa-491d-a740-db465edf54f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Working with a VARIANT Column (Public Preview) - Python\n",
    "\n",
    "#### VARIANT Column Benefits and Considerations:\n",
    "\n",
    "**BENEFITS**\n",
    "- **Open** - Fully open-sourced, no proprietary data lock-in.\n",
    "- **Flexible** - No strict schema. You can put any type of semi-structured data into VARIANT.\n",
    "- **Performant** - Improved performance over existing methods.\n",
    "\n",
    "**CONSIDERATIONS**\n",
    "- Currently in public preview as of 2025 Q2.\n",
    "- [Variant support in Delta Lake](https://docs.databricks.com/aws/en/delta/variant)\n",
    "\n",
    "**RESOURCES**:\n",
    "- [Introducing the Open Variant Data Type in Delta Lake and Apache Spark](https://www.databricks.com/blog/introducing-open-variant-data-type-delta-lake-and-apache-spark)\n",
    "- [Say goodbye to messy JSON headaches with VARIANT](https://www.youtube.com/watch?v=fWdxF7nL3YI)\n",
    "- [Variant Data Type - Making Semi-Structured Data Fast and Simple](https://www.youtube.com/watch?v=jtjOfggD4YY)\n",
    "\n",
    "**NOTE:** Variant data type will not work on Serverless Version 1.\n",
    "\n",
    "1. View the **kafka_events_bronze_decoded_pyspark** table. Confirm the **decoded_value** column contains a JSON formatted string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d6fdced-1cdd-41ce-886f-3773ccb7908b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# View the decoded table\n",
    "print(\"Kafka events bronze decoded table:\")\n",
    "spark.table(\"kafka_events_bronze_decoded_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0a1a36-8ef1-418e-b143-8ffbc0d766e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use the `parse_json()` function to return a VARIANT value from the JSON formatted string.\n",
    "\n",
    "   Run the cell and view the results. Notice that the **json_variant_value** column is of type VARIANT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c7cb2f6-e8a6-4e92-b36e-96be08affa0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Use parse_json SQL function to convert JSON string to VARIANT\n",
    "spark.sql(\"DROP TABLE IF EXISTS kafka_events_bronze_variant_pyspark\")\n",
    "\n",
    "variant_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      decoded_key,\n",
    "      offset,\n",
    "      partition,\n",
    "      timestamp,\n",
    "      topic,\n",
    "      parse_json(decoded_value) AS json_variant_value\n",
    "    FROM kafka_events_bronze_decoded_pyspark\n",
    "\"\"\")\n",
    "\n",
    "# Write to Delta table\n",
    "variant_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"kafka_events_bronze_variant_pyspark\")\n",
    "\n",
    "# Display the table\n",
    "print(\"Kafka events with VARIANT column:\")\n",
    "spark.table(\"kafka_events_bronze_variant_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adeee331-2e6b-40e9-baf6-76216873f5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can parse the VARIANT data type column using `:` to create your desired table.\n",
    "\n",
    "   [VARIANT type](https://docs.databricks.com/aws/en/sql/language-manual/data-types/variant-type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6b21978-4727-43d4-8d97-27c764f19144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Parse VARIANT column using SQL-style path extraction\n",
    "variant_parsed = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      json_variant_value,\n",
    "      json_variant_value:browser :: STRING AS browser,  -- Obtain the value of browser and cast to a string\n",
    "      json_variant_value:page :: STRING AS page,\n",
    "      json_variant_value:location AS location\n",
    "    FROM kafka_events_bronze_variant_pyspark\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Parsed VARIANT column:\")\n",
    "variant_parsed.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "821028bd-b851-4534-bf52-24b7a641a04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Creating Streaming Tables with SQL using Auto Loader - Python\n",
    "\n",
    "In this demonstration we will create a streaming table to incrementally ingest files from a volume using Auto Loader with SQL. \n",
    "\n",
    "When you create a streaming table using the CREATE OR REFRESH STREAMING TABLE statement, the initial data refresh and population begin immediately. These operations do not consume DBSQL warehouse compute. Instead, streaming tables rely on serverless DLT for both creation and refresh. A dedicated serverless DLT pipeline is automatically created and managed by the system for each streaming table.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "- Create streaming tables in Databricks SQL for incremental data ingestion.\n",
    "- Refresh streaming tables using the REFRESH statement.\n",
    "\n",
    "### RECOMMENDATION\n",
    "\n",
    "The CREATE STREAMING TABLE SQL command is the recommended alternative to the legacy COPY INTO SQL command for incremental ingestion from cloud object storage. Databricks recommends using streaming tables to ingest data using Databricks SQL. \n",
    "\n",
    "A streaming table is a table registered to Unity Catalog with extra support for streaming or incremental data processing. A DLT pipeline is automatically created for each streaming table. You can use streaming tables for incremental data loading from Kafka and cloud object storage.\n",
    "\n",
    "**NOTE:** Streaming tables are created using SQL syntax, but we can execute them from Python using `spark.sql()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e14ef8-f3f8-4bec-9d06-49dd45c14d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup for Streaming Tables\n",
    "\n",
    "**REQUIRED - SELECT YOUR SERVERLESS SQL WAREHOUSE**\n",
    "\n",
    "**NOTE: Creating streaming tables with Databricks SQL requires a SQL warehouse.**\n",
    "\n",
    "Before executing cells in this notebook, please select a **SQL WAREHOUSE** in the lab. Follow these steps:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down to select compute (it might say **Connect**).\n",
    "2. Select **More**.\n",
    "3. Then select the **SQL Warehouse** button.\n",
    "4. Select or create a SQL warehouse.\n",
    "5. Then, at the bottom of the pop-up, select **Start and attach**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "216b2396-d7af-443e-8d54-cd3795f44d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create Streaming Tables for Incremental Processing\n",
    "\n",
    "1. Explore the volume `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source` and confirm it contains CSV file(s).\n",
    "\n",
    "   Use Python to list the files in this volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8392828f-c439-4593-9bdf-48d7781ad8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List files in the autoloader source volume\n",
    "files = dbutils.fs.ls(\"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source\")\n",
    "print(\"Files in csv_files_autoloader_source volume:\")\n",
    "for file in files:\n",
    "    print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f1dcc1-eb86-45da-9d00-87223a45f740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the query below to view the data in the CSV file(s) in your cloud storage location. Notice that it was returned in tabular format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aacd853-4bd3-4956-900f-24cfcb67a9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# View data in CSV files\n",
    "csv_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_files(\n",
    "      '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "      format => 'CSV',\n",
    "      sep => '|',\n",
    "      header => true\n",
    "    )\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sample data from CSV files:\")\n",
    "csv_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb6690a-cf37-4991-befa-9407fc7f1a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create a STREAMING TABLE using Databricks SQL\n",
    "\n",
    "3. Your goal is to create an incremental pipeline that only ingests new files (instead of using traditional batch ingestion). You can achieve this by using [streaming tables in Databricks SQL](https://docs.databricks.com/aws/en/dlt/dbsql/streaming) (Auto Loader).\n",
    "\n",
    "   - The SQL code below creates a streaming table that will incrementally ingest only new data.\n",
    "   \n",
    "   - A pipeline is automatically created for each streaming table. You can use streaming tables for incremental data loading from Kafka and cloud object storage.\n",
    "\n",
    "   **NOTE:** Incremental batch ingestion automatically detects new records in the data source and ignores records that have already been ingested. This reduces the amount of data processed, making ingestion jobs faster and more efficient in their use of compute resources.\n",
    "\n",
    "   **REQUIRED: This process will take about a minute to run and set up the incremental ingestion pipeline.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbd63b62-e1f9-4fac-8e0b-59f981f47f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Create streaming table using SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE STREAMING TABLE sql_csv_autoloader_pyspark\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM STREAM read_files(\n",
    "      '/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source',\n",
    "      format => 'CSV',\n",
    "      sep => '|',\n",
    "      header => true\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Streaming table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a365a05-a160-4cf7-a6ca-dac5192017ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the cell below to view the streaming table. Confirm that the results contain the expected number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa2b1a3-2ec5-42b5-b1b3-60fef512af56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# View the streaming table\n",
    "print(\"Streaming table data:\")\n",
    "spark.table(\"sql_csv_autoloader_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cbac56f-0333-4362-bc5f-e1a1eb18c29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Describe the STREAMING TABLE and view the results. Notice the following:\n",
    "\n",
    "- Under **Detailed Table Information**, notice the following rows:\n",
    "  - **View Text**: The query that created the table.\n",
    "  - **Type**: Specifies that it is a STREAMING TABLE.\n",
    "  - **Provider**: Indicates that it is a Delta table.\n",
    "\n",
    "- Under **Refresh Information**, you can see specific refresh details including Last Refreshed, Last Refresh Type, Latest Refresh Status, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6761d627-550f-47f2-9edd-e40dcf0ef9a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Describe the streaming table\n",
    "describe_df = spark.sql(\"DESCRIBE TABLE EXTENDED sql_csv_autoloader_pyspark\")\n",
    "print(\"Table description:\")\n",
    "describe_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684afc82-af0c-4fba-9a2c-06deafd74f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. The `DESCRIBE HISTORY` statement displays a detailed list of all changes, versions, and metadata associated with a Delta streaming table, including information on updates, deletions, and schema changes.\n",
    "\n",
    "   Run the cell below and view the results. Notice the following:\n",
    "\n",
    "   - In the **operation** column, you can see that a streaming table performs operations: **CREATE TABLE**, **DLT SETUP** and **STREAMING UPDATE**.\n",
    "   \n",
    "   - Scroll to the right and find the **operationMetrics** column to see the number of rows processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdd471f5-fc16-417d-b069-4a5ae3aaaf99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Describe history of the streaming table\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY sql_csv_autoloader_pyspark\")\n",
    "print(\"Table history:\")\n",
    "history_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c91f8fa-9d6d-4af4-b1db-dfdd460a35e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. To demonstrate incremental ingestion, manually add another file to your cloud storage location: `/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source`.\n",
    "\n",
    "   **Option 1 - Using Python:**\n",
    "   - Copy a file from the staging volume to the source volume\n",
    "\n",
    "   **Option 2 - Using UI:**\n",
    "   - Click the catalog icon on the left\n",
    "   - Expand the **lakeflow_demo** catalog\n",
    "   - Expand your **lakeflow_schema** schema\n",
    "   - Expand **Volumes**\n",
    "   - Open the **autoloader_staging_files** volume\n",
    "   - Copy a file from there to the **csv_files_autoloader_source** volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5309bea3-5fea-4cf9-b829-19c057f44191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Option 1: Copy a file from staging to source volume using Python\n",
    "def copy_files(copy_from, copy_to, n=1):\n",
    "    files = dbutils.fs.ls(copy_from)\n",
    "    for f in files[:n]:\n",
    "        dbutils.fs.cp(f.path, f\"{copy_to}/{f.name}\")\n",
    "    print(f\"Copied {min(n, len(files))} file(s) from {copy_from} to {copy_to}\")\n",
    "\n",
    "# Copy one additional file for incremental ingestion demo\n",
    "copy_files(\n",
    "    copy_from=\"/Volumes/lakeflow_demo/lakeflow_schema/autoloader_staging_files\",\n",
    "    copy_to=\"/Volumes/lakeflow_demo/lakeflow_schema/csv_files_autoloader_source\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "print(\"File copied. You can now refresh the streaming table to see incremental ingestion.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531c3079-43b3-4c5c-9bba-2bf3e929ac8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Next, manually refresh the STREAMING TABLE using `REFRESH STREAMING TABLE table-name`. \n",
    "\n",
    "   - [Refresh a streaming table](https://docs.databricks.com/aws/en/dlt/dbsql/streaming#refresh-a-streaming-table) documentation\n",
    "\n",
    "   **NOTE:** You can also rerun the CREATE STREAMING TABLE cell to incrementally ingest only new files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a4ad37-9713-4030-945c-5814ad908560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Refresh the streaming table\n",
    "spark.sql(\"REFRESH STREAMING TABLE lakeflow_demo.lakeflow_schema.sql_csv_autoloader_pyspark\")\n",
    "print(\"Streaming table refreshed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498ea35d-c6d8-473e-a392-005812e4af9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Run the cell below to view the data in the **sql_csv_autoloader_pyspark** table. Notice that the table now contains additional rows from the newly added file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d97e60d3-8a8e-4e23-bc90-b66089f53d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# View the streaming table after refresh\n",
    "print(\"Streaming table data after incremental ingestion:\")\n",
    "spark.table(\"sql_csv_autoloader_pyspark\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd45170-a72e-40fe-975e-f255fa98a31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Describe the history of the **sql_csv_autoloader_pyspark** table. Observe the following:\n",
    "\n",
    "  - Additional versions of the streaming table include **STREAMING UPDATE** operations.\n",
    "\n",
    "  - Expand the **operationMetrics** column and note the number of rows that were incrementally ingested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e707f0fd-a77c-4c60-bd6a-e4d9371cffff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Describe history after incremental ingestion\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY sql_csv_autoloader_pyspark\")\n",
    "print(\"Table history after incremental ingestion:\")\n",
    "history_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ae5003-ece8-4b8e-8e3e-d8c4e08d85fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Streaming Tables Documentation](https://docs.databricks.com/aws/en/dlt/dbsql/streaming)\n",
    "- [CREATE STREAMING TABLE Syntax](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-streaming-table)\n",
    "- [Using Streaming Tables in Databricks SQL](https://docs.databricks.com/aws/en/dlt/dbsql/streaming)\n",
    "- [REFRESH (MATERIALIZED VIEW or STREAMING TABLE)](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-refresh-full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070748b5-c0df-4123-b576-6fb3a90f9e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated Python/PySpark equivalents of the SQL-based data ingestion exercises:\n",
    "\n",
    "1. **Parquet Ingestion**: Using `spark.read.format(\"parquet\")` and `df.write.saveAsTable()`\n",
    "2. **Metadata Columns**: Using PySpark functions like `input_file_name()`, `current_timestamp()`, and `from_unixtime()`\n",
    "3. **CSV with Rescued Data**: Using `read_files()` SQL function from PySpark to access `_rescued_data` column\n",
    "4. **JSON Ingestion**: Using `spark.read.format(\"json\")` and JSON parsing functions\n",
    "5. **Base64 Decoding**: Using `unbase64()` function in PySpark\n",
    "6. **JSON Flattening**: Using `get_json_object()` or SQL-style path extraction\n",
    "7. **STRUCT Conversion**: Using `from_json()` with defined schemas to convert JSON strings to STRUCT columns\n",
    "8. **VARIANT Columns**: Using `parse_json()` to convert JSON strings to VARIANT data type\n",
    "9. **Streaming Tables**: Creating streaming tables using SQL executed from Python for incremental data ingestion\n",
    "\n",
    "### Key Differences: SQL vs PySpark\n",
    "\n",
    "- **SQL**: Direct use of `read_files()` with `_metadata` column\n",
    "- **PySpark**: Use `input_file_name()` for file names, or call `read_files()` via `spark.sql()`\n",
    "- **SQL**: `CREATE TABLE AS SELECT` syntax\n",
    "- **PySpark**: `df.write.saveAsTable()` method\n",
    "- **SQL**: Built-in `_metadata` column support\n",
    "- **PySpark**: Access `_metadata` via `read_files()` SQL function or use native functions\n",
    "- **SQL**: Direct `CREATE STREAMING TABLE` syntax\n",
    "- **PySpark**: Execute `CREATE STREAMING TABLE` via `spark.sql()`\n",
    "\n",
    "Both approaches are valid and can be used based on your preference and use case!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Data_Ingestion_with_Lakeflow_Connect_Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
