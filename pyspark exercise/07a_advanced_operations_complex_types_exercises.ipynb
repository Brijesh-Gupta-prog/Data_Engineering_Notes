{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_vJY_rfwafX"
      },
      "source": [
        "# Module 07a - Advanced Operations - Complex Types - Exercises\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This notebook contains exercises based on the concepts learned in Module 07a.\n",
        "\n",
        "- Complete each exercise in the provided code cells\n",
        "- Run the data setup cells first to generate/create necessary data\n",
        "- Test your solutions by running the verification cells (if provided)\n",
        "- Refer back to the main module notebook if you need help\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHS5GyNWwafZ"
      },
      "source": [
        "## Data Setup\n",
        "\n",
        "Run the cells below to set up the data needed for the exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zynRDsGSwafa",
        "outputId": "378c882d-acb0-47d4-ab52-fe68916938af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkSession created successfully!\n",
            "Data directory: c:\\Users\\Amitha.GS\\data\n",
            "Employee DataFrame created:\n",
            "+-------+---+----------+------+\n",
            "|   Name|Age|Department|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Alice| 25|     Sales| 50000|\n",
            "|    Bob| 30|        IT| 60000|\n",
            "|Charlie| 35|     Sales| 70000|\n",
            "|  Diana| 28|        IT| 55000|\n",
            "|    Eve| 32|        HR| 65000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "import os\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Module Exercises\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set data directory\n",
        "data_dir = \"../data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "print(\"SparkSession created successfully!\")\n",
        "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
        "\n",
        "# Create simple employee DataFrame\n",
        "df_employees = spark.createDataFrame([\n",
        "    (\"Alice\", 25, \"Sales\", 50000),\n",
        "    (\"Bob\", 30, \"IT\", 60000),\n",
        "    (\"Charlie\", 35, \"Sales\", 70000),\n",
        "    (\"Diana\", 28, \"IT\", 55000),\n",
        "    (\"Eve\", 32, \"HR\", 65000)\n",
        "], [\"Name\", \"Age\", \"Department\", \"Salary\"])\n",
        "\n",
        "print(\"Employee DataFrame created:\")\n",
        "df_employees.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjn_KKtowafd"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Complete the following exercises based on the concepts from Module 07a.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34E8xUF9wafe"
      },
      "source": [
        "### Exercise 1: Basic Operation\n",
        "\n",
        "Complete a basic operation based on Module 07a concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCVwS9Lhwafe",
        "outputId": "e2e8cbd8-71a9-4435-da3b-f373306311da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|  Alice| 50000|\n",
            "|    Bob| 60000|\n",
            "|Charlie| 70000|\n",
            "|  Diana| 55000|\n",
            "|    Eve| 65000|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select specific column\n",
        "df_employees.select(\"Name\", \"Salary\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA-aIS3Vwaff",
        "outputId": "a333dd31-4faf-459b-aa4b-8aae5279b840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|Name   |Skills              |\n",
            "+-------+--------------------+\n",
            "|Alice  |[Python, SQL, Spark]|\n",
            "|Bob    |[Java, Scala]       |\n",
            "|Charlie|[Python, R, SQL]    |\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Access array elements by index\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", [\"Python\", \"SQL\", \"Spark\"]),\n",
        "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
        "    (\"Charlie\", [\"Python\", \"R\", \"SQL\"])\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Skills\"])\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSa1EWnIwafg",
        "outputId": "3a56124c-b271-4b00-e845-90291f160755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+\n",
            "|   Name|First_Skill|\n",
            "+-------+-----------+\n",
            "|  Alice|     Python|\n",
            "|    Bob|       Java|\n",
            "|Charlie|     Python|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# access first index\n",
        "df.select(\n",
        "    \"Name\",\n",
        "    col(\"Skills\").getItem(0).alias(\"First_Skill\")\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H54Ru3ptwafh",
        "outputId": "ed9050b1-a5d2-4471-96de-2486d1d8a387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+------------+\n",
            "|   Name|First_Skill|Second_Skill|\n",
            "+-------+-----------+------------+\n",
            "|  Alice|     Python|         SQL|\n",
            "|    Bob|       Java|       Scala|\n",
            "|Charlie|     Python|           R|\n",
            "+-------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SQL indexing\n",
        "df.selectExpr(\n",
        "    \"Name\",\n",
        "    \"Skills[0] as First_Skill\",\n",
        "    \"Skills[1] as Second_Skill\"\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpBw9gRowafi",
        "outputId": "def80a4b-d86e-47d6-f636-01ad3d3ca388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|   Name|              Skills|\n",
            "+-------+--------------------+\n",
            "|  Alice|[Python, SQL, Spark]|\n",
            "|Charlie|    [Python, R, SQL]|\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# filter using array index\n",
        "df.filter(col(\"Skills\").getItem(0) == \"Python\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wayzdv_Ewafj",
        "outputId": "b2aeec54-778e-4529-845f-9a2a94da2be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|   Name|              Skills|\n",
            "+-------+--------------------+\n",
            "|  Alice|[Python, SQL, Spark]|\n",
            "|Charlie|    [Python, R, SQL]|\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_contains, col\n",
        "\n",
        "df.filter(array_contains(col(\"Skills\"), \"Python\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8gkJ6Yowafj",
        "outputId": "22cd5b77-1beb-4502-bf20-2ecd2f5ee0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|   Name|              Skills|\n",
            "+-------+--------------------+\n",
            "|  Alice|[Python, SQL, Spark]|\n",
            "|Charlie|    [Python, R, SQL]|\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT Name, Skills\n",
        "    FROM employees\n",
        "    WHERE array_contains(Skills, 'Python')\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlSCXR4twafk",
        "outputId": "c9b05dca-ad3c-40e3-d3b3-7bad37551d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+\n",
            "|   Name| Skill|\n",
            "+-------+------+\n",
            "|  Alice|Python|\n",
            "|  Alice|   SQL|\n",
            "|  Alice| Spark|\n",
            "|    Bob|  Java|\n",
            "|    Bob| Scala|\n",
            "|Charlie|Python|\n",
            "|Charlie|     R|\n",
            "|Charlie|   SQL|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#explode\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_exploded = df.select(\n",
        "    \"Name\",\n",
        "    explode(\"Skills\").alias(\"Skill\")\n",
        ")\n",
        "\n",
        "df_exploded.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9pnp03bwafl",
        "outputId": "faef09fe-be74-4bdb-e780-362f7289c91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            " |-- Details: struct (nullable = false)\n",
            " |    |-- experience: long (nullable = true)\n",
            " |    |-- projects: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# using stuct\n",
        "from pyspark.sql.functions import struct\n",
        "\n",
        "df_with_struct = df_employees.withColumn(\n",
        "    \"Details\",\n",
        "    struct(\n",
        "        col(\"Age\").alias(\"experience\"),\n",
        "        col(\"Salary\").alias(\"projects\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_with_struct.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuZSIFIDwafm",
        "outputId": "e717cd88-121c-4885-d707-083ce72a3de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+----------+------+-----------+\n",
            "|Name   |Age|Department|Salary|Details    |\n",
            "+-------+---+----------+------+-----------+\n",
            "|Alice  |25 |Sales     |50000 |{25, 50000}|\n",
            "|Bob    |30 |IT        |60000 |{30, 60000}|\n",
            "|Charlie|35 |Sales     |70000 |{35, 70000}|\n",
            "|Diana  |28 |IT        |55000 |{28, 55000}|\n",
            "|Eve    |32 |HR        |65000 |{32, 65000}|\n",
            "+-------+---+----------+------+-----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            " |-- Details: struct (nullable = false)\n",
            " |    |-- Age: long (nullable = true)\n",
            " |    |-- Salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import struct, col\n",
        "\n",
        "df_struct = df_employees.withColumn(\n",
        "    \"Details\",\n",
        "    struct(\n",
        "        col(\"Age\"),\n",
        "        col(\"Salary\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_struct.show(truncate=False)\n",
        "df_struct.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J71oFgthwafm",
        "outputId": "5dc70fa4-f9f7-4e84-e21a-cb6fd02fbb43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------------------------------+\n",
            "|Name |Details                          |\n",
            "+-----+---------------------------------+\n",
            "|Alice|{experience -> 5, projects -> 10}|\n",
            "|Bob  |{experience -> 3, projects -> 5} |\n",
            "+-----+---------------------------------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Details: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: long (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", {\"experience\": 5, \"projects\": 10}),\n",
        "    (\"Bob\", {\"experience\": 3, \"projects\": 5})\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    data,\n",
        "    [\"Name\", \"Details\"]\n",
        ")\n",
        "\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv47jIA-wafm",
        "outputId": "aa81edf6-ea25-4d03-d517-8fda7e5afa95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "| Name|Has_Projects|\n",
            "+-----+------------+\n",
            "|Alice|        true|\n",
            "|  Bob|        true|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# key exists are not\n",
        "from pyspark.sql.functions import map_contains_key\n",
        "\n",
        "df.select(\n",
        "    \"Name\",\n",
        "    map_contains_key(col(\"Details\"), \"projects\").alias(\"Has_Projects\")\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxw5EnH_wafm",
        "outputId": "33ec1f10-c405-4321-bd76-7cff843099da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+-----+\n",
            "| Name|       Key|Value|\n",
            "+-----+----------+-----+\n",
            "|Alice|experience|    5|\n",
            "|Alice|  projects|   10|\n",
            "|  Bob|experience|    3|\n",
            "|  Bob|  projects|    5|\n",
            "+-----+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert map to rows\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df.select(\n",
        "    \"Name\",\n",
        "    explode(\"Details\").alias(\"Key\", \"Value\")\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKfv67jYwafn"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Great job completing the exercises! Review your solutions and compare them with the solutions notebook if needed.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}