{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47YiCR5Srpiv"
      },
      "source": [
        "# Module 04 - Data Transformations & Aggregations - Exercises## InstructionsThis notebook contains exercises based on the concepts learned in Module 04.- Complete each exercise in the provided code cells- Run the data setup cells first to generate/create necessary data- Test your solutions by running the verification cells (if provided)- Refer back to the main module notebook if you need help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txDVKZ0Vrpiw"
      },
      "source": [
        "## Data Setup\n",
        "\n",
        "Run the cells below to set up the data needed for the exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTozIe8_rpiw",
        "outputId": "3abec0a4-8ca8-4de2-b5ad-820ca149edae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession created successfully!\n",
            "Data directory: /data\n",
            "Generating large dataset for aggregations (this may take a minute)...\n",
            "Created large CSV file: ../data/large_employees.csv (2000000 records)\n",
            "\n",
            "Small DataFrame for quick exercises:\n",
            "+-------+---+----------+------+-------+\n",
            "|   Name|Age|Department|Salary|   City|\n",
            "+-------+---+----------+------+-------+\n",
            "|  Alice| 25|     Sales| 50000|    NYC|\n",
            "|    Bob| 30|        IT| 60000|     LA|\n",
            "|Charlie| 35|     Sales| 70000|Chicago|\n",
            "|  Diana| 28|        IT| 55000|Houston|\n",
            "|    Eve| 32|        HR| 65000|Phoenix|\n",
            "|  Frank| 27|     Sales| 52000|    NYC|\n",
            "|  Grace| 29|        IT| 58000|     LA|\n",
            "|  Henry| 31|        HR| 62000|Chicago|\n",
            "+-------+---+----------+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(f\"Module 4 Exercises\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set data directory\n",
        "data_dir = \"../data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "print(\"SparkSession created successfully!\")\n",
        "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
        "\n",
        "# Generate larger dataset for aggregations (~500 MB)\n",
        "print(\"Generating large dataset for aggregations (this may take a minute)...\")\n",
        "n_records = 2_000_000  # ~500 MB of data\n",
        "\n",
        "large_data = {\n",
        "    \"employee_id\": range(1, n_records + 1),\n",
        "    \"name\": [f\"Employee_{i}\" for i in range(1, n_records + 1)],\n",
        "    \"department\": np.random.choice([\"Sales\", \"IT\", \"HR\", \"Finance\", \"Marketing\"], n_records),\n",
        "    \"salary\": np.random.randint(40000, 150000, n_records),\n",
        "    \"age\": np.random.randint(22, 65, n_records),\n",
        "    \"city\": np.random.choice(\n",
        "        [\"NYC\", \"LA\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\"], n_records\n",
        "    )\n",
        "}\n",
        "\n",
        "df_large = pd.DataFrame(large_data)\n",
        "df_large.to_csv(f\"{data_dir}/large_employees.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Created large CSV file: {data_dir}/large_employees.csv ({len(df_large)} records)\")\n",
        "\n",
        "# Also create a smaller DataFrame for quick exercises\n",
        "small_data = [\n",
        "    (\"Alice\", 25, \"Sales\", 50000, \"NYC\"),\n",
        "    (\"Bob\", 30, \"IT\", 60000, \"LA\"),\n",
        "    (\"Charlie\", 35, \"Sales\", 70000, \"Chicago\"),\n",
        "    (\"Diana\", 28, \"IT\", 55000, \"Houston\"),\n",
        "    (\"Eve\", 32, \"HR\", 65000, \"Phoenix\"),\n",
        "    (\"Frank\", 27, \"Sales\", 52000, \"NYC\"),\n",
        "    (\"Grace\", 29, \"IT\", 58000, \"LA\"),\n",
        "    (\"Henry\", 31, \"HR\", 62000, \"Chicago\")\n",
        "]\n",
        "\n",
        "small_schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Department\", StringType(), True),\n",
        "    StructField(\"Salary\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True)\n",
        "])\n",
        "\n",
        "df_employees = spark.createDataFrame(small_data, small_schema)\n",
        "\n",
        "print(\"\\nSmall DataFrame for quick exercises:\")\n",
        "df_employees.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHVplsRLrpix"
      },
      "source": [
        "## ExercisesComplete the following exercises based on the concepts from Module 04."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVvEAJtprpix"
      },
      "source": [
        "### Exercise 1: GroupBy and AggregateGroup df_employees by Department and calculate:- Count of employees- Average salary- Maximum salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXBliX2Hrpix",
        "outputId": "1ce4e0b5-75d3-4d37-c9ac-b74a859aef56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------------+--------------+\n",
            "|Department|Total Count|    Avarage Salary|Maximum Salary|\n",
            "+----------+-----------+------------------+--------------+\n",
            "|     Sales|          3|57333.333333333336|         70000|\n",
            "|        IT|          3|57666.666666666664|         60000|\n",
            "|        HR|          2|           63500.0|         65000|\n",
            "+----------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import avg,count,max\n",
        "# Your code here\n",
        "df_1 = df_employees.groupBy(\"Department\").agg(\n",
        "    count(\"*\").alias(\"Total Count\"),\n",
        "    avg(\"Salary\").alias(\"Avarage Salary\"),\n",
        "    max(\"Salary\").alias(\"Maximum Salary\")\n",
        ")\n",
        "df_1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk29xL05rpix"
      },
      "source": [
        "### Exercise 2: Add a ColumnAdd a new column 'Bonus' to df_employees that is 10% of the Salary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOdwapNXrpix",
        "outputId": "44ee2203-c5ab-42ee-84ae-4d81df31711d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+-------+-----------------+\n",
            "|   Name|Age|Department|Salary|   City|            bonus|\n",
            "+-------+---+----------+------+-------+-----------------+\n",
            "|  Alice| 25|     Sales| 50000|    NYC|55000.00000000001|\n",
            "|    Bob| 30|        IT| 60000|     LA|          66000.0|\n",
            "|Charlie| 35|     Sales| 70000|Chicago|          77000.0|\n",
            "|  Diana| 28|        IT| 55000|Houston|60500.00000000001|\n",
            "|    Eve| 32|        HR| 65000|Phoenix|          71500.0|\n",
            "|  Frank| 27|     Sales| 52000|    NYC|57200.00000000001|\n",
            "|  Grace| 29|        IT| 58000|     LA|63800.00000000001|\n",
            "|  Henry| 31|        HR| 62000|Chicago|          68200.0|\n",
            "+-------+---+----------+------+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_2 = df_employees.withColumn(\"bonus\",col(\"Salary\")*1.1)\n",
        "df_2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It84GHiZrpiy"
      },
      "source": [
        "### Exercise 3: Handle Null ValuesFill null values in the 'Age' column with 0 (if any exist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dajVNeXgrpiy",
        "outputId": "06b4757b-7fe5-4b1c-89fc-3b52d9e921c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+-------+\n",
            "|   Name|Age|Department|Salary|   City|\n",
            "+-------+---+----------+------+-------+\n",
            "|  Alice| 25|     Sales| 50000|    NYC|\n",
            "|    Bob| 30|        IT| 60000|     LA|\n",
            "|Charlie| 35|     Sales| 70000|Chicago|\n",
            "|  Diana| 28|        IT| 55000|Houston|\n",
            "|    Eve| 32|        HR| 65000|Phoenix|\n",
            "|  Frank| 27|     Sales| 52000|    NYC|\n",
            "|  Grace| 29|        IT| 58000|     LA|\n",
            "|  Henry| 31|        HR| 62000|Chicago|\n",
            "+-------+---+----------+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_4 = df_employees.fillna({'Age':0})\n",
        "df_4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RTlgTdrpiy"
      },
      "source": [
        "### Exercise 4: Large Dataset AggregationRead the large_employees.csv file and:1. Group by department2. Calculate average salary per department3. Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRedPK0Drpiy",
        "outputId": "8141ddd6-d884-40a3-9059-09ce88729efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|department|   Avarage Salary|\n",
            "+----------+-----------------+\n",
            "|     Sales|95005.34506057782|\n",
            "|        HR|95029.76897117225|\n",
            "|   Finance|94992.25943240403|\n",
            "| Marketing|94980.24231038033|\n",
            "|        IT|94994.32036239898|\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "data = spark.read \\\n",
        "       .format(\"csv\") \\\n",
        "       .option(\"header\",True) \\\n",
        "       .load(f\"{data_dir}/large_employees.csv\")\n",
        "\n",
        "result = data.groupBy(\"department\").agg(\n",
        "    avg(\"salary\").alias(\"Avarage Salary\")\n",
        ")\n",
        "result.show()\n",
        "\n",
        "# Note: This may take a few minutes due to dataset size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLe_0txrrpiy"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Great job completing the exercises! Review your solutions and compare them with the solutions notebook if needed.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}