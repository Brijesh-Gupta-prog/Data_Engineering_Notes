{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 03 - Data Ingestion: Batch and Streaming\n",
        "\n",
        "## Overview\n",
        "\n",
        "Data ingestion is the process of moving data from various sources into Azure storage and processing systems. This module covers different types of data ingestion patterns and the Azure services used for each.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this module, you will understand:\n",
        "- What is data ingestion and why it's important\n",
        "- Types of data (structured, semi-structured, unstructured)\n",
        "- Batch data ingestion patterns and use cases\n",
        "- Streaming data ingestion patterns and use cases\n",
        "- Azure services for data ingestion\n",
        "- Best practices for data ingestion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Data Ingestion?\n",
        "\n",
        "**Data Ingestion** is the process of importing, transferring, loading, and processing data for immediate use or storage in a database or data warehouse.\n",
        "\n",
        "### Why is Data Ingestion Important?\n",
        "\n",
        "- **Data Sources**: Data exists in many places (databases, files, APIs, IoT devices)\n",
        "- **Centralized Storage**: Need to bring data together for analysis\n",
        "- **Real-time Needs**: Some data needs immediate processing\n",
        "- **Scalability**: Handle large volumes of data efficiently\n",
        "- **Reliability**: Ensure data arrives correctly and on time\n",
        "\n",
        "### Data Ingestion Pipeline\n",
        "\n",
        "```\n",
        "Source Systems → Ingestion Layer → Storage → Processing\n",
        "     ↓              ↓                ↓          ↓\n",
        "  Databases    Azure Data      Azure      Spark/Synapse\n",
        "  Files        Factory         Storage    Analytics\n",
        "  APIs         Event Hubs      Data Lake\n",
        "  IoT Devices  Stream Analytics\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Data\n",
        "\n",
        "Understanding data types helps in choosing the right ingestion method and storage.\n",
        "\n",
        "### 1. Structured Data\n",
        "\n",
        "**Definition**: Data with a fixed schema and well-defined format.\n",
        "\n",
        "**Characteristics:**\n",
        "- Organized in rows and columns\n",
        "- Follows a predefined schema\n",
        "- Easy to query and analyze\n",
        "\n",
        "**Examples:**\n",
        "- Relational databases (SQL Server, Oracle, MySQL)\n",
        "- CSV files with consistent columns\n",
        "- Excel spreadsheets\n",
        "- Parquet files\n",
        "\n",
        "**Storage**: Azure SQL Database, Synapse SQL Pools, Tables\n",
        "\n",
        "### 2. Semi-Structured Data\n",
        "\n",
        "**Definition**: Data with some structure but flexible schema.\n",
        "\n",
        "**Characteristics:**\n",
        "- Has tags or markers to separate elements\n",
        "- Schema can vary\n",
        "- Self-describing format\n",
        "\n",
        "**Examples:**\n",
        "- JSON files\n",
        "- XML files\n",
        "- Avro files\n",
        "- NoSQL databases (Cosmos DB)\n",
        "\n",
        "**Storage**: Azure Storage, Data Lake, Cosmos DB\n",
        "\n",
        "### 3. Unstructured Data\n",
        "\n",
        "**Definition**: Data without a predefined structure or schema.\n",
        "\n",
        "**Characteristics:**\n",
        "- No fixed format\n",
        "- Difficult to query directly\n",
        "- Requires processing to extract insights\n",
        "\n",
        "**Examples:**\n",
        "- Text documents\n",
        "- Images\n",
        "- Videos\n",
        "- Audio files\n",
        "- Log files\n",
        "\n",
        "**Storage**: Azure Blob Storage, Data Lake Storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Data Ingestion\n",
        "\n",
        "**Batch ingestion** processes data in large chunks at scheduled intervals or when triggered.\n",
        "\n",
        "### Characteristics\n",
        "\n",
        "- **Volume**: Large amounts of data processed together\n",
        "- **Frequency**: Scheduled (hourly, daily, weekly) or on-demand\n",
        "- **Latency**: Higher latency (minutes to hours)\n",
        "- **Processing**: Bulk operations on entire datasets\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "✅ **ETL Processes**: Extract data from source, transform, load to destination\n",
        "✅ **Historical Data Loading**: Loading large historical datasets\n",
        "✅ **Scheduled Reports**: Daily/weekly data refreshes\n",
        "✅ **Data Warehousing**: Loading data into data warehouses\n",
        "✅ **File Processing**: Processing files uploaded to storage\n",
        "\n",
        "### Example Scenarios\n",
        "\n",
        "1. **Daily Sales Data**\n",
        "   - Source: On-premises SQL Server\n",
        "   - Schedule: Every night at 2 AM\n",
        "   - Destination: Azure Data Lake\n",
        "   - Process: Extract all sales from previous day\n",
        "\n",
        "2. **Monthly Financial Reports**\n",
        "   - Source: Multiple Excel files\n",
        "   - Schedule: First day of each month\n",
        "   - Destination: Azure Synapse Analytics\n",
        "   - Process: Aggregate and consolidate data\n",
        "\n",
        "3. **Customer Data Migration**\n",
        "   - Source: Legacy database\n",
        "   - Schedule: One-time migration\n",
        "   - Destination: Azure SQL Database\n",
        "   - Process: Full data extract and load\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming Data Ingestion\n",
        "\n",
        "**Streaming ingestion** processes data continuously as it arrives, in real-time or near real-time.\n",
        "\n",
        "### Characteristics\n",
        "\n",
        "- **Volume**: Continuous flow of data\n",
        "- **Frequency**: Real-time or near real-time\n",
        "- **Latency**: Low latency (seconds to milliseconds)\n",
        "- **Processing**: Event-by-event or micro-batch processing\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "✅ **IoT Data**: Sensor data from devices\n",
        "✅ **Real-time Analytics**: Live dashboards and monitoring\n",
        "✅ **Event Processing**: User clicks, transactions, logs\n",
        "✅ **Fraud Detection**: Real-time transaction monitoring\n",
        "✅ **Live Recommendations**: Real-time personalization\n",
        "\n",
        "### Example Scenarios\n",
        "\n",
        "1. **IoT Sensor Data**\n",
        "   - Source: Temperature sensors\n",
        "   - Frequency: Every second\n",
        "   - Destination: Azure Event Hubs → Stream Analytics\n",
        "   - Process: Real-time temperature monitoring and alerts\n",
        "\n",
        "2. **E-commerce Clickstream**\n",
        "   - Source: Website user clicks\n",
        "   - Frequency: Continuous\n",
        "   - Destination: Event Hubs → Data Lake\n",
        "   - Process: Real-time user behavior analysis\n",
        "\n",
        "3. **Financial Trading**\n",
        "   - Source: Stock market feeds\n",
        "   - Frequency: Millisecond-level\n",
        "   - Destination: Event Hubs → Stream Analytics\n",
        "   - Process: Real-time trading decisions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch vs Streaming: Comparison\n",
        "\n",
        "| Aspect | Batch Ingestion | Streaming Ingestion |\n",
        "|--------|----------------|---------------------|\n",
        "| **Data Volume** | Large chunks | Continuous flow |\n",
        "| **Processing Time** | Scheduled intervals | Real-time |\n",
        "| **Latency** | Minutes to hours | Seconds to milliseconds |\n",
        "| **Use Cases** | ETL, reports, analytics | Real-time monitoring, alerts |\n",
        "| **Complexity** | Lower | Higher |\n",
        "| **Cost** | Lower (scheduled) | Higher (always-on) |\n",
        "| **Tools** | Azure Data Factory | Event Hubs, Stream Analytics |\n",
        "\n",
        "### When to Use Batch\n",
        "\n",
        "- Data doesn't need immediate processing\n",
        "- Large volumes of historical data\n",
        "- Scheduled reporting and analytics\n",
        "- Cost optimization is important\n",
        "- Data quality checks are needed before processing\n",
        "\n",
        "### When to Use Streaming\n",
        "\n",
        "- Real-time decision making required\n",
        "- Immediate alerts and notifications\n",
        "- Live dashboards and monitoring\n",
        "- Event-driven applications\n",
        "- Low latency is critical\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Azure Services for Data Ingestion\n",
        "\n",
        "### Azure Data Factory (ADF)\n",
        "\n",
        "**Purpose**: Cloud-based ETL/ELT service for batch data movement and transformation.\n",
        "\n",
        "**Key Features:**\n",
        "- Visual pipeline designer\n",
        "- 90+ built-in connectors\n",
        "- Schedule-based or event-driven triggers\n",
        "- Data transformation capabilities\n",
        "- Monitoring and alerting\n",
        "\n",
        "**Use Cases:**\n",
        "- Moving data from on-premises to cloud\n",
        "- Scheduled batch data loads\n",
        "- ETL workflows\n",
        "- Data integration between systems\n",
        "\n",
        "**Example Flow:**\n",
        "```\n",
        "SQL Server → ADF Pipeline → Azure Data Lake\n",
        "```\n",
        "\n",
        "### Azure Event Hubs\n",
        "\n",
        "**Purpose**: Big data streaming platform and event ingestion service.\n",
        "\n",
        "**Key Features:**\n",
        "- High throughput (millions of events per second)\n",
        "- Low latency\n",
        "- Multiple consumer groups\n",
        "- Capture feature (auto-save to storage)\n",
        "\n",
        "**Use Cases:**\n",
        "- IoT data ingestion\n",
        "- Real-time event streaming\n",
        "- Clickstream analytics\n",
        "- Log aggregation\n",
        "\n",
        "**Example Flow:**\n",
        "```\n",
        "IoT Devices → Event Hubs → Stream Analytics → Power BI\n",
        "```\n",
        "\n",
        "### Azure IoT Hub\n",
        "\n",
        "**Purpose**: Managed service for IoT device connectivity and management.\n",
        "\n",
        "**Key Features:**\n",
        "- Device-to-cloud and cloud-to-device messaging\n",
        "- Device management\n",
        "- Security and authentication\n",
        "- Protocol support (MQTT, AMQP, HTTP)\n",
        "\n",
        "**Use Cases:**\n",
        "- IoT device data collection\n",
        "- Device management\n",
        "- Command and control\n",
        "\n",
        "### Azure Stream Analytics\n",
        "\n",
        "**Purpose**: Real-time analytics on streaming data.\n",
        "\n",
        "**Key Features:**\n",
        "- SQL-like query language\n",
        "- Real-time processing\n",
        "- Multiple input/output sources\n",
        "- Windowing functions\n",
        "\n",
        "**Use Cases:**\n",
        "- Real-time dashboards\n",
        "- Anomaly detection\n",
        "- Real-time aggregations\n",
        "- Event filtering and routing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Ingestion Patterns\n",
        "\n",
        "### Pattern 1: Extract and Load (EL)\n",
        "\n",
        "**Simple data movement without transformation.**\n",
        "\n",
        "```\n",
        "Source → Ingestion Service → Destination Storage\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "- Copy files from on-premises to Azure Storage\n",
        "- No transformation needed\n",
        "- Fast and simple\n",
        "\n",
        "### Pattern 2: Extract, Transform, Load (ETL)\n",
        "\n",
        "**Transform data during ingestion process.**\n",
        "\n",
        "```\n",
        "Source → Extract → Transform → Load → Destination\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "- Extract from SQL Server\n",
        "- Transform: Clean, filter, aggregate\n",
        "- Load to Data Lake\n",
        "\n",
        "### Pattern 3: Extract, Load, Transform (ELT)\n",
        "\n",
        "**Load raw data first, then transform in destination.**\n",
        "\n",
        "```\n",
        "Source → Extract → Load → Transform (in destination) → Analytics\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "- Extract raw data to Data Lake\n",
        "- Load to Synapse Analytics\n",
        "- Transform using SQL/Spark in Synapse\n",
        "\n",
        "### Pattern 4: Change Data Capture (CDC)\n",
        "\n",
        "**Capture only changed data since last ingestion.**\n",
        "\n",
        "```\n",
        "Source → CDC → Changed Data Only → Destination\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "- Track changes in source database\n",
        "- Ingest only new/modified records\n",
        "- Efficient for large tables\n",
        "\n",
        "### Pattern 5: Lambda Architecture\n",
        "\n",
        "**Combines batch and streaming for comprehensive analytics.**\n",
        "\n",
        "```\n",
        "Streaming Path: Real-time data → Event Hubs → Stream Analytics → Real-time views\n",
        "Batch Path: Historical data → Data Factory → Data Lake → Batch processing → Batch views\n",
        "Merge: Combine real-time and batch views for complete picture\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Sources and Destinations\n",
        "\n",
        "### Common Data Sources\n",
        "\n",
        "#### On-Premises Sources\n",
        "- **SQL Server**: Relational databases\n",
        "- **File Servers**: CSV, Excel, JSON files\n",
        "- **Oracle/MySQL**: Other relational databases\n",
        "- **SAP**: ERP systems\n",
        "- **Mainframes**: Legacy systems\n",
        "\n",
        "#### Cloud Sources\n",
        "- **Azure SQL Database**: Managed SQL database\n",
        "- **Azure Storage**: Blob, Data Lake\n",
        "- **Azure Cosmos DB**: NoSQL database\n",
        "- **Salesforce**: CRM data\n",
        "- **Dynamics 365**: Business applications\n",
        "- **REST APIs**: Web services\n",
        "\n",
        "#### Streaming Sources\n",
        "- **IoT Devices**: Sensors, devices\n",
        "- **Applications**: Logs, events\n",
        "- **Social Media**: Twitter, Facebook feeds\n",
        "- **Web Clickstream**: User interactions\n",
        "\n",
        "### Common Destinations\n",
        "\n",
        "#### Storage Destinations\n",
        "- **Azure Blob Storage**: Object storage\n",
        "- **Azure Data Lake Storage Gen2**: Analytics storage\n",
        "- **Azure Files**: File shares\n",
        "\n",
        "#### Database Destinations\n",
        "- **Azure SQL Database**: Managed SQL\n",
        "- **Azure Synapse Analytics**: Data warehouse\n",
        "- **Azure Cosmos DB**: NoSQL\n",
        "- **Azure Database for PostgreSQL/MySQL**: Open-source databases\n",
        "\n",
        "#### Analytics Destinations\n",
        "- **Azure Synapse Analytics**: Data warehousing\n",
        "- **Azure Databricks**: Spark analytics\n",
        "- **Power BI**: Business intelligence\n",
        "- **Azure Analysis Services**: Analytics engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Data Ingestion\n",
        "\n",
        "### 1. Data Validation\n",
        "\n",
        "✅ **Validate at Source**: Check data quality before ingestion\n",
        "✅ **Schema Validation**: Ensure data matches expected schema\n",
        "✅ **Data Type Checks**: Verify data types are correct\n",
        "✅ **Null Handling**: Handle missing values appropriately\n",
        "✅ **Error Handling**: Log and handle errors gracefully\n",
        "\n",
        "### 2. Incremental Loading\n",
        "\n",
        "✅ **Use Timestamps**: Track last ingestion time\n",
        "✅ **Change Data Capture**: Only ingest changed data\n",
        "✅ **Partitioning**: Partition by date/time for efficiency\n",
        "✅ **Idempotency**: Ensure re-running doesn't create duplicates\n",
        "\n",
        "### 3. Performance Optimization\n",
        "\n",
        "✅ **Parallel Processing**: Process multiple files/partitions in parallel\n",
        "✅ **Compression**: Compress data during transfer\n",
        "✅ **Batch Sizes**: Optimize batch sizes for throughput\n",
        "✅ **Network Optimization**: Use ExpressRoute for on-premises\n",
        "✅ **Resource Scaling**: Scale resources based on workload\n",
        "\n",
        "### 4. Monitoring and Alerting\n",
        "\n",
        "✅ **Pipeline Monitoring**: Track pipeline execution status\n",
        "✅ **Data Quality Metrics**: Monitor data quality\n",
        "✅ **Latency Tracking**: Monitor ingestion latency\n",
        "✅ **Error Alerts**: Set up alerts for failures\n",
        "✅ **Cost Monitoring**: Track ingestion costs\n",
        "\n",
        "### 5. Security\n",
        "\n",
        "✅ **Encryption**: Encrypt data in transit and at rest\n",
        "✅ **Authentication**: Use managed identities or service principals\n",
        "✅ **Network Security**: Use private endpoints when possible\n",
        "✅ **Access Control**: Implement least privilege access\n",
        "✅ **Audit Logging**: Log all data access and changes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Ingestion Architecture Example\n",
        "\n",
        "### Hybrid Architecture (Batch + Streaming)\n",
        "\n",
        "```\n",
        "┌─────────────────┐\n",
        "│  On-Premises    │\n",
        "│  SQL Server     │──┐\n",
        "└─────────────────┘  │\n",
        "                     │\n",
        "┌─────────────────┐  │    ┌──────────────────┐\n",
        "│  IoT Devices    │──┼───▶│  Azure Event Hubs │\n",
        "└─────────────────┘  │    └────────┬─────────┘\n",
        "                     │             │\n",
        "┌─────────────────┐  │             │\n",
        "│  File Server    │──┘             │\n",
        "└─────────────────┘                │\n",
        "                                   │\n",
        "                    ┌──────────────┴──────────────┐\n",
        "                    │                             │\n",
        "         ┌──────────▼──────────┐    ┌────────────▼─────────┐\n",
        "         │ Azure Data Factory  │    │ Stream Analytics     │\n",
        "         │ (Batch Processing)  │    │ (Real-time Processing)│\n",
        "         └──────────┬──────────┘    └────────────┬─────────┘\n",
        "                    │                            │\n",
        "         ┌──────────▼──────────┐    ┌────────────▼─────────┐\n",
        "         │ Azure Data Lake     │    │ Azure Data Lake      │\n",
        "         │ (Raw/Batch Data)    │    │ (Streaming Data)     │\n",
        "         └──────────┬──────────┘    └────────────┬─────────┘\n",
        "                    │                            │\n",
        "                    └────────────┬───────────────┘\n",
        "                                 │\n",
        "                    ┌────────────▼────────────┐\n",
        "                    │ Azure Synapse Analytics │\n",
        "                    │ (Unified Analytics)     │\n",
        "                    └─────────────────────────┘\n",
        "```\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Batch Path**: SQL Server, File Server → Data Factory → Data Lake\n",
        "2. **Streaming Path**: IoT Devices → Event Hubs → Stream Analytics → Data Lake\n",
        "3. **Unified Analytics**: Both paths feed into Synapse Analytics for comprehensive analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this module, we've covered:\n",
        "\n",
        "✅ What is data ingestion and its importance\n",
        "✅ Types of data (structured, semi-structured, unstructured)\n",
        "✅ Batch data ingestion patterns and use cases\n",
        "✅ Streaming data ingestion patterns and use cases\n",
        "✅ Comparison between batch and streaming\n",
        "✅ Azure services for data ingestion (ADF, Event Hubs, IoT Hub, Stream Analytics)\n",
        "✅ Data ingestion patterns (EL, ETL, ELT, CDC, Lambda)\n",
        "✅ Common data sources and destinations\n",
        "✅ Best practices for data ingestion\n",
        "✅ Example data ingestion architecture\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Data Ingestion** is the first step in the data engineering pipeline\n",
        "2. **Batch ingestion** is for scheduled, large-volume data processing\n",
        "3. **Streaming ingestion** is for real-time, continuous data processing\n",
        "4. **Choose the right service** based on your latency and volume requirements\n",
        "5. **Azure Data Factory** is the primary service for batch data movement\n",
        "6. **Event Hubs** is the primary service for streaming data ingestion\n",
        "7. **Consider data types** when designing ingestion pipelines\n",
        "8. **Follow best practices** for validation, performance, and security\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Proceed to **Module 04: ETL Concepts and Data Transformation** to learn about:\n",
        "- ETL fundamentals\n",
        "- Data mapping and transformation\n",
        "- Data profiling\n",
        "- Transformation techniques\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
