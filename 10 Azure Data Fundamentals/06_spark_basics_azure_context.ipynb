{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 06 - Spark Basics in Azure Context\n",
        "\n",
        "## Overview\n",
        "\n",
        "This module covers Apache Spark fundamentals in the context of Azure data engineering. You'll learn how Spark is used in Azure for processing large-scale data.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this module, you will understand:\n",
        "- What is Apache Spark and why it's important\n",
        "- Spark architecture and components\n",
        "- Spark in Azure (Databricks, Synapse, HDInsight)\n",
        "- Basic Spark operations (transformations and actions)\n",
        "- Working with DataFrames in Spark\n",
        "- Spark best practices in Azure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Apache Spark?\n",
        "\n",
        "**Apache Spark** is an open-source, distributed processing system used for big data workloads. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Fast Processing**: In-memory computing for faster processing\n",
        "- **Distributed**: Processes data across multiple nodes\n",
        "- **Fault Tolerant**: Handles node failures gracefully\n",
        "- **Multiple Languages**: Supports Python, Scala, Java, R, SQL\n",
        "- **Multiple Data Sources**: Works with various data formats\n",
        "- **Unified Platform**: Batch and streaming processing\n",
        "\n",
        "### Why Spark for Big Data?\n",
        "\n",
        "- **Performance**: 100x faster than Hadoop MapReduce for certain workloads\n",
        "- **Ease of Use**: High-level APIs (DataFrames, SQL)\n",
        "- **Versatility**: Batch, streaming, machine learning, graph processing\n",
        "- **Scalability**: Handles petabytes of data\n",
        "- **Ecosystem**: Rich ecosystem of libraries\n",
        "\n",
        "### Spark Use Cases\n",
        "\n",
        "✅ **ETL Processing**: Transform large datasets\n",
        "✅ **Data Analytics**: Analyze big data\n",
        "✅ **Real-time Processing**: Stream processing\n",
        "✅ **Machine Learning**: MLlib for ML workloads\n",
        "✅ **Data Warehousing**: Query large datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark Architecture\n",
        "\n",
        "### Core Components\n",
        "\n",
        "#### 1. Spark Driver\n",
        "- **Purpose**: Main program that creates SparkContext\n",
        "- **Responsibilities**: \n",
        "  - Convert user code into tasks\n",
        "  - Schedule tasks on executors\n",
        "  - Coordinate execution\n",
        "\n",
        "#### 2. Spark Executors\n",
        "- **Purpose**: Worker nodes that execute tasks\n",
        "- **Responsibilities**:\n",
        "  - Run tasks assigned by driver\n",
        "  - Store data in memory/disk\n",
        "  - Report status to driver\n",
        "\n",
        "#### 3. Cluster Manager\n",
        "- **Purpose**: Manages cluster resources\n",
        "- **Types**: Standalone, YARN, Mesos, Kubernetes\n",
        "\n",
        "### Spark Architecture Diagram\n",
        "\n",
        "```\n",
        "┌─────────────┐\n",
        "│   Driver    │  (Main Program)\n",
        "└──────┬──────┘\n",
        "       │\n",
        "       ▼\n",
        "┌─────────────┐\n",
        "│   Cluster   │\n",
        "│   Manager   │\n",
        "└──────┬──────┘\n",
        "       │\n",
        "   ┌───┴───┐\n",
        "   │       │\n",
        "┌──▼──┐ ┌──▼──┐\n",
        "│Exec1│ │Exec2│  (Workers)\n",
        "└─────┘ └─────┘\n",
        "```\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **RDD (Resilient Distributed Dataset)**: Immutable distributed collection\n",
        "- **DataFrame**: Distributed collection organized into named columns\n",
        "- **Dataset**: Type-safe DataFrame (Scala/Java)\n",
        "- **Partition**: Logical division of data across nodes\n",
        "- **Task**: Unit of work sent to executor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark in Azure\n",
        "\n",
        "Azure provides multiple options for running Spark workloads:\n",
        "\n",
        "### 1. Azure Databricks\n",
        "\n",
        "**Purpose**: Unified analytics platform built on Spark\n",
        "\n",
        "**Features:**\n",
        "- Fully managed Spark clusters\n",
        "- Collaborative notebooks\n",
        "- Optimized Spark runtime\n",
        "- Integration with Azure services\n",
        "- MLflow for ML lifecycle\n",
        "\n",
        "**Use Cases:**\n",
        "- Data engineering and ETL\n",
        "- Data science and ML\n",
        "- Real-time analytics\n",
        "- Collaborative analytics\n",
        "\n",
        "### 2. Azure Synapse Analytics (Spark Pools)\n",
        "\n",
        "**Purpose**: Spark pools within Synapse workspace\n",
        "\n",
        "**Features:**\n",
        "- Integrated with Synapse workspace\n",
        "- Serverless or provisioned pools\n",
        "- Direct integration with Data Lake\n",
        "- SQL and Spark in one platform\n",
        "\n",
        "**Use Cases:**\n",
        "- Data warehousing with Spark\n",
        "- Unified analytics platform\n",
        "- ELT workloads\n",
        "\n",
        "### 3. Azure HDInsight\n",
        "\n",
        "**Purpose**: Managed Hadoop and Spark clusters\n",
        "\n",
        "**Features:**\n",
        "- Multiple cluster types (Spark, Hive, etc.)\n",
        "- Open-source Hadoop ecosystem\n",
        "- Integration with Azure services\n",
        "\n",
        "**Use Cases:**\n",
        "- Big data processing\n",
        "- Hadoop ecosystem needs\n",
        "- Legacy Hadoop migrations\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| Feature | Databricks | Synapse Spark | HDInsight |\n",
        "|---------|-----------|---------------|-----------|\n",
        "| **Managed** | Fully | Fully | Fully |\n",
        "| **Optimization** | High | Medium | Standard |\n",
        "| **Integration** | Excellent | Native | Good |\n",
        "| **ML Support** | Excellent | Good | Standard |\n",
        "| **Cost** | Higher | Medium | Lower |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark Operations\n",
        "\n",
        "### Transformations vs Actions\n",
        "\n",
        "#### Transformations\n",
        "- **Lazy Evaluation**: Not executed immediately\n",
        "- **Create New RDD/DataFrame**: Return new dataset\n",
        "- **Examples**: `filter()`, `map()`, `select()`, `groupBy()`\n",
        "\n",
        "#### Actions\n",
        "- **Eager Evaluation**: Executed immediately\n",
        "- **Trigger Execution**: Cause transformations to run\n",
        "- **Return Results**: Return values to driver\n",
        "- **Examples**: `count()`, `collect()`, `show()`, `write()`\n",
        "\n",
        "### Common Transformations\n",
        "\n",
        "```python\n",
        "# Filter rows\n",
        "df_filtered = df.filter(df.Amount > 1000)\n",
        "\n",
        "# Select columns\n",
        "df_selected = df.select(\"CustomerID\", \"Amount\")\n",
        "\n",
        "# Add column\n",
        "df_with_new = df.withColumn(\"Total\", df.Amount * 1.1)\n",
        "\n",
        "# Group by and aggregate\n",
        "df_agg = df.groupBy(\"Region\").agg(\n",
        "    sum(\"Amount\").alias(\"TotalSales\"),\n",
        "    count(\"*\").alias(\"OrderCount\")\n",
        ")\n",
        "\n",
        "# Join\n",
        "df_joined = df1.join(df2, on=\"CustomerID\", how=\"inner\")\n",
        "\n",
        "# Sort\n",
        "df_sorted = df.orderBy(df.Amount.desc())\n",
        "```\n",
        "\n",
        "### Common Actions\n",
        "\n",
        "```python\n",
        "# Show data\n",
        "df.show()\n",
        "\n",
        "# Count rows\n",
        "row_count = df.count()\n",
        "\n",
        "# Collect to driver (use carefully!)\n",
        "data = df.collect()\n",
        "\n",
        "# Write to storage\n",
        "df.write.format(\"parquet\").save(\"path/to/output\")\n",
        "\n",
        "# Take first N rows\n",
        "first_rows = df.take(10)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with DataFrames\n",
        "\n",
        "### Creating DataFrames\n",
        "\n",
        "```python\n",
        "# From CSV file\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# From Parquet\n",
        "df = spark.read.parquet(\"path/to/file.parquet\")\n",
        "\n",
        "# From JSON\n",
        "df = spark.read.json(\"path/to/file.json\")\n",
        "\n",
        "# From Azure Data Lake\n",
        "df = spark.read.format(\"csv\").load(\"abfss://container@storage.dfs.core.windows.net/path\")\n",
        "\n",
        "# From SQL Database\n",
        "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:sqlserver://...\").load()\n",
        "```\n",
        "\n",
        "### DataFrame Operations\n",
        "\n",
        "```python\n",
        "# View schema\n",
        "df.printSchema()\n",
        "\n",
        "# Show data\n",
        "df.show(10)\n",
        "\n",
        "# Select columns\n",
        "df.select(\"col1\", \"col2\")\n",
        "\n",
        "# Filter\n",
        "df.filter(df.col1 > 100)\n",
        "\n",
        "# Group by\n",
        "df.groupBy(\"category\").agg(sum(\"amount\"))\n",
        "\n",
        "# Join\n",
        "df1.join(df2, df1.id == df2.id, \"inner\")\n",
        "\n",
        "# Write\n",
        "df.write.format(\"parquet\").mode(\"overwrite\").save(\"output/path\")\n",
        "```\n",
        "\n",
        "### Reading from Azure Data Lake\n",
        "\n",
        "```python\n",
        "# Using abfss protocol (recommended)\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"abfss://container@storageaccount.dfs.core.windows.net/path/to/data\")\n",
        "\n",
        "# Using wasbs protocol (also works)\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"wasbs://container@storageaccount.blob.core.windows.net/path/to/data\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark Best Practices in Azure\n",
        "\n",
        "### 1. Partitioning\n",
        "\n",
        "✅ **Partition Data**: Partition by date, region, etc.\n",
        "✅ **Optimal Partition Size**: 128MB - 1GB per partition\n",
        "✅ **Avoid Too Many Partitions**: Can cause overhead\n",
        "✅ **Coalesce When Needed**: Reduce partitions if too many\n",
        "\n",
        "```python\n",
        "# Partition when writing\n",
        "df.write.partitionBy(\"year\", \"month\").parquet(\"output/path\")\n",
        "\n",
        "# Repartition if needed\n",
        "df_repartitioned = df.repartition(10)\n",
        "```\n",
        "\n",
        "### 2. Caching\n",
        "\n",
        "✅ **Cache Frequently Used Data**: Use `cache()` or `persist()`\n",
        "✅ **Unpersist When Done**: Free up memory\n",
        "✅ **Choose Storage Level**: MEMORY_ONLY, DISK_ONLY, etc.\n",
        "\n",
        "```python\n",
        "# Cache DataFrame\n",
        "df.cache()\n",
        "\n",
        "# Use cached data\n",
        "result = df.filter(...).groupBy(...)\n",
        "\n",
        "# Unpersist when done\n",
        "df.unpersist()\n",
        "```\n",
        "\n",
        "### 3. Data Formats\n",
        "\n",
        "✅ **Use Columnar Formats**: Parquet, Delta Lake\n",
        "✅ **Avoid Text Formats**: CSV, JSON for large data\n",
        "✅ **Compression**: Enable compression (snappy, gzip)\n",
        "\n",
        "```python\n",
        "# Prefer Parquet\n",
        "df.write.format(\"parquet\").save(\"path\")\n",
        "\n",
        "# Use Delta Lake for ACID transactions\n",
        "df.write.format(\"delta\").save(\"path\")\n",
        "```\n",
        "\n",
        "### 4. Resource Management\n",
        "\n",
        "✅ **Right-Size Clusters**: Match cluster size to workload\n",
        "✅ **Auto-Scaling**: Enable auto-scaling when available\n",
        "✅ **Monitor Resource Usage**: Watch CPU, memory usage\n",
        "✅ **Shut Down Idle Clusters**: Save costs\n",
        "\n",
        "### 5. Error Handling\n",
        "\n",
        "✅ **Handle Nulls**: Use `na.drop()` or `na.fill()`\n",
        "✅ **Validate Data**: Check data quality\n",
        "✅ **Log Errors**: Log failures for debugging\n",
        "✅ **Retry Logic**: Implement retry for transient failures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this module, we've covered:\n",
        "\n",
        "✅ What is Apache Spark and its importance\n",
        "✅ Spark architecture and components\n",
        "✅ Spark in Azure (Databricks, Synapse, HDInsight)\n",
        "✅ Spark operations (transformations and actions)\n",
        "✅ Working with DataFrames\n",
        "✅ Spark best practices in Azure\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Spark** is a distributed processing engine for big data\n",
        "2. **Transformations** are lazy, **Actions** trigger execution\n",
        "3. **Azure offers multiple Spark options**: Databricks, Synapse, HDInsight\n",
        "4. **DataFrames** provide high-level API for data processing\n",
        "5. **Partitioning** is crucial for performance\n",
        "6. **Use columnar formats** (Parquet, Delta) for better performance\n",
        "7. **Cache frequently used data** to avoid recomputation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Proceed to **Module 07: Azure Synapse Analytics Basics** to learn about:\n",
        "- Synapse workspace\n",
        "- SQL Pools (dedicated and serverless)\n",
        "- Spark Pools\n",
        "- Unified analytics platform\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
