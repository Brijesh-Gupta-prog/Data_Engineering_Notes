{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 04 - ETL Concepts and Data Transformation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This module covers fundamental ETL (Extract, Transform, Load) concepts, data mapping, data profiling, and transformation techniques that form the foundation of data engineering.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this module, you will understand:\n",
        "- ETL fundamentals and concepts\n",
        "- ETL vs ELT patterns\n",
        "- Data mapping techniques\n",
        "- Data profiling and quality assessment\n",
        "- Common data transformation operations\n",
        "- Best practices for ETL design\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is ETL?\n",
        "\n",
        "**ETL** stands for **Extract, Transform, Load** - a three-phase process for moving and transforming data.\n",
        "\n",
        "### The Three Phases\n",
        "\n",
        "#### 1. Extract\n",
        "- **Purpose**: Read data from source systems\n",
        "- **Activities**: \n",
        "  - Connect to source systems (databases, files, APIs)\n",
        "  - Query or read data\n",
        "  - Handle different data formats\n",
        "  - Manage connection errors\n",
        "\n",
        "#### 2. Transform\n",
        "- **Purpose**: Clean, validate, and transform data\n",
        "- **Activities**:\n",
        "  - Data cleaning (remove duplicates, handle nulls)\n",
        "  - Data validation (check data types, ranges)\n",
        "  - Data enrichment (add calculated fields)\n",
        "  - Data aggregation (summarize data)\n",
        "  - Data formatting (standardize formats)\n",
        "\n",
        "#### 3. Load\n",
        "- **Purpose**: Write transformed data to destination\n",
        "- **Activities**:\n",
        "  - Connect to target system\n",
        "  - Write data in appropriate format\n",
        "  - Handle loading errors\n",
        "  - Maintain data integrity\n",
        "\n",
        "### ETL Process Flow\n",
        "\n",
        "```\n",
        "Source Systems → Extract → Transform → Load → Target System\n",
        "     ↓             ↓          ↓          ↓          ↓\n",
        "  Databases    Read Data   Clean      Write    Data Warehouse\n",
        "  Files        Query       Validate   Insert   Data Lake\n",
        "  APIs         Fetch       Enrich     Update   Database\n",
        "```\n",
        "\n",
        "### Why ETL?\n",
        "\n",
        "- **Data Integration**: Combine data from multiple sources\n",
        "- **Data Quality**: Clean and validate data before use\n",
        "- **Data Consistency**: Standardize formats and values\n",
        "- **Performance**: Optimize data for analytics\n",
        "- **Compliance**: Ensure data meets business rules\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ETL vs ELT\n",
        "\n",
        "### ETL (Extract, Transform, Load)\n",
        "\n",
        "**Traditional Approach**: Transform data before loading to target.\n",
        "\n",
        "```\n",
        "Source → Extract → Transform (in ETL tool) → Load → Target\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- Transformations happen in ETL tool/memory\n",
        "- Uses ETL server processing power\n",
        "- Data is cleaned before loading\n",
        "- Smaller data volumes during transformation\n",
        "\n",
        "**Advantages:**\n",
        "- Data is clean when loaded\n",
        "- Reduced storage in target\n",
        "- Better for complex transformations\n",
        "- Works with limited target resources\n",
        "\n",
        "**Disadvantages:**\n",
        "- Requires powerful ETL servers\n",
        "- Slower for large data volumes\n",
        "- Limited scalability\n",
        "- Higher infrastructure costs\n",
        "\n",
        "**Use Cases:**\n",
        "- Small to medium data volumes\n",
        "- Complex transformations needed\n",
        "- Target system has limited resources\n",
        "- On-premises data warehouses\n",
        "\n",
        "### ELT (Extract, Load, Transform)\n",
        "\n",
        "**Modern Approach**: Load raw data first, then transform in target.\n",
        "\n",
        "```\n",
        "Source → Extract → Load → Transform (in target) → Analytics\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- Transformations happen in target system\n",
        "- Uses target system processing power (cloud scale)\n",
        "- Raw data loaded first\n",
        "- Leverages cloud/data lake capabilities\n",
        "\n",
        "**Advantages:**\n",
        "- Leverages target system power (cloud)\n",
        "- Faster for large data volumes\n",
        "- More scalable\n",
        "- Lower infrastructure costs\n",
        "- Preserves raw data for reprocessing\n",
        "\n",
        "**Disadvantages:**\n",
        "- Requires powerful target system\n",
        "- Raw data takes more storage\n",
        "- Transformations may be slower if target is busy\n",
        "\n",
        "**Use Cases:**\n",
        "- Large data volumes (big data)\n",
        "- Cloud data warehouses/lakes\n",
        "- Need to preserve raw data\n",
        "- Target system has powerful compute (Synapse, Databricks)\n",
        "\n",
        "### When to Choose Which?\n",
        "\n",
        "**Choose ETL when:**\n",
        "- Small to medium data volumes\n",
        "- Complex transformations needed\n",
        "- Target system has limited resources\n",
        "- On-premises environment\n",
        "\n",
        "**Choose ELT when:**\n",
        "- Large data volumes (big data)\n",
        "- Cloud-based target systems\n",
        "- Need to preserve raw data\n",
        "- Target has powerful compute (Synapse, Databricks, Spark)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Mapping\n",
        "\n",
        "**Data Mapping** is the process of defining how data from source systems maps to target systems, including field mappings, transformations, and business rules.\n",
        "\n",
        "### Types of Data Mapping\n",
        "\n",
        "#### 1. Direct Mapping\n",
        "- Source field maps directly to target field\n",
        "- No transformation needed\n",
        "- Example: `CustomerID → Customer_ID`\n",
        "\n",
        "#### 2. Calculated Mapping\n",
        "- Target field is calculated from source fields\n",
        "- Uses formulas or expressions\n",
        "- Example: `FullName = FirstName + ' ' + LastName`\n",
        "\n",
        "#### 3. Lookup Mapping\n",
        "- Target field value comes from lookup table\n",
        "- Reference data mapping\n",
        "- Example: `CountryCode → CountryName` (from lookup table)\n",
        "\n",
        "#### 4. Conditional Mapping\n",
        "- Target field value depends on conditions\n",
        "- Uses IF-THEN-ELSE logic\n",
        "- Example: `Status = IF(Amount > 1000, 'High', 'Low')`\n",
        "\n",
        "#### 5. Aggregation Mapping\n",
        "- Target field is aggregated from multiple source rows\n",
        "- SUM, COUNT, AVG, MIN, MAX\n",
        "- Example: `TotalSales = SUM(SalesAmount)`\n",
        "\n",
        "### Data Mapping Document\n",
        "\n",
        "A data mapping document typically includes:\n",
        "\n",
        "| Source Field | Source Data Type | Target Field | Target Data Type | Transformation Rule | Notes |\n",
        "|-------------|------------------|--------------|------------------|---------------------|-------|\n",
        "| CustID | INT | Customer_ID | INT | Direct | Primary key |\n",
        "| FName | VARCHAR(50) | First_Name | VARCHAR(100) | Direct | Truncate if > 100 |\n",
        "| LName | VARCHAR(50) | Last_Name | VARCHAR(100) | Direct | Truncate if > 100 |\n",
        "| FullName | - | Full_Name | VARCHAR(200) | FName + ' ' + LName | Calculated |\n",
        "| DOB | DATE | Birth_Date | DATE | Direct | Format: YYYY-MM-DD |\n",
        "| Salary | DECIMAL(10,2) | Annual_Salary | DECIMAL(12,2) | Direct | Scale change |\n",
        "\n",
        "### Best Practices for Data Mapping\n",
        "\n",
        "✅ **Document Everything**: Maintain detailed mapping documentation\n",
        "✅ **Validate Mappings**: Test mappings with sample data\n",
        "✅ **Handle Nulls**: Define how null values are handled\n",
        "✅ **Data Type Compatibility**: Ensure compatible data types\n",
        "✅ **Business Rules**: Document all business logic\n",
        "✅ **Version Control**: Track changes to mappings\n",
        "✅ **Review with Stakeholders**: Get business approval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Profiling\n",
        "\n",
        "**Data Profiling** is the process of examining data to understand its structure, quality, and characteristics before designing ETL processes.\n",
        "\n",
        "### Why Data Profiling?\n",
        "\n",
        "- **Understand Data**: Know what you're working with\n",
        "- **Identify Issues**: Find data quality problems early\n",
        "- **Design ETL**: Make informed decisions about transformations\n",
        "- **Estimate Effort**: Understand complexity and time needed\n",
        "- **Validate Assumptions**: Verify data matches expectations\n",
        "\n",
        "### Data Profiling Activities\n",
        "\n",
        "#### 1. Structure Analysis\n",
        "- **Column Count**: Number of columns/fields\n",
        "- **Row Count**: Number of records\n",
        "- **Data Types**: Types of each field\n",
        "- **Schema**: Overall structure\n",
        "\n",
        "#### 2. Content Analysis\n",
        "- **Sample Data**: View sample records\n",
        "- **Value Patterns**: Common patterns in data\n",
        "- **Uniqueness**: Identify unique values\n",
        "- **Distributions**: Value distributions\n",
        "\n",
        "#### 3. Quality Analysis\n",
        "- **Null Values**: Count and percentage of nulls\n",
        "- **Duplicates**: Identify duplicate records\n",
        "- **Data Ranges**: Min, max, average values\n",
        "- **Outliers**: Unusual values\n",
        "- **Format Consistency**: Consistent formats?\n",
        "\n",
        "#### 4. Relationship Analysis\n",
        "- **Foreign Keys**: Relationships between tables\n",
        "- **Referential Integrity**: Valid references?\n",
        "- **Dependencies**: Data dependencies\n",
        "\n",
        "### Data Profiling Metrics\n",
        "\n",
        "| Metric | Description | Example |\n",
        "|--------|-------------|---------|\n",
        "| **Completeness** | Percentage of non-null values | 95% complete |\n",
        "| **Uniqueness** | Percentage of unique values | 80% unique |\n",
        "| **Validity** | Percentage of valid values | 90% valid |\n",
        "| **Consistency** | Percentage of consistent values | 85% consistent |\n",
        "| **Accuracy** | Percentage of accurate values | 92% accurate |\n",
        "\n",
        "### Data Profiling Tools\n",
        "\n",
        "- **Azure Data Factory**: Built-in data profiling\n",
        "- **SQL Queries**: Manual profiling with SQL\n",
        "- **Python/Pandas**: Programmatic profiling\n",
        "- **Azure Databricks**: Spark-based profiling\n",
        "- **Third-party Tools**: Talend, Informatica, etc.\n",
        "\n",
        "### Example: Data Profiling SQL\n",
        "\n",
        "```sql\n",
        "-- Row count\n",
        "SELECT COUNT(*) as TotalRows FROM SourceTable;\n",
        "\n",
        "-- Null analysis\n",
        "SELECT \n",
        "    COUNT(*) as TotalRows,\n",
        "    SUM(CASE WHEN Column1 IS NULL THEN 1 ELSE 0 END) as NullCount,\n",
        "    AVG(CASE WHEN Column1 IS NULL THEN 0 ELSE 1 END) * 100 as Completeness\n",
        "FROM SourceTable;\n",
        "\n",
        "-- Value distribution\n",
        "SELECT Column1, COUNT(*) as Frequency\n",
        "FROM SourceTable\n",
        "GROUP BY Column1\n",
        "ORDER BY Frequency DESC;\n",
        "\n",
        "-- Data range\n",
        "SELECT \n",
        "    MIN(Amount) as MinAmount,\n",
        "    MAX(Amount) as MaxAmount,\n",
        "    AVG(Amount) as AvgAmount\n",
        "FROM SourceTable;\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Data Transformations\n",
        "\n",
        "### 1. Data Cleaning\n",
        "\n",
        "**Purpose**: Remove or fix data quality issues.\n",
        "\n",
        "**Operations:**\n",
        "- **Remove Duplicates**: Eliminate duplicate records\n",
        "- **Handle Nulls**: Fill nulls with defaults or remove\n",
        "- **Trim Whitespace**: Remove leading/trailing spaces\n",
        "- **Standardize Formats**: Consistent date, phone formats\n",
        "- **Remove Invalid Characters**: Clean special characters\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Fill nulls\n",
        "df['Column1'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Trim whitespace\n",
        "df['Name'] = df['Name'].str.strip()\n",
        "```\n",
        "\n",
        "### 2. Data Type Conversion\n",
        "\n",
        "**Purpose**: Convert data to appropriate types.\n",
        "\n",
        "**Operations:**\n",
        "- **String to Number**: Convert text to numeric\n",
        "- **String to Date**: Parse date strings\n",
        "- **Number to String**: Convert numbers to text\n",
        "- **Type Casting**: Change data types\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# String to number\n",
        "df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
        "\n",
        "# String to date\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
        "```\n",
        "\n",
        "### 3. Data Enrichment\n",
        "\n",
        "**Purpose**: Add calculated or derived fields.\n",
        "\n",
        "**Operations:**\n",
        "- **Calculated Fields**: Add computed columns\n",
        "- **Lookups**: Join with reference data\n",
        "- **Concatenation**: Combine fields\n",
        "- **Derived Values**: Calculate from existing fields\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Calculated field\n",
        "df['FullName'] = df['FirstName'] + ' ' + df['LastName']\n",
        "\n",
        "# Derived value\n",
        "df['Age'] = (pd.Timestamp.now() - df['BirthDate']).dt.days // 365\n",
        "```\n",
        "\n",
        "### 4. Data Filtering\n",
        "\n",
        "**Purpose**: Select subset of data based on conditions.\n",
        "\n",
        "**Operations:**\n",
        "- **Row Filtering**: Filter rows by conditions\n",
        "- **Column Selection**: Select specific columns\n",
        "- **Conditional Logic**: IF-THEN-ELSE filtering\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Filter rows\n",
        "df_filtered = df[df['Amount'] > 1000]\n",
        "\n",
        "# Select columns\n",
        "df_selected = df[['CustomerID', 'Name', 'Amount']]\n",
        "```\n",
        "\n",
        "### 5. Data Aggregation\n",
        "\n",
        "**Purpose**: Summarize data by groups.\n",
        "\n",
        "**Operations:**\n",
        "- **Group By**: Group data by fields\n",
        "- **Aggregate Functions**: SUM, COUNT, AVG, MIN, MAX\n",
        "- **Pivoting**: Reshape data\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Aggregation\n",
        "df_agg = df.groupby('Region').agg({\n",
        "    'Sales': 'sum',\n",
        "    'Orders': 'count',\n",
        "    'Amount': 'mean'\n",
        "})\n",
        "```\n",
        "\n",
        "### 6. Data Joining\n",
        "\n",
        "**Purpose**: Combine data from multiple sources.\n",
        "\n",
        "**Operations:**\n",
        "- **Inner Join**: Matching records only\n",
        "- **Left Join**: All left records + matching right\n",
        "- **Right Join**: All right records + matching left\n",
        "- **Full Outer Join**: All records from both\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Join\n",
        "df_joined = df1.merge(df2, on='CustomerID', how='left')\n",
        "```\n",
        "\n",
        "### 7. Data Splitting\n",
        "\n",
        "**Purpose**: Split data into multiple outputs.\n",
        "\n",
        "**Operations:**\n",
        "- **Split by Condition**: Route data based on conditions\n",
        "- **Split by Value**: Separate by field values\n",
        "- **Multiple Outputs**: Send to different destinations\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Split by condition\n",
        "df_high = df[df['Amount'] > 1000]\n",
        "df_low = df[df['Amount'] <= 1000]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ETL Design Best Practices\n",
        "\n",
        "### 1. Incremental Loading\n",
        "\n",
        "**Load only new or changed data**, not full datasets.\n",
        "\n",
        "**Benefits:**\n",
        "- Faster execution\n",
        "- Lower resource usage\n",
        "- Reduced network traffic\n",
        "- Lower costs\n",
        "\n",
        "**Techniques:**\n",
        "- **Timestamp-based**: Load records modified after last run\n",
        "- **Change Data Capture (CDC)**: Track changes at source\n",
        "- **Watermarking**: Track last processed record\n",
        "- **Hash Comparison**: Compare data hashes\n",
        "\n",
        "### 2. Error Handling\n",
        "\n",
        "**Handle errors gracefully** to ensure pipeline reliability.\n",
        "\n",
        "**Strategies:**\n",
        "- **Try-Catch Blocks**: Catch and handle exceptions\n",
        "- **Error Logging**: Log all errors for debugging\n",
        "- **Retry Logic**: Retry failed operations\n",
        "- **Dead Letter Queue**: Store failed records\n",
        "- **Notifications**: Alert on critical errors\n",
        "\n",
        "### 3. Idempotency\n",
        "\n",
        "**Design ETL to be idempotent** - running multiple times produces same result.\n",
        "\n",
        "**Techniques:**\n",
        "- **Upsert Operations**: Insert or update\n",
        "- **Delete Before Insert**: Clear target before load\n",
        "- **Unique Constraints**: Prevent duplicates\n",
        "- **Transaction Management**: Atomic operations\n",
        "\n",
        "### 4. Performance Optimization\n",
        "\n",
        "**Optimize ETL for speed and efficiency.**\n",
        "\n",
        "**Techniques:**\n",
        "- **Parallel Processing**: Process multiple files/partitions\n",
        "- **Partitioning**: Partition data for parallel processing\n",
        "- **Indexing**: Create indexes on key columns\n",
        "- **Batch Processing**: Process in batches\n",
        "- **Resource Scaling**: Scale resources as needed\n",
        "\n",
        "### 5. Data Validation\n",
        "\n",
        "**Validate data at each stage** of ETL process.\n",
        "\n",
        "**Validations:**\n",
        "- **Row Count Checks**: Verify expected row counts\n",
        "- **Data Type Validation**: Check data types\n",
        "- **Range Checks**: Validate value ranges\n",
        "- **Referential Integrity**: Check foreign keys\n",
        "- **Business Rules**: Validate business logic\n",
        "\n",
        "### 6. Monitoring and Logging\n",
        "\n",
        "**Monitor ETL execution** for visibility and troubleshooting.\n",
        "\n",
        "**Monitor:**\n",
        "- **Execution Status**: Success/failure\n",
        "- **Execution Time**: Duration of each step\n",
        "- **Data Volumes**: Records processed\n",
        "- **Error Rates**: Number of errors\n",
        "- **Resource Usage**: CPU, memory, network\n",
        "\n",
        "### 7. Documentation\n",
        "\n",
        "**Document ETL processes** for maintainability.\n",
        "\n",
        "**Document:**\n",
        "- **Data Mapping**: Source to target mappings\n",
        "- **Transformation Logic**: Business rules\n",
        "- **Dependencies**: Data and process dependencies\n",
        "- **Schedule**: When ETL runs\n",
        "- **Contacts**: Who to contact for issues\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ETL Patterns\n",
        "\n",
        "### Pattern 1: Full Load\n",
        "\n",
        "**Load entire dataset every time.**\n",
        "\n",
        "```\n",
        "Source → Extract All → Transform → Load (Replace) → Target\n",
        "```\n",
        "\n",
        "**Use When:**\n",
        "- Small datasets\n",
        "- Data changes frequently\n",
        "- Simple requirements\n",
        "\n",
        "**Pros:** Simple, always current\n",
        "**Cons:** Slow for large data, high resource usage\n",
        "\n",
        "### Pattern 2: Incremental Load\n",
        "\n",
        "**Load only new or changed data.**\n",
        "\n",
        "```\n",
        "Source → Extract Changed → Transform → Load (Append/Update) → Target\n",
        "```\n",
        "\n",
        "**Use When:**\n",
        "- Large datasets\n",
        "- Only new/changed data needed\n",
        "- Performance is important\n",
        "\n",
        "**Pros:** Fast, efficient, lower resource usage\n",
        "**Cons:** More complex, need change tracking\n",
        "\n",
        "### Pattern 3: Change Data Capture (CDC)\n",
        "\n",
        "**Capture and process only changed data.**\n",
        "\n",
        "```\n",
        "Source → CDC → Changed Records → Transform → Load → Target\n",
        "```\n",
        "\n",
        "**Use When:**\n",
        "- Real-time or near-real-time needs\n",
        "- Source supports CDC\n",
        "- Need to track all changes\n",
        "\n",
        "**Pros:** Real-time, efficient, tracks history\n",
        "**Cons:** Complex, requires CDC support\n",
        "\n",
        "### Pattern 4: Slowly Changing Dimensions (SCD)\n",
        "\n",
        "**Handle dimension data that changes over time.**\n",
        "\n",
        "**SCD Type 1**: Overwrite old values\n",
        "**SCD Type 2**: Keep history with versioning\n",
        "**SCD Type 3**: Keep limited history\n",
        "\n",
        "**Use When:**\n",
        "- Dimension data changes\n",
        "- Need historical tracking\n",
        "- Data warehousing scenarios\n",
        "\n",
        "### Pattern 5: Staging Area\n",
        "\n",
        "**Load to staging first, then to final target.**\n",
        "\n",
        "```\n",
        "Source → Extract → Load to Staging → Transform → Load to Target\n",
        "```\n",
        "\n",
        "**Use When:**\n",
        "- Complex transformations\n",
        "- Need to validate before final load\n",
        "- Multiple target systems\n",
        "\n",
        "**Pros:** Validation, rollback capability, flexibility\n",
        "**Cons:** Extra storage, additional step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this module, we've covered:\n",
        "\n",
        "✅ ETL fundamentals (Extract, Transform, Load)\n",
        "✅ ETL vs ELT patterns and when to use each\n",
        "✅ Data mapping techniques and documentation\n",
        "✅ Data profiling and quality assessment\n",
        "✅ Common data transformation operations\n",
        "✅ ETL design best practices\n",
        "✅ ETL patterns (Full Load, Incremental, CDC, SCD, Staging)\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **ETL** is Extract, Transform, Load - fundamental data engineering process\n",
        "2. **ETL vs ELT**: Choose based on data volume and target system capabilities\n",
        "3. **Data Mapping** defines how source data maps to target\n",
        "4. **Data Profiling** helps understand data before designing ETL\n",
        "5. **Transformations** clean, enrich, and prepare data\n",
        "6. **Best Practices** ensure reliable, maintainable ETL processes\n",
        "7. **ETL Patterns** provide reusable solutions for common scenarios\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Proceed to **Module 05: Azure Data Factory Basics** to learn about:\n",
        "- Azure Data Factory components\n",
        "- Linked Services and Datasets\n",
        "- Pipelines and Activities\n",
        "- Creating ETL pipelines in ADF\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
