{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 05 - Azure Data Factory Basics\n",
        "\n",
        "## Overview\n",
        "\n",
        "Azure Data Factory (ADF) is a cloud-based ETL/ELT service for creating data-driven workflows. This module covers the fundamental components of ADF and how to build data pipelines.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this module, you will understand:\n",
        "- What is Azure Data Factory and its purpose\n",
        "- Linked Services - connecting to data sources\n",
        "- Datasets - representing data structures\n",
        "- Pipelines - orchestrating data workflows\n",
        "- Activities - individual tasks in pipelines\n",
        "- Source and Sink concepts\n",
        "- Creating basic data pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Azure Data Factory?\n",
        "\n",
        "**Azure Data Factory (ADF)** is a cloud-based data integration service that allows you to create, schedule, and orchestrate data-driven workflows (pipelines) to move and transform data.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Visual Pipeline Designer**: Drag-and-drop interface for building pipelines\n",
        "- **90+ Built-in Connectors**: Connect to various data sources\n",
        "- **Code-Free ETL**: Build pipelines without writing code\n",
        "- **Scheduling**: Schedule pipelines to run automatically\n",
        "- **Monitoring**: Track pipeline execution and performance\n",
        "- **Hybrid Data Movement**: Move data from on-premises to cloud\n",
        "- **Data Transformation**: Transform data using various activities\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "✅ **Data Migration**: Move data from on-premises to cloud\n",
        "✅ **ETL/ELT Workflows**: Extract, transform, and load data\n",
        "✅ **Data Integration**: Integrate data from multiple sources\n",
        "✅ **Scheduled Data Loads**: Automate daily/weekly data refreshes\n",
        "✅ **Data Orchestration**: Coordinate multiple data processes\n",
        "\n",
        "### ADF Architecture\n",
        "\n",
        "```\n",
        "Data Factory\n",
        "├── Linked Services (Connections)\n",
        "├── Datasets (Data Structures)\n",
        "├── Pipelines (Workflows)\n",
        "│   └── Activities (Tasks)\n",
        "└── Triggers (Scheduling)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linked Services\n",
        "\n",
        "**Linked Services** define connection information to external data sources or compute services. Think of them as connection strings or connection configurations.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- **Store Connection Details**: Connection strings, credentials, endpoints\n",
        "- **Reusability**: Use same connection across multiple pipelines\n",
        "- **Security**: Store credentials securely (Azure Key Vault)\n",
        "- **Abstraction**: Hide connection details from pipelines\n",
        "\n",
        "### Common Linked Service Types\n",
        "\n",
        "#### Storage Linked Services\n",
        "- **Azure Blob Storage**: Connect to blob storage\n",
        "- **Azure Data Lake Storage Gen2**: Connect to ADLS Gen2\n",
        "- **Azure Files**: Connect to file shares\n",
        "- **Amazon S3**: Connect to AWS S3\n",
        "\n",
        "#### Database Linked Services\n",
        "- **Azure SQL Database**: Connect to SQL Database\n",
        "- **Azure Synapse Analytics**: Connect to Synapse\n",
        "- **SQL Server**: Connect to on-premises SQL Server\n",
        "- **Oracle/MySQL/PostgreSQL**: Connect to various databases\n",
        "\n",
        "#### Compute Linked Services\n",
        "- **Azure Databricks**: Connect to Databricks clusters\n",
        "- **Azure HDInsight**: Connect to HDInsight clusters\n",
        "- **Azure Batch**: Connect to Batch compute\n",
        "\n",
        "### Linked Service Example\n",
        "\n",
        "**Azure Blob Storage Linked Service:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"AzureBlobStorage1\",\n",
        "  \"type\": \"AzureBlobStorage\",\n",
        "  \"typeProperties\": {\n",
        "    \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=...\",\n",
        "    \"accountKey\": {\n",
        "      \"type\": \"AzureKeyVaultSecret\",\n",
        "      \"store\": {\n",
        "        \"referenceName\": \"AzureKeyVault1\"\n",
        "      },\n",
        "      \"secretName\": \"storageAccountKey\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "✅ **Use Azure Key Vault**: Store sensitive credentials in Key Vault\n",
        "✅ **Naming Convention**: Use descriptive names (e.g., `LS_SQLServer_Production`)\n",
        "✅ **Parameterize**: Use parameters for different environments\n",
        "✅ **Reuse**: Create linked services that can be reused across pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n",
        "\n",
        "**Datasets** represent data structures within data stores. They point to the data you want to use in your activities as inputs or outputs.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- **Define Data Structure**: Specify schema, format, location\n",
        "- **Reference Data**: Point to specific data in linked services\n",
        "- **Reusability**: Use same dataset definition across activities\n",
        "- **Schema Definition**: Define columns, data types, constraints\n",
        "\n",
        "### Dataset Components\n",
        "\n",
        "1. **Linked Service Reference**: Which data store to connect to\n",
        "2. **Structure/Schema**: Column names and data types\n",
        "3. **Location/Path**: Where the data is located\n",
        "4. **Format**: File format (CSV, JSON, Parquet, etc.)\n",
        "5. **Properties**: Additional settings (compression, encoding)\n",
        "\n",
        "### Common Dataset Types\n",
        "\n",
        "#### File-Based Datasets\n",
        "- **DelimitedText**: CSV, TSV files\n",
        "- **Json**: JSON files\n",
        "- **Parquet**: Parquet files\n",
        "- **Avro**: Avro files\n",
        "- **Excel**: Excel files\n",
        "\n",
        "#### Database Datasets\n",
        "- **AzureSqlTable**: SQL Database tables\n",
        "- **SqlServerTable**: SQL Server tables\n",
        "- **OracleTable**: Oracle tables\n",
        "\n",
        "### Dataset Example\n",
        "\n",
        "**CSV Dataset:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"SalesDataCSV\",\n",
        "  \"type\": \"DelimitedText\",\n",
        "  \"linkedServiceName\": {\n",
        "    \"referenceName\": \"AzureBlobStorage1\",\n",
        "    \"type\": \"LinkedServiceReference\"\n",
        "  },\n",
        "  \"schema\": [\n",
        "    {\n",
        "      \"name\": \"CustomerID\",\n",
        "      \"type\": \"Int32\"\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"SalesAmount\",\n",
        "      \"type\": \"Decimal\"\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"SaleDate\",\n",
        "      \"type\": \"DateTime\"\n",
        "    }\n",
        "  ],\n",
        "  \"typeProperties\": {\n",
        "    \"location\": {\n",
        "      \"type\": \"AzureBlobStorageLocation\",\n",
        "      \"container\": \"raw-data\",\n",
        "      \"folderPath\": \"sales/2024\"\n",
        "    },\n",
        "    \"columnDelimiter\": \",\",\n",
        "    \"firstRowAsHeader\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "✅ **Parameterize Paths**: Use parameters for dynamic paths (dates, partitions)\n",
        "✅ **Define Schema**: Explicitly define schema when known\n",
        "✅ **Use Descriptive Names**: Clear, meaningful dataset names\n",
        "✅ **Reuse**: Create reusable dataset definitions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipelines\n",
        "\n",
        "**Pipelines** are logical groupings of activities that together perform a task. A pipeline is a workflow that orchestrates data movement and transformation.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- **Orchestrate Workflows**: Coordinate multiple activities\n",
        "- **Define Dependencies**: Set activity execution order\n",
        "- **Parameterize**: Accept parameters for flexibility\n",
        "- **Schedule**: Can be triggered on schedule or event\n",
        "\n",
        "### Pipeline Components\n",
        "\n",
        "1. **Activities**: Individual tasks (copy, transform, etc.)\n",
        "2. **Parameters**: Input parameters for flexibility\n",
        "3. **Variables**: Internal variables for pipeline logic\n",
        "4. **Dependencies**: Activity execution order\n",
        "5. **Error Handling**: How to handle failures\n",
        "\n",
        "### Pipeline Example Flow\n",
        "\n",
        "```\n",
        "Pipeline: Load Sales Data\n",
        "├── Activity 1: Copy from SQL Server to Blob Storage\n",
        "├── Activity 2: Transform data (Data Flow)\n",
        "└── Activity 3: Copy from Blob Storage to Synapse\n",
        "```\n",
        "\n",
        "### Pipeline Parameters\n",
        "\n",
        "Pipelines can accept parameters for dynamic behavior:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"LoadSalesData\",\n",
        "  \"parameters\": {\n",
        "    \"sourceTable\": {\n",
        "      \"type\": \"String\"\n",
        "    },\n",
        "    \"targetFolder\": {\n",
        "      \"type\": \"String\"\n",
        "    },\n",
        "    \"loadDate\": {\n",
        "      \"type\": \"String\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "✅ **Single Responsibility**: Each pipeline should do one thing well\n",
        "✅ **Parameterize**: Use parameters for flexibility\n",
        "✅ **Error Handling**: Implement proper error handling\n",
        "✅ **Logging**: Add logging for debugging\n",
        "✅ **Naming**: Use descriptive pipeline names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activities\n",
        "\n",
        "**Activities** are individual tasks within a pipeline. Each activity performs a specific operation on data.\n",
        "\n",
        "### Activity Types\n",
        "\n",
        "#### 1. Data Movement Activities\n",
        "\n",
        "**Copy Activity**: Copy data from source to sink\n",
        "- Most common activity\n",
        "- Supports 90+ data sources\n",
        "- Handles schema mapping\n",
        "- Supports transformations during copy\n",
        "\n",
        "#### 2. Data Transformation Activities\n",
        "\n",
        "**Data Flow Activity**: Transform data using visual data flows\n",
        "- Code-free transformations\n",
        "- Spark-based execution\n",
        "- Supports complex transformations\n",
        "\n",
        "**Stored Procedure Activity**: Execute stored procedures\n",
        "- Run SQL stored procedures\n",
        "- Pass parameters\n",
        "- Get return values\n",
        "\n",
        "**Lookup Activity**: Look up values from datasets\n",
        "- Get single value or row\n",
        "- Use in conditional logic\n",
        "- Reference data lookups\n",
        "\n",
        "#### 3. Control Flow Activities\n",
        "\n",
        "**If Condition Activity**: Conditional branching\n",
        "- Execute activities based on conditions\n",
        "- IF-THEN-ELSE logic\n",
        "\n",
        "**ForEach Activity**: Loop through items\n",
        "- Iterate over arrays\n",
        "- Execute activities for each item\n",
        "- Parallel or sequential execution\n",
        "\n",
        "**Wait Activity**: Pause pipeline execution\n",
        "- Wait for specified duration\n",
        "- Wait for external events\n",
        "\n",
        "**Until Activity**: Loop until condition is met\n",
        "- Retry logic\n",
        "- Polling scenarios\n",
        "\n",
        "### Copy Activity Example\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"CopySalesData\",\n",
        "  \"type\": \"Copy\",\n",
        "  \"inputs\": [\n",
        "    {\n",
        "      \"referenceName\": \"SourceDataset\",\n",
        "      \"type\": \"DatasetReference\"\n",
        "    }\n",
        "  ],\n",
        "  \"outputs\": [\n",
        "    {\n",
        "      \"referenceName\": \"SinkDataset\",\n",
        "      \"type\": \"DatasetReference\"\n",
        "    }\n",
        "  ],\n",
        "  \"typeProperties\": {\n",
        "    \"source\": {\n",
        "      \"type\": \"DelimitedTextSource\",\n",
        "      \"skipLineCount\": 1\n",
        "    },\n",
        "    \"sink\": {\n",
        "      \"type\": \"DelimitedTextSink\",\n",
        "      \"writeBehavior\": \"append\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Source and Sink\n",
        "\n",
        "### Source\n",
        "\n",
        "**Source** is where data comes from in a data movement activity (typically Copy Activity).\n",
        "\n",
        "**Source Properties:**\n",
        "- **Dataset Reference**: Points to source dataset\n",
        "- **Query**: SQL query for databases\n",
        "- **File Path**: Path to files in storage\n",
        "- **Filter**: Filter data at source\n",
        "\n",
        "**Common Source Types:**\n",
        "- **DelimitedTextSource**: CSV, TSV files\n",
        "- **JsonSource**: JSON files\n",
        "- **SqlSource**: SQL databases\n",
        "- **BlobSource**: Blob storage\n",
        "- **ParquetSource**: Parquet files\n",
        "\n",
        "### Sink\n",
        "\n",
        "**Sink** is where data goes to in a data movement activity.\n",
        "\n",
        "**Sink Properties:**\n",
        "- **Dataset Reference**: Points to sink dataset\n",
        "- **Write Behavior**: Append, Upsert, or Replace\n",
        "- **Pre-copy Script**: SQL script to run before copy\n",
        "- **Table Option**: Auto-create table if not exists\n",
        "\n",
        "**Common Sink Types:**\n",
        "- **DelimitedTextSink**: CSV, TSV files\n",
        "- **JsonSink**: JSON files\n",
        "- **SqlSink**: SQL databases\n",
        "- **BlobSink**: Blob storage\n",
        "- **ParquetSink**: Parquet files\n",
        "\n",
        "### Source and Sink Example\n",
        "\n",
        "```\n",
        "Source (SQL Server)\n",
        "├── Dataset: SQLServerTable\n",
        "├── Query: SELECT * FROM Sales WHERE SaleDate >= @StartDate\n",
        "└── Connection: Linked Service to SQL Server\n",
        "\n",
        "    ↓ Copy Activity ↓\n",
        "\n",
        "Sink (Azure Data Lake)\n",
        "├── Dataset: DelimitedText\n",
        "├── Path: /raw/sales/2024/01/\n",
        "├── Format: CSV\n",
        "└── Connection: Linked Service to ADLS Gen2\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "✅ **Filter at Source**: Use source queries to filter data early\n",
        "✅ **Partitioning**: Use partitioned sinks for large data\n",
        "✅ **Compression**: Compress data during transfer\n",
        "✅ **Parallel Copy**: Enable parallel copy for performance\n",
        "✅ **Error Handling**: Handle source/sink errors gracefully\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Simple Pipeline\n",
        "\n",
        "### Step-by-Step Process\n",
        "\n",
        "#### Step 1: Create Linked Services\n",
        "1. Create linked service for source (e.g., Azure Blob Storage)\n",
        "2. Create linked service for sink (e.g., Azure SQL Database)\n",
        "3. Configure connection details and credentials\n",
        "\n",
        "#### Step 2: Create Datasets\n",
        "1. Create source dataset (points to source data)\n",
        "2. Create sink dataset (points to destination)\n",
        "3. Define schema and format\n",
        "\n",
        "#### Step 3: Create Pipeline\n",
        "1. Create new pipeline\n",
        "2. Add Copy Activity\n",
        "3. Configure source and sink\n",
        "4. Set up dependencies\n",
        "\n",
        "#### Step 4: Configure Triggers\n",
        "1. Create schedule trigger (e.g., daily at 2 AM)\n",
        "2. Attach trigger to pipeline\n",
        "3. Set parameters if needed\n",
        "\n",
        "#### Step 5: Publish and Monitor\n",
        "1. Publish pipeline to Data Factory\n",
        "2. Trigger pipeline manually or wait for schedule\n",
        "3. Monitor execution in Monitor hub\n",
        "\n",
        "### Example: Simple Copy Pipeline\n",
        "\n",
        "**Scenario**: Copy CSV file from Blob Storage to SQL Database\n",
        "\n",
        "```\n",
        "1. Linked Service: LS_BlobStorage\n",
        "   └── Connection to Azure Blob Storage\n",
        "\n",
        "2. Linked Service: LS_SQLDatabase\n",
        "   └── Connection to Azure SQL Database\n",
        "\n",
        "3. Dataset: DS_SalesCSV (Source)\n",
        "   └── Points to: container/sales/data.csv\n",
        "   └── Format: CSV\n",
        "\n",
        "4. Dataset: DS_SalesTable (Sink)\n",
        "   └── Points to: dbo.Sales table\n",
        "   └── Format: SQL Table\n",
        "\n",
        "5. Pipeline: PL_CopySalesData\n",
        "   └── Activity: Copy Activity\n",
        "       ├── Source: DS_SalesCSV\n",
        "       └── Sink: DS_SalesTable\n",
        "\n",
        "6. Trigger: TR_DailyAt2AM\n",
        "   └── Schedule: Daily at 2:00 AM\n",
        "   └── Pipeline: PL_CopySalesData\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Triggers\n",
        "\n",
        "**Triggers** determine when a pipeline execution should be kicked off. They can be scheduled or event-based.\n",
        "\n",
        "### Trigger Types\n",
        "\n",
        "#### 1. Schedule Trigger\n",
        "- **Purpose**: Run pipeline on a schedule\n",
        "- **Examples**: Daily, weekly, monthly, custom cron expressions\n",
        "- **Use Cases**: Scheduled data loads, regular ETL jobs\n",
        "\n",
        "#### 2. Tumbling Window Trigger\n",
        "- **Purpose**: Run pipeline at regular intervals\n",
        "- **Examples**: Every hour, every 15 minutes\n",
        "- **Use Cases**: Periodic data processing\n",
        "\n",
        "#### 3. Event-Based Trigger\n",
        "- **Purpose**: Trigger on events (file arrival, blob creation)\n",
        "- **Examples**: File added to storage, message in queue\n",
        "- **Use Cases**: Process files as they arrive\n",
        "\n",
        "#### 4. Manual Trigger\n",
        "- **Purpose**: Trigger pipeline manually\n",
        "- **Examples**: On-demand execution\n",
        "- **Use Cases**: Testing, ad-hoc processing\n",
        "\n",
        "### Trigger Example\n",
        "\n",
        "**Schedule Trigger:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DailyTrigger\",\n",
        "  \"type\": \"ScheduleTrigger\",\n",
        "  \"typeProperties\": {\n",
        "    \"recurrence\": {\n",
        "      \"frequency\": \"Day\",\n",
        "      \"interval\": 1,\n",
        "      \"startTime\": \"2024-01-01T02:00:00Z\",\n",
        "      \"timeZone\": \"UTC\"\n",
        "    }\n",
        "  },\n",
        "  \"pipelines\": [\n",
        "    {\n",
        "      \"pipelineReference\": {\n",
        "        \"referenceName\": \"LoadSalesData\"\n",
        "      },\n",
        "      \"parameters\": {\n",
        "        \"loadDate\": \"@trigger().scheduledTime\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "✅ **Use Parameters**: Pass dynamic values via trigger parameters\n",
        "✅ **Time Zones**: Be aware of time zone settings\n",
        "✅ **Error Handling**: Configure retry policies\n",
        "✅ **Monitoring**: Monitor trigger executions\n",
        "✅ **Naming**: Use descriptive trigger names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this module, we've covered:\n",
        "\n",
        "✅ What is Azure Data Factory and its purpose\n",
        "✅ Linked Services - connection configurations\n",
        "✅ Datasets - data structure definitions\n",
        "✅ Pipelines - workflow orchestration\n",
        "✅ Activities - individual tasks\n",
        "✅ Source and Sink concepts\n",
        "✅ Building simple pipelines\n",
        "✅ Triggers - scheduling and event-based execution\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Linked Services** define connections to data sources and compute\n",
        "2. **Datasets** represent data structures and locations\n",
        "3. **Pipelines** orchestrate workflows of activities\n",
        "4. **Activities** perform individual tasks (copy, transform, etc.)\n",
        "5. **Source** is where data comes from, **Sink** is where it goes\n",
        "6. **Triggers** determine when pipelines run\n",
        "7. **ADF** provides visual, code-free ETL capabilities\n",
        "\n",
        "### Component Hierarchy\n",
        "\n",
        "```\n",
        "Data Factory\n",
        "├── Linked Services (Connections)\n",
        "├── Datasets (Data Definitions)\n",
        "├── Pipelines (Workflows)\n",
        "│   ├── Activities (Tasks)\n",
        "│   │   ├── Source (Input)\n",
        "│   │   └── Sink (Output)\n",
        "│   └── Parameters & Variables\n",
        "└── Triggers (Scheduling)\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Proceed to **Module 06: Spark Basics in Azure** to learn about:\n",
        "- Apache Spark in Azure context\n",
        "- Azure Databricks\n",
        "- Processing data with Spark\n",
        "- Spark transformations and actions\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
