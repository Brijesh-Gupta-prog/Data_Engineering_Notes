{"cells":[{"cell_type":"markdown","metadata":{"id":"zcGDxH9-NEQs"},"source":["# Databricks Spark Practice - 30 Questions\n","\n","## Introduction\n","\n","This notebook contains 30 comprehensive practice questions covering all major PySpark concepts. These questions are designed to be solved on Databricks and will help you master:\n","\n","- SparkSession and basic operations\n","- Reading and writing data\n","- DataFrame transformations\n","- Aggregations and GroupBy\n","- Spark SQL\n","- Joins\n","- Window functions\n","- Complex data types\n","- Performance optimization\n","- Databricks-specific features\n","\n","## Instructions\n","\n","1. **In Databricks**: SparkSession is automatically available as `spark`\n","2. **For local testing**: Uncomment the SparkSession creation code in the setup cell\n","3. Complete each exercise in the provided code cells\n","4. Run the data setup cells first to create sample data\n","5. Test your solutions by running the code and checking outputs\n","6. Refer back to the PySpark module notebooks if you need help\n"]},{"cell_type":"markdown","metadata":{"id":"FDCsS7nXNEQ0"},"source":["## Data Setup\n","\n","Run the cells below to set up all the sample data needed for the exercises.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__VDKmHqNEQ3"},"outputs":[],"source":["# In Databricks, SparkSession is already available\n","# For local testing, uncomment the following:\n","\n","# from pyspark.sql import SparkSession\n","# spark = SparkSession.builder \\\n","#     .appName(\"Databricks Practice\") \\\n","#     .master(\"local[*]\") \\\n","#     .getOrCreate()\n","\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, ArrayType\n","from pyspark.sql.functions import col, when, lit, expr, sum, avg, count, max, min, row_number, rank, dense_rank, lead, lag, window\n","\n","print(\"Setup complete! SparkSession ready.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwPsAjZZNEQ5"},"outputs":[],"source":["# Create employees DataFrame\n","employees_data = [\n","    (1, \"Alice\", 25, \"Sales\", 50000, \"NYC\", \"2020-01-15\"),\n","    (2, \"Bob\", 30, \"IT\", 60000, \"LA\", \"2019-03-20\"),\n","    (3, \"Charlie\", 35, \"Sales\", 70000, \"Chicago\", \"2018-06-10\"),\n","    (4, \"Diana\", 28, \"IT\", 55000, \"NYC\", \"2021-02-14\"),\n","    (5, \"Eve\", 32, \"HR\", 65000, \"Houston\", \"2019-11-05\"),\n","    (6, \"Frank\", 27, \"Sales\", 52000, \"LA\", \"2022-01-08\"),\n","    (7, \"Grace\", 29, \"IT\", 58000, \"Chicago\", \"2020-09-12\"),\n","    (8, \"Henry\", 31, \"HR\", 62000, \"NYC\", \"2018-12-01\"),\n","    (9, \"Ivy\", 26, \"Sales\", 51000, \"Houston\", \"2021-07-22\"),\n","    (10, \"Jack\", 33, \"Finance\", 75000, \"LA\", \"2017-05-30\")\n","]\n","\n","employees_schema = StructType([\n","    StructField(\"emp_id\", IntegerType(), True),\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"age\", IntegerType(), True),\n","    StructField(\"department\", StringType(), True),\n","    StructField(\"salary\", IntegerType(), True),\n","    StructField(\"city\", StringType(), True),\n","    StructField(\"hire_date\", StringType(), True)\n","])\n","\n","df_employees = spark.createDataFrame(employees_data, employees_schema)\n","print(\"Employees DataFrame created:\")\n","df_employees.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUUUh_FNNEQ6"},"outputs":[],"source":["# Create departments DataFrame\n","departments_data = [\n","    (\"Sales\", \"John\", 1000000),\n","    (\"IT\", \"Sarah\", 1500000),\n","    (\"HR\", \"Mike\", 800000),\n","    (\"Finance\", \"Lisa\", 1200000),\n","    (\"Marketing\", \"Tom\", 900000)\n","]\n","\n","departments_schema = StructType([\n","    StructField(\"dept_name\", StringType(), True),\n","    StructField(\"manager\", StringType(), True),\n","    StructField(\"budget\", IntegerType(), True)\n","])\n","\n","df_departments = spark.createDataFrame(departments_data, departments_schema)\n","print(\"Departments DataFrame created:\")\n","df_departments.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mn7wLHekNEQ7"},"outputs":[],"source":["# Create sales DataFrame\n","sales_data = [\n","    (1, \"2024-01-15\", 1000, \"Product A\"),\n","    (1, \"2024-02-20\", 1500, \"Product B\"),\n","    (2, \"2024-01-10\", 2000, \"Product A\"),\n","    (3, \"2024-02-05\", 1200, \"Product C\"),\n","    (1, \"2024-03-12\", 1800, \"Product A\"),\n","    (4, \"2024-01-25\", 900, \"Product B\"),\n","    (2, \"2024-02-28\", 2200, \"Product C\"),\n","    (5, \"2024-03-01\", 1100, \"Product A\"),\n","    (3, \"2024-03-15\", 1300, \"Product B\")\n","]\n","\n","sales_schema = StructType([\n","    StructField(\"emp_id\", IntegerType(), True),\n","    StructField(\"sale_date\", StringType(), True),\n","    StructField(\"amount\", IntegerType(), True),\n","    StructField(\"product\", StringType(), True)\n","])\n","\n","df_sales = spark.createDataFrame(sales_data, sales_schema)\n","print(\"Sales DataFrame created:\")\n","df_sales.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTTpuENyNEQ9"},"outputs":[],"source":["# Create products DataFrame\n","products_data = [\n","    (\"Product A\", \"Electronics\", 500),\n","    (\"Product B\", \"Clothing\", 300),\n","    (\"Product C\", \"Electronics\", 800),\n","    (\"Product D\", \"Food\", 50)\n","]\n","\n","products_schema = StructType([\n","    StructField(\"product_name\", StringType(), True),\n","    StructField(\"category\", StringType(), True),\n","    StructField(\"base_price\", IntegerType(), True)\n","])\n","\n","df_products = spark.createDataFrame(products_data, products_schema)\n","print(\"Products DataFrame created:\")\n","df_products.show()\n"]},{"cell_type":"markdown","metadata":{"id":"M23Iw7fGNEQ-"},"source":["---\n","\n","## Questions\n","\n","### Questions 1-5: Basic DataFrame Operations\n"]},{"cell_type":"markdown","metadata":{"id":"XG3UfuduNEQ_"},"source":["### Question 1: Filter and Select\n","\n","Filter `df_employees` to show only employees from the 'Sales' department, and select only the columns: `name`, `age`, and `salary`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4amGmbCeNERA"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"PRYkwSJfNERB"},"source":["### Question 2: Sort Data\n","\n","Sort `df_employees` by `salary` in descending order and show the top 5 employees.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Yonixo0NERB"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"Z8pw3aaONERC"},"source":["### Question 3: Add Calculated Column\n","\n","Add a new column `annual_bonus` to `df_employees` that is 10% of the salary. Display the result with columns: `name`, `salary`, and `annual_bonus`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3J3w3DjNERC"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"UXTvCRgiNERD"},"source":["### Question 4: Conditional Logic\n","\n","Create a new column `salary_category` in `df_employees` that categorizes salaries as:\n","- \"High\" if salary >= 65000\n","- \"Medium\" if salary >= 55000 and < 65000\n","- \"Low\" if salary < 55000\n","\n","Show `name`, `salary`, and `salary_category`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLFttGKYNERD"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"zrf9VAuaNERE"},"source":["### Question 5: Remove Duplicates and Null Handling\n","\n","Filter `df_employees` to remove any rows where `age` is null, then remove duplicate rows based on all columns. Count the total number of rows remaining.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ya-iAOkNNERF"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"AdzOKfSANERF"},"source":["---\n","\n","### Questions 6-10: Aggregations and GroupBy\n"]},{"cell_type":"markdown","metadata":{"id":"9cjyv0lnNERG"},"source":["### Question 6: Basic Aggregation\n","\n","Calculate the average salary for each department in `df_employees`. Show department and average salary, sorted by average salary in descending order.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCvSIzeyNERG"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"cl8u0YsdNERG"},"source":["### Question 7: Multiple Aggregations\n","\n","For each department, calculate:\n","- Total number of employees\n","- Average salary\n","- Maximum salary\n","- Minimum salary\n","\n","Display the results sorted by department name.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQJf2w6jNERH"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"02hKWxouNERH"},"source":["### Question 8: GroupBy with Filter\n","\n","Find the total sales amount (`amount`) for each employee (`emp_id`) in `df_sales`, but only include employees who have total sales greater than 2000. Show `emp_id` and total sales amount.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iEqc87zNERH"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"uOu1xFq4NERH"},"source":["### Question 9: Count Distinct\n","\n","Count the number of distinct cities where employees work in `df_employees`. Also, for each city, count how many employees work there.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROc8zJUzNERI"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"NTmBbTYfNERI"},"source":["### Question 10: Aggregation with Conditions\n","\n","Calculate the average age of employees for each department, but only include employees who are 30 years or older in the calculation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsuDbEr0NERJ"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"poz3W6PGNERJ"},"source":["---\n","\n","### Questions 11-15: Spark SQL\n"]},{"cell_type":"markdown","metadata":{"id":"m6oQtYTnNERJ"},"source":["### Question 11: Create Temporary View and Query\n","\n","Create a temporary view from `df_employees` called `employees_view` and write a SQL query to find all employees in the 'IT' department with salary greater than 55000. Show `name`, `age`, and `salary`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VnzQgLaNERK"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"D_KL4_JzNERK"},"source":["### Question 12: SQL Aggregation\n","\n","Using Spark SQL, write a query to find the department with the highest total salary. Show the department name and total salary.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOGx9v-MNERL"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"X8xBT-fBNERL"},"source":["### Question 13: SQL with CASE Statement\n","\n","Using Spark SQL, create a query that shows `name`, `salary`, and a new column `salary_band`:\n","- 'A' for salary >= 70000\n","- 'B' for salary >= 60000 and < 70000\n","- 'C' for salary < 60000\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BYRjn9ANERM"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"5GYBkG65NERM"},"source":["### Question 14: SQL Subquery\n","\n","Using Spark SQL, find all employees whose salary is greater than the average salary of all employees. Show `name`, `department`, and `salary`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIpO8wfsNERM"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"c2-DjRidNERN"},"source":["### Question 15: SQL Window Function\n","\n","Using Spark SQL, rank employees within each department by their salary (highest salary gets rank 1). Show `name`, `department`, `salary`, and `rank`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFQKiposNERW"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"2x9Ap5NhNERX"},"source":["---\n","\n","### Questions 16-20: Joins\n"]},{"cell_type":"markdown","metadata":{"id":"NjyS_HC9NERX"},"source":["### Question 16: Inner Join\n","\n","Perform an inner join between `df_employees` and `df_departments` on `department` = `dept_name`. Show `name`, `department`, `salary`, and `manager`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGWJ8szGNERX"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"v-MNYhsJNERY"},"source":["### Question 17: Left Join\n","\n","Perform a left join between `df_employees` and `df_departments` on `department` = `dept_name`. This will show all employees even if their department doesn't exist in the departments table. Show `name`, `department`, and `manager`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaYgJX-JNERY"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"dKL-jLwnNERY"},"source":["### Question 18: Multiple Table Join\n","\n","Join `df_employees`, `df_sales`, and `df_products` to show:\n","- Employee name\n","- Sale date\n","- Sale amount\n","- Product name\n","- Product category\n","\n","Use appropriate join types to include all sales records.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmciTBMsNERZ"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"H9BVnjfyNERZ"},"source":["### Question 19: Left Semi Join\n","\n","Use a left semi join to find all employees from `df_employees` who have made at least one sale (exist in `df_sales`). Show only the employee information: `name`, `department`, and `salary`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOmeHFjvNERZ"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"GHfSiWviNERa"},"source":["### Question 20: Anti Join\n","\n","Use an anti join to find all employees from `df_employees` who have NOT made any sales (do not exist in `df_sales`). Show `name`, `department`, and `salary`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGoobdHPNERa"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"4RqMowT-NERa"},"source":["---\n","\n","### Questions 21-25: Window Functions\n"]},{"cell_type":"markdown","metadata":{"id":"RgefpNVGNERa"},"source":["### Question 21: Row Number\n","\n","Use a window function to assign row numbers to employees within each department, ordered by salary in descending order. Show `name`, `department`, `salary`, and `row_number`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pc6bAkkENERb"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"utrIX9fRNERb"},"source":["### Question 22: Rank and Dense Rank\n","\n","Calculate both `rank` and `dense_rank` for employees within each department based on salary. Show `name`, `department`, `salary`, `rank`, and `dense_rank`. Notice the difference between rank and dense_rank when there are ties.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"po3UeQwINERb"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"YwNvs5b-NERc"},"source":["### Question 23: Running Total\n","\n","Calculate a running total of sales amounts for each employee in `df_sales`, ordered by `sale_date`. Show `emp_id`, `sale_date`, `amount`, and `running_total`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iM4JVnF0NERc"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"T2lWaLvENERd"},"source":["### Question 24: Lead and Lag\n","\n","For each employee's sales in `df_sales`, show:\n","- Current sale amount\n","- Previous sale amount (lag)\n","- Next sale amount (lead)\n","\n","Order by `emp_id` and `sale_date`. Show `emp_id`, `sale_date`, `amount`, `prev_amount`, and `next_amount`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuYoYN88NERd"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"k1swuQGZNERe"},"source":["### Question 25: Window Aggregation\n","\n","For each sale in `df_sales`, calculate:\n","- Average sale amount for the same employee\n","- Maximum sale amount for the same employee\n","- Minimum sale amount for the same employee\n","\n","Show `emp_id`, `sale_date`, `amount`, `avg_amount`, `max_amount`, and `min_amount`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aKsIHB3NERe"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"UAdZu7J2NERe"},"source":["---\n","\n","### Questions 26-30: Advanced Topics\n"]},{"cell_type":"markdown","metadata":{"id":"XjFS2rxMNERe"},"source":["### Question 26: Pivot Operation\n","\n","Pivot `df_sales` to show total sales amount for each employee (`emp_id`) by product. The result should have columns: `emp_id`, `Product A`, `Product B`, `Product C` (and `Product D` if applicable).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEnQkhdSNERf"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"qT79QaumNERf"},"source":["### Question 27: Union Operation\n","\n","Create two DataFrames:\n","1. Employees from 'Sales' department\n","2. Employees from 'IT' department\n","\n","Union them together and show the result with columns: `name`, `department`, `salary`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mged3x3VNERf"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"kKx53_wZNERf"},"source":["### Question 28: Complex Aggregation with Multiple Conditions\n","\n","For each department in `df_employees`, calculate:\n","- Total number of employees\n","- Number of employees with salary > 60000\n","- Average salary for employees with salary > 60000\n","- Average salary for all employees\n","\n","Show all results in a single query.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXTFDwCONERf"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"WiOhwgoXNERg"},"source":["### Question 29: Reading and Writing Data (Databricks)\n","\n","**In Databricks:**\n","1. Write `df_employees` to a Parquet file in Volumes at path `Volumes/workspace/default/databricks_practice/employees/`\n","2. Read the data back from that path into a new DataFrame\n","3. Verify by showing the first 5 rows\n","\n","**Note:**\n","- In Databricks, use the Volumes path format: `Volumes/workspace/default/<catalog_name>/<schema_name>/<path>`\n","- For local testing, use a local path like `./data/output/employees/`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqgQBdVBNERg"},"outputs":[],"source":["# Your solution here\n","# For Databricks (Volumes path):\n","# output_path = \"Volumes/workspace/default/databricks_practice/employees/\"\n","# For local:\n","# output_path = \"./data/output/employees/\"\n"]},{"cell_type":"markdown","metadata":{"id":"cPiTjwdYNERg"},"source":["### Question 30: Complete ETL Pipeline\n","\n","Create a complete ETL pipeline that:\n","1. **Extract**: Join `df_employees` and `df_sales` to get employee sales data\n","2. **Transform**:\n","   - Calculate total sales per employee\n","   - Add a column `performance` that is \"Excellent\" if total sales > 3000, \"Good\" if > 2000, else \"Average\"\n","   - Join with `df_employees` to get employee details\n","3. **Load**: Select and display the final result with columns: `name`, `department`, `total_sales`, `performance`\n","\n","Chain all operations together.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDmlswizNERh"},"outputs":[],"source":["# Your solution here\n"]},{"cell_type":"markdown","metadata":{"id":"GTtrahmTNERh"},"source":["---\n","\n","## Additional Challenges (Optional)\n","\n","If you've completed all 30 questions, try these advanced challenges:\n","\n","1. **Performance Optimization**: Repartition `df_employees` by `department` and cache it. Measure the performance improvement.\n","\n","2. **Complex Window Function**: Calculate the 3-month moving average of sales for each employee.\n","\n","3. **Broadcast Join**: Use broadcast join hint for joining `df_employees` with `df_departments` (assuming departments is small).\n","\n","4. **Date Operations**: Convert `hire_date` in `df_employees` to DateType and calculate the number of years each employee has been with the company.\n","\n","5. **Array Operations**: Create an array column containing all cities where each department has employees, then explode it.\n","\n","---\n","\n","## Summary\n","\n","Congratulations on completing the practice questions! These exercises covered:\n","\n","✅ Basic DataFrame operations (filter, select, sort)\n","\n","✅ Aggregations and GroupBy\n","\n","✅ Spark SQL queries\n","\n","✅ Various join types\n","\n","✅ Window functions\n","\n","✅ Advanced transformations\n","\n","✅ ETL pipeline creation\n","\n","Keep practicing and refer back to the PySpark module notebooks for detailed explanations!\n"]},{"cell_type":"markdown","metadata":{"id":"XusnE2SlNERi"},"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}