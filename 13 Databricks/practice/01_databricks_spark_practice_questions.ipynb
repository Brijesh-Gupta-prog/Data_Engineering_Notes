{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c793207-e5af-4fea-ad75-3ab2ba70ca91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Spark Practice - 30 Questions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains 30 comprehensive practice questions covering all major PySpark concepts. These questions are designed to be solved on Databricks and will help you master:\n",
    "\n",
    "- SparkSession and basic operations\n",
    "- Reading and writing data\n",
    "- DataFrame transformations\n",
    "- Aggregations and GroupBy\n",
    "- Spark SQL\n",
    "- Joins\n",
    "- Window functions\n",
    "- Complex data types\n",
    "- Performance optimization\n",
    "- Databricks-specific features\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **In Databricks**: SparkSession is automatically available as `spark`\n",
    "2. **For local testing**: Uncomment the SparkSession creation code in the setup cell\n",
    "3. Complete each exercise in the provided code cells\n",
    "4. Run the data setup cells first to create sample data\n",
    "5. Test your solutions by running the code and checking outputs\n",
    "6. Refer back to the PySpark module notebooks if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4481c8d-6317-4767-9007-65125556ef20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up all the sample data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bdefd73-2fed-4ddc-aff8-230224652577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! SparkSession ready.\n"
     ]
    }
   ],
   "source": [
    "# In Databricks, SparkSession is already available\n",
    "# For local testing, uncomment the following:\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Databricks Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, ArrayType\n",
    "from pyspark.sql.functions import col, when, lit, expr, sum, avg, count, max, min, row_number, rank, dense_rank, lead, lag, window\n",
    "\n",
    "print(\"Setup complete! SparkSession ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbf6afe-0c0d-4301-a78d-82c539ea4488",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1767866607224}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees DataFrame created:\n+------+-------+---+----------+------+-------+----------+\n|emp_id|   name|age|department|salary|   city| hire_date|\n+------+-------+---+----------+------+-------+----------+\n|     1|  Alice| 25|     Sales| 50000|    NYC|2020-01-15|\n|     2|    Bob| 30|        IT| 60000|     LA|2019-03-20|\n|     3|Charlie| 35|     Sales| 70000|Chicago|2018-06-10|\n|     4|  Diana| 28|        IT| 55000|    NYC|2021-02-14|\n|     5|    Eve| 32|        HR| 65000|Houston|2019-11-05|\n|     6|  Frank| 27|     Sales| 52000|     LA|2022-01-08|\n|     7|  Grace| 29|        IT| 58000|Chicago|2020-09-12|\n|     8|  Henry| 31|        HR| 62000|    NYC|2018-12-01|\n|     9|    Ivy| 26|     Sales| 51000|Houston|2021-07-22|\n|    10|   Jack| 33|   Finance| 75000|     LA|2017-05-30|\n+------+-------+---+----------+------+-------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>age</th><th>department</th><th>salary</th><th>city</th><th>hire_date</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>25</td><td>Sales</td><td>50000</td><td>NYC</td><td>2020-01-15</td></tr><tr><td>2</td><td>Bob</td><td>30</td><td>IT</td><td>60000</td><td>LA</td><td>2019-03-20</td></tr><tr><td>3</td><td>Charlie</td><td>35</td><td>Sales</td><td>70000</td><td>Chicago</td><td>2018-06-10</td></tr><tr><td>4</td><td>Diana</td><td>28</td><td>IT</td><td>55000</td><td>NYC</td><td>2021-02-14</td></tr><tr><td>5</td><td>Eve</td><td>32</td><td>HR</td><td>65000</td><td>Houston</td><td>2019-11-05</td></tr><tr><td>6</td><td>Frank</td><td>27</td><td>Sales</td><td>52000</td><td>LA</td><td>2022-01-08</td></tr><tr><td>7</td><td>Grace</td><td>29</td><td>IT</td><td>58000</td><td>Chicago</td><td>2020-09-12</td></tr><tr><td>8</td><td>Henry</td><td>31</td><td>HR</td><td>62000</td><td>NYC</td><td>2018-12-01</td></tr><tr><td>9</td><td>Ivy</td><td>26</td><td>Sales</td><td>51000</td><td>Houston</td><td>2021-07-22</td></tr><tr><td>10</td><td>Jack</td><td>33</td><td>Finance</td><td>75000</td><td>LA</td><td>2017-05-30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         25,
         "Sales",
         50000,
         "NYC",
         "2020-01-15"
        ],
        [
         2,
         "Bob",
         30,
         "IT",
         60000,
         "LA",
         "2019-03-20"
        ],
        [
         3,
         "Charlie",
         35,
         "Sales",
         70000,
         "Chicago",
         "2018-06-10"
        ],
        [
         4,
         "Diana",
         28,
         "IT",
         55000,
         "NYC",
         "2021-02-14"
        ],
        [
         5,
         "Eve",
         32,
         "HR",
         65000,
         "Houston",
         "2019-11-05"
        ],
        [
         6,
         "Frank",
         27,
         "Sales",
         52000,
         "LA",
         "2022-01-08"
        ],
        [
         7,
         "Grace",
         29,
         "IT",
         58000,
         "Chicago",
         "2020-09-12"
        ],
        [
         8,
         "Henry",
         31,
         "HR",
         62000,
         "NYC",
         "2018-12-01"
        ],
        [
         9,
         "Ivy",
         26,
         "Sales",
         51000,
         "Houston",
         "2021-07-22"
        ],
        [
         10,
         "Jack",
         33,
         "Finance",
         75000,
         "LA",
         "2017-05-30"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hire_date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create employees DataFrame\n",
    "employees_data = [\n",
    "    (1, \"Alice\", 25, \"Sales\", 50000, \"NYC\", \"2020-01-15\"),\n",
    "    (2, \"Bob\", 30, \"IT\", 60000, \"LA\", \"2019-03-20\"),\n",
    "    (3, \"Charlie\", 35, \"Sales\", 70000, \"Chicago\", \"2018-06-10\"),\n",
    "    (4, \"Diana\", 28, \"IT\", 55000, \"NYC\", \"2021-02-14\"),\n",
    "    (5, \"Eve\", 32, \"HR\", 65000, \"Houston\", \"2019-11-05\"),\n",
    "    (6, \"Frank\", 27, \"Sales\", 52000, \"LA\", \"2022-01-08\"),\n",
    "    (7, \"Grace\", 29, \"IT\", 58000, \"Chicago\", \"2020-09-12\"),\n",
    "    (8, \"Henry\", 31, \"HR\", 62000, \"NYC\", \"2018-12-01\"),\n",
    "    (9, \"Ivy\", 26, \"Sales\", 51000, \"Houston\", \"2021-07-22\"),\n",
    "    (10, \"Jack\", 33, \"Finance\", 75000, \"LA\", \"2017-05-30\")\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(employees_data, employees_schema)\n",
    "print(\"Employees DataFrame created:\")\n",
    "df_employees.show()\n",
    "df_employees.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2542bd0-2bf5-4a5d-b675-14302a43ebc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departments DataFrame created:\n+---------+-------+-------+\n|dept_name|manager| budget|\n+---------+-------+-------+\n|    Sales|   John|1000000|\n|       IT|  Sarah|1500000|\n|       HR|   Mike| 800000|\n|  Finance|   Lisa|1200000|\n|Marketing|    Tom| 900000|\n+---------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create departments DataFrame\n",
    "departments_data = [\n",
    "    (\"Sales\", \"John\", 1000000),\n",
    "    (\"IT\", \"Sarah\", 1500000),\n",
    "    (\"HR\", \"Mike\", 800000),\n",
    "    (\"Finance\", \"Lisa\", 1200000),\n",
    "    (\"Marketing\", \"Tom\", 900000)\n",
    "]\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"dept_name\", StringType(), True),\n",
    "    StructField(\"manager\", StringType(), True),\n",
    "    StructField(\"budget\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_departments = spark.createDataFrame(departments_data, departments_schema)\n",
    "print(\"Departments DataFrame created:\")\n",
    "df_departments.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e6ae645-f746-46df-9dc7-7afc4213885f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales DataFrame created:\n+------+----------+------+---------+\n|emp_id| sale_date|amount|  product|\n+------+----------+------+---------+\n|     1|2024-01-15|  1000|Product A|\n|     1|2024-02-20|  1500|Product B|\n|     2|2024-01-10|  2000|Product A|\n|     3|2024-02-05|  1200|Product C|\n|     1|2024-03-12|  1800|Product A|\n|     4|2024-01-25|   900|Product B|\n|     2|2024-02-28|  2200|Product C|\n|     5|2024-03-01|  1100|Product A|\n|     3|2024-03-15|  1300|Product B|\n+------+----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create sales DataFrame\n",
    "sales_data = [\n",
    "    (1, \"2024-01-15\", 1000, \"Product A\"),\n",
    "    (1, \"2024-02-20\", 1500, \"Product B\"),\n",
    "    (2, \"2024-01-10\", 2000, \"Product A\"),\n",
    "    (3, \"2024-02-05\", 1200, \"Product C\"),\n",
    "    (1, \"2024-03-12\", 1800, \"Product A\"),\n",
    "    (4, \"2024-01-25\", 900, \"Product B\"),\n",
    "    (2, \"2024-02-28\", 2200, \"Product C\"),\n",
    "    (5, \"2024-03-01\", 1100, \"Product A\"),\n",
    "    (3, \"2024-03-15\", 1300, \"Product B\")\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, sales_schema)\n",
    "print(\"Sales DataFrame created:\")\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9596da3a-ea1e-41ba-9ff9-9e2504cb3bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products DataFrame created:\n+------------+-----------+----------+\n|product_name|   category|base_price|\n+------------+-----------+----------+\n|   Product A|Electronics|       500|\n|   Product B|   Clothing|       300|\n|   Product C|Electronics|       800|\n|   Product D|       Food|        50|\n+------------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create products DataFrame\n",
    "products_data = [\n",
    "    (\"Product A\", \"Electronics\", 500),\n",
    "    (\"Product B\", \"Clothing\", 300),\n",
    "    (\"Product C\", \"Electronics\", 800),\n",
    "    (\"Product D\", \"Food\", 50)\n",
    "]\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"base_price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_products = spark.createDataFrame(products_data, products_schema)\n",
    "print(\"Products DataFrame created:\")\n",
    "df_products.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3127fbdd-a6ee-4c1b-b428-1df08e2e2078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Questions\n",
    "\n",
    "### Questions 1-5: Basic DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f61c741-fe91-41d8-831a-5dfe27761b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 1: Filter and Select\n",
    "\n",
    "Filter `df_employees` to show only employees from the 'Sales' department, and select only the columns: `name`, `age`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ebb9428-919c-42df-9dbb-a753940e0f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n|   name|age|salary|\n+-------+---+------+\n|  Alice| 25| 50000|\n|Charlie| 35| 70000|\n|  Frank| 27| 52000|\n|    Ivy| 26| 51000|\n+-------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.filter(df_employees.department == 'Sales') \\\n",
    "                       .select('name', 'age', 'salary') \\\n",
    "                           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad35fcc7-3f06-418e-a6d2-170a7f835e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 2: Sort Data\n",
    "\n",
    "Sort `df_employees` by `salary` in descending order and show the top 5 employees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b41bd1-726f-483d-bdc7-860064f1806b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n|   name|age|salary|\n+-------+---+------+\n|  Alice| 25| 50000|\n|    Ivy| 26| 51000|\n|  Frank| 27| 52000|\n|Charlie| 35| 70000|\n+-------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.filter(df_employees.department == 'Sales') \\\n",
    "                       .select('name', 'age', 'salary') \\\n",
    "                        .orderBy('salary') \\\n",
    "                            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b9f529-2bcc-40c3-90ee-ecbf585fb57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 3: Add Calculated Column\n",
    "\n",
    "Add a new column `annual_bonus` to `df_employees` that is 10% of the salary. Display the result with columns: `name`, `salary`, and `annual_bonus`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e2df65-8243-4192-b04f-a2fae5de8e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n|   name|age| bonus|\n+-------+---+------+\n|  Alice| 25|5000.0|\n|    Bob| 30|6000.0|\n|Charlie| 35|7000.0|\n|  Diana| 28|5500.0|\n|    Eve| 32|6500.0|\n|  Frank| 27|5200.0|\n|  Grace| 29|5800.0|\n|  Henry| 31|6200.0|\n|    Ivy| 26|5100.0|\n|   Jack| 33|7500.0|\n+-------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.withColumn('bonus', df_employees.salary * 0.1) \\\n",
    "            .select('name','age','bonus') \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "462db584-ad85-4ac8-93b7-69d5ba78f44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 4: Conditional Logic\n",
    "\n",
    "Create a new column `salary_category` in `df_employees` that categorizes salaries as:\n",
    "- \"High\" if salary >= 65000\n",
    "- \"Medium\" if salary >= 55000 and < 65000\n",
    "- \"Low\" if salary < 55000\n",
    "\n",
    "Show `name`, `salary`, and `salary_category`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f107dbef-bb5b-47e1-abcb-41d366bdd7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------+\n|   name|salary|salary_category|\n+-------+------+---------------+\n|  Alice| 50000|            Low|\n|    Bob| 60000|         Medium|\n|Charlie| 70000|           High|\n|  Diana| 55000|         Medium|\n|    Eve| 65000|           High|\n|  Frank| 52000|            Low|\n|  Grace| 58000|         Medium|\n|  Henry| 62000|         Medium|\n|    Ivy| 51000|            Low|\n|   Jack| 75000|           High|\n+-------+------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.withColumn(\n",
    "            'salary_category',\n",
    "            when(df_employees.salary>=65000,'High').\n",
    "            when((df_employees.salary>=55000) & (df_employees.salary<65000),'Medium')\n",
    "            .otherwise('Low')) \\\n",
    "            .select('name','salary','salary_category') \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "078909a4-a2fe-4593-b0d6-74003307a731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 5: Remove Duplicates and Null Handling\n",
    "\n",
    "Filter `df_employees` to remove any rows where `age` is null, then remove duplicate rows based on all columns. Count the total number of rows remaining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81427a65-4d93-4842-97f5-ac290d019ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.dropna(subset=['age']) \\\n",
    "            .dropDuplicates() \\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1fc89b9-f191-4094-83bd-19c15e810678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 6-10: Aggregations and GroupBy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e17c37a-d929-4876-baf4-48a8202fa858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 6: Basic Aggregation\n",
    "\n",
    "Calculate the average salary for each department in `df_employees`. Show department and average salary, sorted by average salary in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025459d9-b45d-4621-a48c-9db3a7c02ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|department|        avg_salary|\n+----------+------------------+\n|   Finance|           75000.0|\n|        HR|           63500.0|\n|        IT|57666.666666666664|\n|     Sales|           55750.0|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_avg_salary = df_employees.groupBy('department').agg(\n",
    "    avg('salary').alias('avg_salary')\n",
    ")\n",
    "\n",
    "\n",
    "df_avg_salary_sorted = df_avg_salary.orderBy(col('avg_salary'), ascending=False)\n",
    "\n",
    "\n",
    "df_avg_salary_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71720967-4685-4ce1-aff1-6fcdd8fdd7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 7: Multiple Aggregations\n",
    "\n",
    "For each department, calculate:\n",
    "- Total number of employees\n",
    "- Average salary\n",
    "- Maximum salary\n",
    "- Minimum salary\n",
    "\n",
    "Display the results sorted by department name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2e47d1-9dbf-4460-b32c-e6d314e4d8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+----------+----------+\n|department|total_employees|        avg_salary|max_salary|min_salary|\n+----------+---------------+------------------+----------+----------+\n|   Finance|              1|           75000.0|     75000|     75000|\n|        HR|              2|           63500.0|     65000|     62000|\n|        IT|              3|57666.666666666664|     60000|     55000|\n|     Sales|              4|           55750.0|     70000|     50000|\n+----------+---------------+------------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_department_stats = df_employees.groupBy('department').agg(\n",
    "    count('name').alias('total_employees'),\n",
    "    avg('salary').alias('avg_salary'),     \n",
    "    max('salary').alias('max_salary'),     \n",
    "    min('salary').alias('min_salary')      \n",
    ")\n",
    "\n",
    "\n",
    "df_sorted_stats = df_department_stats.orderBy('department')\n",
    "\n",
    "\n",
    "df_sorted_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70ac0d8c-0507-4987-8b9d-513e9232477e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 8: GroupBy with Filter\n",
    "\n",
    "Find the total sales amount (`amount`) for each employee (`emp_id`) in `df_sales`, but only include employees who have total sales greater than 2000. Show `emp_id` and total sales amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1a1289-ddab-4905-b378-cdecf275a733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|emp_id|total_sales|\n+------+-----------+\n|     1|       4300|\n|     2|       4200|\n|     3|       2500|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_sales_total = df_sales.groupBy('emp_id').agg(\n",
    "    sum('amount').alias('total_sales')\n",
    ")\n",
    "\n",
    "\n",
    "df_sales_filtered = df_sales_total.filter(df_sales_total.total_sales > 2000)\n",
    "\n",
    "\n",
    "df_sales_filtered.select('emp_id', 'total_sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe8e759-12d5-46bf-bf64-5567f4e7dfc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 9: Count Distinct\n",
    "\n",
    "Count the number of distinct cities where employees work in `df_employees`. Also, for each city, count how many employees work there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92f2abc-b441-43ca-a9cb-8645dee1e3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------+\n|   city|distinct_city_count|employee_count|\n+-------+-------------------+--------------+\n|    NYC|                  3|             3|\n|     LA|                  3|             3|\n|Chicago|                  2|             2|\n|Houston|                  2|             2|\n+-------+-------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_city_count = df_employees.groupBy('city').agg(\n",
    "    count('city').alias('distinct_city_count'),\n",
    "    count('emp_id').alias('employee_count')\n",
    ")\n",
    "\n",
    "df_city_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f47846-6e39-4ee9-b29d-7067c0977127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 10: Aggregation with Conditions\n",
    "\n",
    "Calculate the average age of employees for each department, but only include employees who are 30 years or older in the calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae0818f-90fc-41d0-a8a2-f8230e668ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n|department|avg_age|\n+----------+-------+\n|        IT|   30.0|\n|     Sales|   35.0|\n|        HR|   31.5|\n|   Finance|   33.0|\n+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_filtered = df_employees.filter(df_employees.age >= 30)\n",
    "\n",
    "\n",
    "df_avg_age = df_filtered.groupBy('department').agg(\n",
    "    avg('age').alias('avg_age')\n",
    ")\n",
    "\n",
    "\n",
    "df_avg_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eebdc6a4-40fe-493e-be00-0d61d5154037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 11-15: Spark SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e519a9-5280-424a-876a-9a11c70b3892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 11: Create Temporary View and Query\n",
    "\n",
    "Create a temporary view from `df_employees` called `employees_view` and write a SQL query to find all employees in the 'IT' department with salary greater than 55000. Show `name`, `age`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb5f82d-0439-47ae-b896-8e8875bb60f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView('employees_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cffee77-6035-47b9-831e-728b2999e6ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th></tr></thead><tbody><tr><td>Bob</td><td>30</td><td>60000</td></tr><tr><td>Grace</td><td>29</td><td>58000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Bob",
         30,
         60000
        ],
        [
         "Grace",
         29,
         58000
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "age",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "salary",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 77
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT name, age, salary\n",
    "FROM employees_view\n",
    "WHERE department = 'IT' AND salary > 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dacfcf41-589f-4235-8ffd-3c3c3e493c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n| name|age|salary|\n+-----+---+------+\n|  Bob| 30| 60000|\n|Grace| 29| 58000|\n+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.createOrReplaceTempView('employees_view')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT name, age, salary\n",
    "FROM employees_view\n",
    "WHERE department = 'IT' AND salary > 55000\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6434a6e-dfb5-4583-a3b7-00a1f1917edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 12: SQL Aggregation\n",
    "\n",
    "Using Spark SQL, write a query to find the department with the highest total salary. Show the department name and total salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0971c9e5-8e35-4453-af79-52be84d9055a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|department|total_salary|\n+----------+------------+\n|     Sales|      223000|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.createOrReplaceTempView('employees_view')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT department, SUM(salary) AS total_salary\n",
    "FROM employees_view\n",
    "GROUP BY department\n",
    "ORDER BY total_salary DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba65b4f2-4392-4f71-93f6-72522311dd3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 13: SQL with CASE Statement\n",
    "\n",
    "Using Spark SQL, create a query that shows `name`, `salary`, and a new column `salary_band`:\n",
    "- 'A' for salary >= 70000\n",
    "- 'B' for salary >= 60000 and < 70000\n",
    "- 'C' for salary < 60000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad92990b-4850-4239-9b39-63e8da8973b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+\n|   name|salary|salary_band|\n+-------+------+-----------+\n|  Alice| 50000|          C|\n|    Bob| 60000|          B|\n|Charlie| 70000|          A|\n|  Diana| 55000|          C|\n|    Eve| 65000|          B|\n|  Frank| 52000|          C|\n|  Grace| 58000|          C|\n|  Henry| 62000|          B|\n|    Ivy| 51000|          C|\n|   Jack| 75000|          A|\n+-------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.createOrReplaceTempView('employees_view')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT name, salary,\n",
    "    CASE \n",
    "        WHEN salary >= 70000 THEN 'A'\n",
    "        WHEN salary >= 60000 AND salary < 70000 THEN 'B'\n",
    "        ELSE 'C'\n",
    "    END AS salary_band\n",
    "FROM employees_view\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b1cfa18-842d-45d4-a85a-9778078551ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 14: SQL Subquery\n",
    "\n",
    "Using Spark SQL, find all employees whose salary is greater than the average salary of all employees. Show `name`, `department`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19aa8c32-4365-4c5e-96d5-a3b4907061ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n|   name|department|salary|\n+-------+----------+------+\n|    Bob|        IT| 60000|\n|Charlie|     Sales| 70000|\n|    Eve|        HR| 65000|\n|  Henry|        HR| 62000|\n|   Jack|   Finance| 75000|\n+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.createOrReplaceTempView('employees_view')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT name, department, salary\n",
    "FROM employees_view\n",
    "WHERE salary > (SELECT AVG(salary) FROM employees_view)\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb73b8d3-553e-45af-a2df-073526eb32a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 15: SQL Window Function\n",
    "\n",
    "Using Spark SQL, rank employees within each department by their salary (highest salary gets rank 1). Show `name`, `department`, `salary`, and `rank`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12395a83-7c85-4bdd-9d0c-a27b6151a40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n|   name|department|salary|rank|\n+-------+----------+------+----+\n|   Jack|   Finance| 75000|   1|\n|    Eve|        HR| 65000|   1|\n|  Henry|        HR| 62000|   2|\n|    Bob|        IT| 60000|   1|\n|  Grace|        IT| 58000|   2|\n|  Diana|        IT| 55000|   3|\n|Charlie|     Sales| 70000|   1|\n|  Frank|     Sales| 52000|   2|\n|    Ivy|     Sales| 51000|   3|\n|  Alice|     Sales| 50000|   4|\n+-------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "df_employees.createOrReplaceTempView('employees_view')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT name, department, salary,\n",
    "       RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rank\n",
    "FROM employees_view\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa9b8778-3324-4aef-a34d-b2f3f92cdd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 16-20: Joins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ff4bb5-7846-41ab-8f62-283b0691fdae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 16: Inner Join\n",
    "\n",
    "Perform an inner join between `df_employees` and `df_departments` on `department` = `dept_name`. Show `name`, `department`, `salary`, and `manager`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003536f2-9e62-46e1-82bb-5e32eb8826f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+\n|   name|department|salary|manager|\n+-------+----------+------+-------+\n|  Alice|     Sales| 50000|   John|\n|    Bob|        IT| 60000|  Sarah|\n|Charlie|     Sales| 70000|   John|\n|  Diana|        IT| 55000|  Sarah|\n|    Eve|        HR| 65000|   Mike|\n|  Frank|     Sales| 52000|   John|\n|  Grace|        IT| 58000|  Sarah|\n|  Henry|        HR| 62000|   Mike|\n|    Ivy|     Sales| 51000|   John|\n|   Jack|   Finance| 75000|   Lisa|\n+-------+----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.join(df_departments, df_employees.department == df_departments.dept_name, \"inner\") \\\n",
    "    .select(df_employees.name, df_employees.department, df_employees.salary, df_departments.manager) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d735d8be-b0ed-4565-a948-8f91db5cf4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 17: Left Join\n",
    "\n",
    "Perform a left join between `df_employees` and `df_departments` on `department` = `dept_name`. This will show all employees even if their department doesn't exist in the departments table. Show `name`, `department`, and `manager`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9502c18-cd3c-45be-b021-70b827b4086e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+\n|   name|department|manager|\n+-------+----------+-------+\n|  Alice|     Sales|   John|\n|    Bob|        IT|  Sarah|\n|Charlie|     Sales|   John|\n|  Diana|        IT|  Sarah|\n|    Eve|        HR|   Mike|\n|  Frank|     Sales|   John|\n|  Grace|        IT|  Sarah|\n|  Henry|        HR|   Mike|\n|    Ivy|     Sales|   John|\n|   Jack|   Finance|   Lisa|\n+-------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.join(df_departments, df_employees.department == df_departments.dept_name, \"left\") \\\n",
    "    .select(df_employees.name, df_employees.department, df_departments.manager) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66548acb-f7e1-4efa-be78-a3030068d89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 18: Multiple Table Join\n",
    "\n",
    "Join `df_employees`, `df_sales`, and `df_products` to show:\n",
    "- Employee name\n",
    "- Sale date\n",
    "- Sale amount\n",
    "- Product name\n",
    "- Product category\n",
    "\n",
    "Use appropriate join types to include all sales records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e2689c-3626-4fbd-89ad-73b35dd334e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+------------+----------------+\n|   name| sale_date|sale_amount|product_name|product_category|\n+-------+----------+-----------+------------+----------------+\n|  Alice|2024-01-15|       1000|   Product A|     Electronics|\n|  Alice|2024-02-20|       1500|   Product B|        Clothing|\n|    Bob|2024-01-10|       2000|   Product A|     Electronics|\n|Charlie|2024-02-05|       1200|   Product C|     Electronics|\n|  Alice|2024-03-12|       1800|   Product A|     Electronics|\n|  Diana|2024-01-25|        900|   Product B|        Clothing|\n|    Bob|2024-02-28|       2200|   Product C|     Electronics|\n|    Eve|2024-03-01|       1100|   Product A|     Electronics|\n|Charlie|2024-03-15|       1300|   Product B|        Clothing|\n+-------+----------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_employees.join(df_sales, df_employees.emp_id == df_sales.emp_id, \"inner\") \\\n",
    "    .join(df_products, df_sales.product == df_products.product_name, \"inner\") \\\n",
    "    .select(df_employees.name, df_sales.sale_date, df_sales.amount.alias(\"sale_amount\"), df_products.product_name, df_products.category.alias(\"product_category\")) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3327fb00-9e94-423f-b5cc-13f5af7806d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 19: Left Semi Join\n",
    "\n",
    "Use a left semi join to find all employees from `df_employees` who have made at least one sale (exist in `df_sales`). Show only the employee information: `name`, `department`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56038544-85f2-4905-bf4a-9a189d167363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n|   name|department|salary|\n+-------+----------+------+\n|  Alice|     Sales| 50000|\n|    Bob|        IT| 60000|\n|Charlie|     Sales| 70000|\n|  Diana|        IT| 55000|\n|    Eve|        HR| 65000|\n+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.join(df_sales, df_employees.emp_id == df_sales.emp_id, \"left_semi\") \\\n",
    "    .select(df_employees.name, df_employees.department, df_employees.salary) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417cd912-4380-4030-acd4-359e79ec1e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 20: Anti Join\n",
    "\n",
    "Use an anti join to find all employees from `df_employees` who have NOT made any sales (do not exist in `df_sales`). Show `name`, `department`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6574d13-53f9-40f2-bdb5-ce5e408bf723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+\n| name|department|salary|\n+-----+----------+------+\n|Frank|     Sales| 52000|\n|Grace|        IT| 58000|\n|Henry|        HR| 62000|\n|  Ivy|     Sales| 51000|\n| Jack|   Finance| 75000|\n+-----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.join(df_sales, df_employees.emp_id == df_sales.emp_id, \"left_anti\") \\\n",
    "    .select(df_employees.name, df_employees.department, df_employees.salary) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de82549-80f4-4c45-89d5-0b31c3c90261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 21-25: Window Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e8f5acc-e599-48fc-9771-0bb9410b1f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 21: Row Number\n",
    "\n",
    "Use a window function to assign row numbers to employees within each department, ordered by salary in descending order. Show `name`, `department`, `salary`, and `row_number`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38009459-c698-4484-8885-85f55e6dec35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n|   name|department|salary|row_number|\n+-------+----------+------+----------+\n|   Jack|   Finance| 75000|         1|\n|    Eve|        HR| 65000|         1|\n|  Henry|        HR| 62000|         2|\n|    Bob|        IT| 60000|         1|\n|  Grace|        IT| 58000|         2|\n|  Diana|        IT| 55000|         3|\n|Charlie|     Sales| 70000|         1|\n|  Frank|     Sales| 52000|         2|\n|    Ivy|     Sales| 51000|         3|\n|  Alice|     Sales| 50000|         4|\n+-------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(df_employees.department).orderBy(functions.col(\"salary\").desc())\n",
    "\n",
    "df_employees.withColumn(\"row_number\", functions.row_number().over(window_spec)) \\\n",
    "    .select(\"name\", \"department\", \"salary\", \"row_number\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60748455-8861-4409-af49-c89941192912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 22: Rank and Dense Rank\n",
    "\n",
    "Calculate both `rank` and `dense_rank` for employees within each department based on salary. Show `name`, `department`, `salary`, `rank`, and `dense_rank`. Notice the difference between rank and dense_rank when there are ties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320f795e-2aea-4838-a3bf-4c859db6bfe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+\n|   name|department|salary|rank|dense_rank|\n+-------+----------+------+----+----------+\n|   Jack|   Finance| 75000|   1|         1|\n|    Eve|        HR| 65000|   1|         1|\n|  Henry|        HR| 62000|   2|         2|\n|    Bob|        IT| 60000|   1|         1|\n|  Grace|        IT| 58000|   2|         2|\n|  Diana|        IT| 55000|   3|         3|\n|Charlie|     Sales| 70000|   1|         1|\n|  Frank|     Sales| 52000|   2|         2|\n|    Ivy|     Sales| 51000|   3|         3|\n|  Alice|     Sales| 50000|   4|         4|\n+-------+----------+------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(df_employees.department).orderBy(functions.col(\"salary\").desc())\n",
    "\n",
    "df_employees.withColumn(\"rank\", functions.rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", functions.dense_rank().over(window_spec)) \\\n",
    "    .select(\"name\", \"department\", \"salary\", \"rank\", \"dense_rank\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12e485cd-f7ec-417e-a83c-d5dcc9acee21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 23: Running Total\n",
    "\n",
    "Calculate a running total of sales amounts for each employee in `df_sales`, ordered by `sale_date`. Show `emp_id`, `sale_date`, `amount`, and `running_total`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388153fc-5332-402c-be0a-47d906c0286a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-------------+\n|emp_id| sale_date|amount|running_total|\n+------+----------+------+-------------+\n|     1|2024-01-15|  1000|         1000|\n|     1|2024-02-20|  1500|         2500|\n|     1|2024-03-12|  1800|         4300|\n|     2|2024-01-10|  2000|         2000|\n|     2|2024-02-28|  2200|         4200|\n|     3|2024-02-05|  1200|         1200|\n|     3|2024-03-15|  1300|         2500|\n|     4|2024-01-25|   900|          900|\n|     5|2024-03-01|  1100|         1100|\n+------+----------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "\n",
    "df_sales.withColumn(\"running_total\", sum(\"amount\").over(window_spec)) \\\n",
    "    .select(\"emp_id\", \"sale_date\", \"amount\", \"running_total\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c1fb04f-779f-4907-adec-166e52fcfbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 24: Lead and Lag\n",
    "\n",
    "For each employee's sales in `df_sales`, show:\n",
    "- Current sale amount\n",
    "- Previous sale amount (lag)\n",
    "- Next sale amount (lead)\n",
    "\n",
    "Order by `emp_id` and `sale_date`. Show `emp_id`, `sale_date`, `amount`, `prev_amount`, and `next_amount`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8f8654-427e-4679-922e-f39769c4845f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----------+-----------+\n|emp_id| sale_date|amount|prev_amount|next_amount|\n+------+----------+------+-----------+-----------+\n|     1|2024-01-15|  1000|       NULL|       1500|\n|     1|2024-02-20|  1500|       1000|       1800|\n|     1|2024-03-12|  1800|       1500|       NULL|\n|     2|2024-01-10|  2000|       NULL|       2200|\n|     2|2024-02-28|  2200|       2000|       NULL|\n|     3|2024-02-05|  1200|       NULL|       1300|\n|     3|2024-03-15|  1300|       1200|       NULL|\n|     4|2024-01-25|   900|       NULL|       NULL|\n|     5|2024-03-01|  1100|       NULL|       NULL|\n+------+----------+------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\").orderBy(\"sale_date\")\n",
    "\n",
    "df_sales.withColumn(\"prev_amount\", lag(\"amount\").over(window_spec)) \\\n",
    "    .withColumn(\"next_amount\", lead(\"amount\").over(window_spec)) \\\n",
    "    .select(\"emp_id\", \"sale_date\", \"amount\", \"prev_amount\", \"next_amount\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "864baa3c-0598-456c-8d6b-bc403d2da4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 25: Window Aggregation\n",
    "\n",
    "For each sale in `df_sales`, calculate:\n",
    "- Average sale amount for the same employee\n",
    "- Maximum sale amount for the same employee\n",
    "- Minimum sale amount for the same employee\n",
    "\n",
    "Show `emp_id`, `sale_date`, `amount`, `avg_amount`, `max_amount`, and `min_amount`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df17c40e-9769-4957-b7b2-1c52f34944d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------------------+----------+----------+\n|emp_id| sale_date|amount|        avg_amount|max_amount|min_amount|\n+------+----------+------+------------------+----------+----------+\n|     1|2024-01-15|  1000|1433.3333333333333|      1800|      1000|\n|     1|2024-02-20|  1500|1433.3333333333333|      1800|      1000|\n|     1|2024-03-12|  1800|1433.3333333333333|      1800|      1000|\n|     2|2024-01-10|  2000|            2100.0|      2200|      2000|\n|     2|2024-02-28|  2200|            2100.0|      2200|      2000|\n|     3|2024-02-05|  1200|            1250.0|      1300|      1200|\n|     3|2024-03-15|  1300|            1250.0|      1300|      1200|\n|     4|2024-01-25|   900|             900.0|       900|       900|\n|     5|2024-03-01|  1100|            1100.0|      1100|      1100|\n+------+----------+------+------------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "window_spec = Window.partitionBy(\"emp_id\")\n",
    "\n",
    "df_sales.withColumn(\"avg_amount\", avg(\"amount\").over(window_spec)) \\\n",
    "    .withColumn(\"max_amount\", max(\"amount\").over(window_spec)) \\\n",
    "    .withColumn(\"min_amount\", min(\"amount\").over(window_spec)) \\\n",
    "    .select(\"emp_id\", \"sale_date\", \"amount\", \"avg_amount\", \"max_amount\", \"min_amount\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "732a7cc6-bd07-4a41-bd2f-14b5c2712af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 26-30: Advanced Topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d22b862c-7471-4478-b5e2-01d2014ff18e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 26: Pivot Operation\n",
    "\n",
    "Pivot `df_sales` to show total sales amount for each employee (`emp_id`) by product. The result should have columns: `emp_id`, `Product A`, `Product B`, `Product C` (and `Product D` if applicable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f58030-dc44-428f-8508-3276a18da4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+---------+\n|emp_id|Product A|Product B|Product C|\n+------+---------+---------+---------+\n|     1|     2800|     1500|     NULL|\n|     2|     2000|     NULL|     2200|\n|     3|     NULL|     1300|     1200|\n|     4|     NULL|      900|     NULL|\n|     5|     1100|     NULL|     NULL|\n+------+---------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_sales.groupBy(\"emp_id\") \\\n",
    "    .pivot(\"product\") \\\n",
    "    .agg(sum(\"amount\")) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fce8a6e8-d6ef-4fc7-b3d3-6526769fdf35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 27: Union Operation\n",
    "\n",
    "Create two DataFrames:\n",
    "1. Employees from 'Sales' department\n",
    "2. Employees from 'IT' department\n",
    "\n",
    "Union them together and show the result with columns: `name`, `department`, `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05d95ae-10f0-423d-9c8a-28c386b7277a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n|   name|department|salary|\n+-------+----------+------+\n|  Alice|     Sales| 50000|\n|Charlie|     Sales| 70000|\n|  Frank|     Sales| 52000|\n|    Ivy|     Sales| 51000|\n|    Bob|        IT| 60000|\n|  Diana|        IT| 55000|\n|  Grace|        IT| 58000|\n+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_sales_employees = df_employees.filter(df_employees.department == \"Sales\")\n",
    "df_it_employees = df_employees.filter(df_employees.department == \"IT\")\n",
    "\n",
    "df_sales_employees.select(\"name\", \"department\", \"salary\") \\\n",
    "    .union(df_it_employees.select(\"name\", \"department\", \"salary\")) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8001a8b8-3d51-4c37-9129-1e7f300b8ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 28: Complex Aggregation with Multiple Conditions\n",
    "\n",
    "For each department in `df_employees`, calculate:\n",
    "- Total number of employees\n",
    "- Number of employees with salary > 60000\n",
    "- Average salary for employees with salary > 60000\n",
    "- Average salary for all employees\n",
    "\n",
    "Show all results in a single query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84579901-cd47-4487-81b4-efa093d31ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------------+---------------+------------------+\n|department|total_employees|high_salary_employees|avg_high_salary|        avg_salary|\n+----------+---------------+---------------------+---------------+------------------+\n|     Sales|              4|                    1|        70000.0|           55750.0|\n|        IT|              3|                    0|           NULL|57666.666666666664|\n|        HR|              2|                    2|        63500.0|           63500.0|\n|   Finance|              1|                    1|        75000.0|           75000.0|\n+----------+---------------+---------------------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employees.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_employees\"),\n",
    "        count(when(df_employees.salary > 60000, 1)).alias(\"high_salary_employees\"),\n",
    "        avg(when(df_employees.salary > 60000, df_employees.salary)).alias(\"avg_high_salary\"),\n",
    "        avg(df_employees.salary).alias(\"avg_salary\")\n",
    "    ) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d797513-5087-4a57-acc3-625cc247e1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 29: Reading and Writing Data (Databricks)\n",
    "\n",
    "**In Databricks:**\n",
    "1. Write `df_employees` to a Parquet file in Volumes at path `Volumes/workspace/default/databricks_practice/employees/`\n",
    "2. Read the data back from that path into a new DataFrame\n",
    "3. Verify by showing the first 5 rows\n",
    "\n",
    "**Note:** \n",
    "- In Databricks, use the Volumes path format: `Volumes/workspace/default/<catalog_name>/<schema_name>/<path>`\n",
    "- For local testing, use a local path like `./data/output/employees/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc63caa-af52-42a2-b270-5e7487f1bc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"/Volumes/demo_catalog/demo_schema/demo_volume/sellers_dataset.csv\"\n",
    "\n",
    "df_employees.write.mode(\"overwrite\").csv(output_path)\n",
    "df_employees_parquet = spark.read.csv(output_path)\n",
    "display(df_employees_parquet.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a338f375-e7b0-431f-b209-1d0d5e3b1437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 30: Complete ETL Pipeline\n",
    "\n",
    "Create a complete ETL pipeline that:\n",
    "1. **Extract**: Join `df_employees` and `df_sales` to get employee sales data\n",
    "2. **Transform**: \n",
    "   - Calculate total sales per employee\n",
    "   - Add a column `performance` that is \"Excellent\" if total sales > 3000, \"Good\" if > 2000, else \"Average\"\n",
    "   - Join with `df_employees` to get employee details\n",
    "3. **Load**: Select and display the final result with columns: `name`, `department`, `total_sales`, `performance`\n",
    "\n",
    "Chain all operations together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a793d129-5196-47a6-94d4-c90a20b34263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-----------+\n|   name|department|total_sales|performance|\n+-------+----------+-----------+-----------+\n|  Alice|     Sales|       4300|  Excellent|\n|    Bob|        IT|       4200|  Excellent|\n|Charlie|     Sales|       2500|       Good|\n|  Diana|        IT|        900|    Average|\n|    Eve|        HR|       1100|    Average|\n+-------+----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "df_employee_sales = df_employees.join(df_sales, df_employees.emp_id == df_sales.emp_id, \"inner\")\n",
    "\n",
    "df_sales_total = df_employee_sales.groupBy(df_employees.emp_id, df_employees.name, df_employees.department) \\\n",
    "    .agg({\"amount\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_sales\")\n",
    "\n",
    "df_sales_total = df_sales_total.withColumn(\n",
    "    \"performance\",\n",
    "    when(df_sales_total.total_sales > 3000, \"Excellent\")\n",
    "    .when(df_sales_total.total_sales > 2000, \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "\n",
    "df_final_result = df_sales_total.select(\"name\", \"department\", \"total_sales\", \"performance\")\n",
    "\n",
    "df_final_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28afd0f9-739d-495c-af54-5fde9c5ee426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Additional Challenges (Optional)\n",
    "\n",
    "If you've completed all 30 questions, try these advanced challenges:\n",
    "\n",
    "1. **Performance Optimization**: Repartition `df_employees` by `department` and cache it. Measure the performance improvement.\n",
    "\n",
    "2. **Complex Window Function**: Calculate the 3-month moving average of sales for each employee.\n",
    "\n",
    "3. **Broadcast Join**: Use broadcast join hint for joining `df_employees` with `df_departments` (assuming departments is small).\n",
    "\n",
    "4. **Date Operations**: Convert `hire_date` in `df_employees` to DateType and calculate the number of years each employee has been with the company.\n",
    "\n",
    "5. **Array Operations**: Create an array column containing all cities where each department has employees, then explode it.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing the practice questions! These exercises covered:\n",
    "\n",
    " Basic DataFrame operations (filter, select, sort)\n",
    "\n",
    " Aggregations and GroupBy\n",
    "\n",
    " Spark SQL queries\n",
    "\n",
    " Various join types\n",
    "\n",
    " Window functions\n",
    "\n",
    " Advanced transformations\n",
    "\n",
    " ETL pipeline creation\n",
    "\n",
    "Keep practicing and refer back to the PySpark module notebooks for detailed explanations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddf3974c-3e7e-4d1f-a8c6-e73ffe15f861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6622542726987515,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_databricks_spark_practice_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}