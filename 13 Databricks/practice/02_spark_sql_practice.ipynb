{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61236d61-8b79-4c91-8f4b-24b6feda7e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark SQL Practice - Revenue Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains a practice question on computing monthly revenue by region using Spark SQL. This exercise will help you master:\n",
    "\n",
    "- Date filtering and window functions\n",
    "- Handling latest records per group\n",
    "- Joins across multiple tables\n",
    "- Monthly aggregation with date functions\n",
    "- Complex business logic implementation\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **In Databricks**: SparkSession is automatically available as `spark`\n",
    "2. **For local testing**: Uncomment the SparkSession creation code in the setup cell\n",
    "3. Run the data setup cells first to create sample data\n",
    "4. Complete the exercise in the provided code cell\n",
    "5. Test your solution and verify the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5074eda-5021-4fea-975c-6c8b2abaeae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up all the sample data needed for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c591e2d-94da-4be9-aa52-a734d61cdba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! SparkSession ready.\n"
     ]
    }
   ],
   "source": [
    "# In Databricks, SparkSession is already available\n",
    "# For local testing, uncomment the following:\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark SQL Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp, expr, date_sub, date_format, sum as spark_sum, max as spark_max, row_number, window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Setup complete! SparkSession ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bdc0842-9b56-4ed7-8c1a-3fd8735c4026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers table created:\n+-----------+------+\n|customer_id|region|\n+-----------+------+\n|          1| North|\n|          2| South|\n|          3|  East|\n|          4|  West|\n|          5| North|\n|          6| South|\n|          7|  East|\n|          8|  West|\n|          9| North|\n|         10| South|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create customers table\n",
    "# Schema: customer_id, region\n",
    "\n",
    "customers_data =display [\n",
    "    (1, \"North\"),\n",
    "    (2, \"South\"),\n",
    "    (3, \"East\"),\n",
    "    (4, \"West\"),\n",
    "    (5, \"North\"),\n",
    "    (6, \"South\"),\n",
    "    (7, \"East\"),\n",
    "    (8, \"West\"),\n",
    "    (9, \"North\"),\n",
    "    (10, \"South\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.createDataFrame(customers_data, customers_schema)\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "print(\"Customers table created:\")\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e2dfc2-d2d2-4096-b5a2-42af0a42e27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders table created:\n+--------+-----------+-------------------+-------+\n|order_id|customer_id|order_ts           |amount |\n+--------+-----------+-------------------+-------+\n|2       |1          |2025-09-11 03:44:29|1062.88|\n|16      |7          |2025-09-21 03:44:29|965.27 |\n|9       |4          |2025-09-23 03:44:29|1589.54|\n|14      |6          |2025-10-01 03:44:29|1795.87|\n|12      |5          |2025-10-04 03:44:29|1293.37|\n|10      |4          |2025-10-17 03:44:29|1008.83|\n|11      |5          |2025-10-19 03:44:29|305.81 |\n|18      |8          |2025-10-19 03:44:29|1354.15|\n|24      |10         |2025-10-21 03:44:29|586.77 |\n|4       |2          |2025-10-22 03:44:29|1955.52|\n|7       |3          |2025-10-22 03:44:29|1956.53|\n|17      |7          |2025-10-28 03:44:29|1807.68|\n|1       |1          |2025-11-06 03:44:29|370.41 |\n|13      |6          |2025-11-08 03:44:29|225.65 |\n|26      |10         |2025-11-16 03:44:29|1876.35|\n|3       |1          |2025-12-08 03:44:29|988.53 |\n|20      |8          |2025-12-10 03:44:29|1705.49|\n|21      |9          |2025-12-10 03:44:29|1943.44|\n|15      |7          |2025-12-15 03:44:29|1228.31|\n|23      |10         |2025-12-16 03:44:29|159.32 |\n|5       |2          |2025-12-18 03:44:29|862.3  |\n|8       |3          |2025-12-24 03:44:29|1166.53|\n|6       |3          |2025-12-30 03:44:29|1756.15|\n|19      |8          |2026-01-06 03:44:29|425.86 |\n|25      |10         |2026-01-08 03:44:29|711.7  |\n|22      |9          |2026-01-09 03:44:29|617.47 |\n+--------+-----------+-------------------+-------+\n\n\nTotal orders: 26\n"
     ]
    }
   ],
   "source": [
    "# Create orders table\n",
    "# Schema: order_id, customer_id, order_ts, amount\n",
    "# We'll create orders spanning the last 120 days to have data beyond the 90-day window\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "# Generate orders over the last 120 days\n",
    "orders_data = []\n",
    "order_id = 1\n",
    "\n",
    "# Create orders for each customer across different dates\n",
    "for customer_id in range(1, 11):\n",
    "    # Create 2-4 orders per customer at different dates\n",
    "    num_orders = random.randint(2, 4)\n",
    "    for _ in range(num_orders):\n",
    "        # Random date within last 120 days\n",
    "        days_ago = random.randint(0, 120)\n",
    "        order_date = current_ts - timedelta(days=days_ago)\n",
    "        order_ts = order_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        amount = round(random.uniform(100, 2000), 2)\n",
    "        orders_data.append((order_id, customer_id, order_ts, amount))\n",
    "        order_id += 1\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_ts\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(orders_data, orders_schema)\n",
    "# Convert order_ts to timestamp type\n",
    "df_orders = df_orders.withColumn(\"order_ts\", to_timestamp(col(\"order_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "print(\"Orders table created:\")\n",
    "df_orders.orderBy(\"order_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal orders: {df_orders.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b633e8-4fb7-4e90-8413-e58343044636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payments table created:\n+----------+--------+-------+-------------------+\n|payment_id|order_id|amount |paid_ts            |\n+----------+--------+-------+-------------------+\n|2         |1       |327.64 |2025-11-11 14:44:29|\n|1         |1       |42.77  |2025-11-27 03:44:29|\n|3         |2       |1062.88|2025-09-27 09:44:29|\n|4         |3       |988.53 |2026-01-04 14:44:29|\n|5         |4       |1955.52|2025-11-01 07:44:29|\n|6         |5       |645.93 |2025-12-27 08:44:29|\n|7         |5       |216.37 |2026-01-11 04:44:29|\n|8         |6       |738.81 |2026-01-11 12:44:29|\n|9         |6       |1017.34|2026-01-15 14:44:29|\n|10        |7       |1956.53|2025-10-26 15:44:29|\n|11        |8       |1166.53|2026-01-12 21:44:29|\n|12        |9       |1589.54|2025-09-24 03:44:29|\n|13        |10      |1008.83|2025-11-11 02:44:29|\n|14        |11      |61.61  |2025-10-28 07:44:29|\n|16        |11      |216.84 |2025-11-05 14:44:29|\n|15        |11      |27.36  |2025-11-06 00:44:29|\n|17        |12      |1293.37|2025-10-25 12:44:29|\n|19        |13      |9.62   |2025-11-22 12:44:29|\n|20        |13      |46.39  |2025-12-04 21:44:29|\n|18        |13      |169.64 |2025-12-07 11:44:29|\n|23        |14      |319.21 |2025-10-06 16:44:29|\n|21        |14      |255.87 |2025-10-22 11:44:29|\n|22        |14      |1220.79|2025-10-31 11:44:29|\n|24        |15      |1228.31|2026-01-06 06:44:29|\n|25        |16      |965.27 |2025-09-21 21:44:29|\n|26        |17      |1807.68|2025-11-03 12:44:29|\n|27        |18      |1354.15|2025-11-04 12:44:29|\n|30        |19      |147.77 |2026-01-14 12:44:29|\n|29        |19      |61.22  |2026-01-15 01:44:29|\n|28        |19      |216.87 |2026-01-24 07:44:29|\n|31        |20      |1705.49|2025-12-10 19:44:29|\n|32        |21      |569.21 |2025-12-14 10:44:29|\n|34        |21      |1274.29|2025-12-25 19:44:29|\n|33        |21      |99.94  |2026-01-03 09:44:29|\n|35        |22      |617.47 |2026-02-03 06:44:29|\n|37        |23      |152.61 |2025-12-17 12:44:29|\n|36        |23      |6.71   |2026-01-06 23:44:29|\n|38        |24      |586.77 |2025-11-03 11:44:29|\n|41        |25      |243.69 |2026-01-19 00:44:29|\n|39        |25      |95.69  |2026-02-03 02:44:29|\n|40        |25      |372.32 |2026-02-07 22:44:29|\n|42        |26      |1876.35|2025-12-05 10:44:29|\n+----------+--------+-------+-------------------+\n\n\nTotal payments: 42\n\nExample: Multiple payments for order_id = 1:\n+----------+--------+------+-------------------+\n|payment_id|order_id|amount|paid_ts            |\n+----------+--------+------+-------------------+\n|2         |1       |327.64|2025-11-11 14:44:29|\n|1         |1       |42.77 |2025-11-27 03:44:29|\n+----------+--------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create payments table\n",
    "# Schema: payment_id, order_id, amount, paid_ts\n",
    "# Note: An order can have multiple payments (partial payments, refunds, etc.)\n",
    "# We need to identify the LATEST payment per order\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "payments_data = []\n",
    "payment_id = 1\n",
    "\n",
    "# For each order, create 1-3 payments at different times\n",
    "for order_row in orders_data:\n",
    "    order_id = order_row[0]\n",
    "    order_date_str = order_row[2]\n",
    "    order_date = datetime.strptime(order_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    order_amount = order_row[3]\n",
    "    \n",
    "    # Create 1-3 payments per order\n",
    "    num_payments = random.randint(1, 3)\n",
    "    remaining_amount = order_amount\n",
    "    \n",
    "    for i in range(num_payments):\n",
    "        # Payment date is after order date, within 30 days\n",
    "        days_after_order = random.randint(0, 30)\n",
    "        payment_date = order_date + timedelta(days=days_after_order, hours=random.randint(0, 23))\n",
    "        paid_ts = payment_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Last payment gets remaining amount, others are partial\n",
    "        if i == num_payments - 1:\n",
    "            payment_amount = round(remaining_amount, 2)\n",
    "        else:\n",
    "            payment_amount = round(random.uniform(0.1, remaining_amount * 0.8), 2)\n",
    "            remaining_amount -= payment_amount\n",
    "        \n",
    "        payments_data.append((payment_id, order_id, payment_amount, paid_ts))\n",
    "        payment_id += 1\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"paid_ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_payments = spark.createDataFrame(payments_data, payments_schema)\n",
    "# Convert paid_ts to timestamp type\n",
    "df_payments = df_payments.withColumn(\"paid_ts\", to_timestamp(col(\"paid_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_payments.createOrReplaceTempView(\"payments\")\n",
    "\n",
    "print(\"Payments table created:\")\n",
    "df_payments.orderBy(\"order_id\", \"paid_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal payments: {df_payments.count()}\")\n",
    "\n",
    "# Show example: multiple payments for same order\n",
    "print(\"\\nExample: Multiple payments for order_id = 1:\")\n",
    "df_payments.filter(col(\"order_id\") == 1).orderBy(\"paid_ts\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "544330c2-97a1-4a94-a4d7-d1452b476bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Practice Question\n",
    "\n",
    "### Task 1: Monthly Revenue by Region (Last 90 Days)\n",
    "\n",
    "**Requirement**: For the last 90 days, compute monthly revenue by region based on the **latest payment per order**.\n",
    "\n",
    "**Key Points to Consider:**\n",
    "1. Filter to last 90 days based on payment date (`paid_ts`)\n",
    "2. For each order, use only the **latest payment** (most recent `paid_ts`)\n",
    "3. Join with customers to get the region\n",
    "4. Group by month and region\n",
    "5. Sum the payment amounts\n",
    "\n",
    "**Tables:**\n",
    "- `customers(customer_id, region)`\n",
    "- `orders(order_id, customer_id, order_ts, amount)`\n",
    "- `payments(payment_id, order_id, amount, paid_ts)`\n",
    "\n",
    "**Expected Output Columns:**\n",
    "- `month` (e.g., \"2024-01\", \"2024-02\")\n",
    "- `region`\n",
    "- `revenue` (sum of latest payment amounts)\n",
    "\n",
    "**Hints:**\n",
    "- Use window functions to identify the latest payment per order\n",
    "- Consider using CTEs (Common Table Expressions) to break down the problem\n",
    "- Remember to filter by date before applying window functions for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f92598b9-82a6-426f-99bb-a42df0776427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08379acb-29bb-4422-a0f3-039c110140eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Write your Spark SQL query or PySpark code to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecee3bcf-427b-42bc-bc5c-7521fa496f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Verification Queries\n",
    "\n",
    "Use these queries to verify your understanding and check intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7403f116-b207-4c97-b5b5-e738ef3bc622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: How many payments per order (to verify multiple payments exist)\n",
    "print(\"Payments per order (sample):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        COUNT(*) as payment_count,\n",
    "        MIN(paid_ts) as first_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        SUM(amount) as total_paid\n",
    "    FROM payments\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    "    ORDER BY payment_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dae9988d-42d5-4ff6-9204-2b3d5715af8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: Date range of payments in last 90 days\n",
    "print(\"Payment date range (last 90 days):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(paid_ts) as earliest_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        COUNT(DISTINCT DATE_FORMAT(paid_ts, 'yyyy-MM')) as distinct_months,\n",
    "        COUNT(*) as total_payments\n",
    "    FROM payments\n",
    "    WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b3a5e0-e35a-4285-a671-5d719a324cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: Sample data exploration\n",
    "print(\"Sample customers:\")\n",
    "df_customers.show()\n",
    "\n",
    "print(\"\\nSample orders:\")\n",
    "df_orders.show(10)\n",
    "\n",
    "print(\"\\nSample payments:\")\n",
    "df_payments.show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_spark_sql_practice",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}