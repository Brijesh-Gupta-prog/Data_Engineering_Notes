{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82ac2ce",
   "metadata": {},
   "source": [
    "# Spark SQL Practice - Revenue Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains a practice question on computing monthly revenue by region using Spark SQL. This exercise will help you master:\n",
    "\n",
    "- Date filtering and window functions\n",
    "- Handling latest records per group\n",
    "- Joins across multiple tables\n",
    "- Monthly aggregation with date functions\n",
    "- Complex business logic implementation\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **In Databricks**: SparkSession is automatically available as `spark`\n",
    "2. **For local testing**: Uncomment the SparkSession creation code in the setup cell\n",
    "3. Run the data setup cells first to create sample data\n",
    "4. Complete the exercise in the provided code cell\n",
    "5. Review the solution and explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce71d49",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up all the sample data needed for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Databricks, SparkSession is already available\n",
    "# For local testing, uncomment the following:\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark SQL Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp, expr, date_sub, date_format, sum as spark_sum, max as spark_max, row_number, window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Setup complete! SparkSession ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customers table\n",
    "# Schema: customer_id, region\n",
    "\n",
    "customers_data = [\n",
    "    (1, \"North\"),\n",
    "    (2, \"South\"),\n",
    "    (3, \"East\"),\n",
    "    (4, \"West\"),\n",
    "    (5, \"North\"),\n",
    "    (6, \"South\"),\n",
    "    (7, \"East\"),\n",
    "    (8, \"West\"),\n",
    "    (9, \"North\"),\n",
    "    (10, \"South\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.createDataFrame(customers_data, customers_schema)\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "print(\"Customers table created:\")\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209045f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create orders table\n",
    "# Schema: order_id, customer_id, order_ts, amount\n",
    "# We'll create orders spanning the last 120 days to have data beyond the 90-day window\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "# Generate orders over the last 120 days\n",
    "orders_data = []\n",
    "order_id = 1\n",
    "\n",
    "# Create orders for each customer across different dates\n",
    "for customer_id in range(1, 11):\n",
    "    # Create 2-4 orders per customer at different dates\n",
    "    num_orders = random.randint(2, 4)\n",
    "    for _ in range(num_orders):\n",
    "        # Random date within last 120 days\n",
    "        days_ago = random.randint(0, 120)\n",
    "        order_date = current_ts - timedelta(days=days_ago)\n",
    "        order_ts = order_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        amount = round(random.uniform(100, 2000), 2)\n",
    "        orders_data.append((order_id, customer_id, order_ts, amount))\n",
    "        order_id += 1\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_ts\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(orders_data, orders_schema)\n",
    "# Convert order_ts to timestamp type\n",
    "df_orders = df_orders.withColumn(\"order_ts\", to_timestamp(col(\"order_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "print(\"Orders table created:\")\n",
    "df_orders.orderBy(\"order_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal orders: {df_orders.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcff4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payments table\n",
    "# Schema: payment_id, order_id, amount, paid_ts\n",
    "# Note: An order can have multiple payments (partial payments, refunds, etc.)\n",
    "# We need to identify the LATEST payment per order\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "payments_data = []\n",
    "payment_id = 1\n",
    "\n",
    "# For each order, create 1-3 payments at different times\n",
    "for order_row in orders_data:\n",
    "    order_id = order_row[0]\n",
    "    order_date_str = order_row[2]\n",
    "    order_date = datetime.strptime(order_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    order_amount = order_row[3]\n",
    "    \n",
    "    # Create 1-3 payments per order\n",
    "    num_payments = random.randint(1, 3)\n",
    "    remaining_amount = order_amount\n",
    "    \n",
    "    for i in range(num_payments):\n",
    "        # Payment date is after order date, within 30 days\n",
    "        days_after_order = random.randint(0, 30)\n",
    "        payment_date = order_date + timedelta(days=days_after_order, hours=random.randint(0, 23))\n",
    "        paid_ts = payment_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Last payment gets remaining amount, others are partial\n",
    "        if i == num_payments - 1:\n",
    "            payment_amount = round(remaining_amount, 2)\n",
    "        else:\n",
    "            payment_amount = round(random.uniform(0.1, remaining_amount * 0.8), 2)\n",
    "            remaining_amount -= payment_amount\n",
    "        \n",
    "        payments_data.append((payment_id, order_id, payment_amount, paid_ts))\n",
    "        payment_id += 1\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"paid_ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_payments = spark.createDataFrame(payments_data, payments_schema)\n",
    "# Convert paid_ts to timestamp type\n",
    "df_payments = df_payments.withColumn(\"paid_ts\", to_timestamp(col(\"paid_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_payments.createOrReplaceTempView(\"payments\")\n",
    "\n",
    "print(\"Payments table created:\")\n",
    "df_payments.orderBy(\"order_id\", \"paid_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal payments: {df_payments.count()}\")\n",
    "\n",
    "# Show example: multiple payments for same order\n",
    "print(\"\\nExample: Multiple payments for order_id = 1:\")\n",
    "df_payments.filter(col(\"order_id\") == 1).orderBy(\"paid_ts\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22e3ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Question\n",
    "\n",
    "### Task 1: Monthly Revenue by Region (Last 90 Days)\n",
    "\n",
    "**Requirement**: For the last 90 days, compute monthly revenue by region based on the **latest payment per order**.\n",
    "\n",
    "**Key Points to Consider:**\n",
    "1. Filter to last 90 days based on payment date (`paid_ts`)\n",
    "2. For each order, use only the **latest payment** (most recent `paid_ts`)\n",
    "3. Join with customers to get the region\n",
    "4. Group by month and region\n",
    "5. Sum the payment amounts\n",
    "\n",
    "**Tables:**\n",
    "- `customers(customer_id, region)`\n",
    "- `orders(order_id, customer_id, order_ts, amount)`\n",
    "- `payments(payment_id, order_id, amount, paid_ts)`\n",
    "\n",
    "**Expected Output Columns:**\n",
    "- `month` (e.g., \"2024-01\", \"2024-02\")\n",
    "- `region`\n",
    "- `revenue` (sum of latest payment amounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Write your Spark SQL query or PySpark code to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a9b97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "Below is the solution with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fe531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution using Spark SQL\n",
    "\n",
    "solution_query = \"\"\"\n",
    "WITH latest_payments AS (\n",
    "    -- Step 1: Get the latest payment for each order\n",
    "    SELECT \n",
    "        order_id,\n",
    "        amount,\n",
    "        paid_ts,\n",
    "        ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY paid_ts DESC) as rn\n",
    "    FROM payments\n",
    "    WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "),\n",
    "latest_payment_per_order AS (\n",
    "    -- Step 2: Filter to only the latest payment (rn = 1)\n",
    "    SELECT \n",
    "        order_id,\n",
    "        amount as payment_amount,\n",
    "        paid_ts\n",
    "    FROM latest_payments\n",
    "    WHERE rn = 1\n",
    "),\n",
    "orders_with_payments AS (\n",
    "    -- Step 3: Join orders with latest payments and customers\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        lp.payment_amount,\n",
    "        lp.paid_ts,\n",
    "        c.region\n",
    "    FROM orders o\n",
    "    INNER JOIN latest_payment_per_order lp ON o.order_id = lp.order_id\n",
    "    INNER JOIN customers c ON o.customer_id = c.customer_id\n",
    ")\n",
    "-- Step 4: Group by month and region, sum the revenue\n",
    "SELECT \n",
    "    DATE_FORMAT(paid_ts, 'yyyy-MM') as month,\n",
    "    region,\n",
    "    ROUND(SUM(payment_amount), 2) as revenue\n",
    "FROM orders_with_payments\n",
    "GROUP BY DATE_FORMAT(paid_ts, 'yyyy-MM'), region\n",
    "ORDER BY month DESC, region\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(solution_query)\n",
    "print(\"Monthly Revenue by Region (Last 90 Days):\")\n",
    "result.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Solution using PySpark DataFrame API\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Filter payments to last 90 days\n",
    "current_date = spark.sql(\"SELECT current_timestamp() as current_ts\").collect()[0]['current_ts']\n",
    "df_payments_filtered = df_payments.filter(col(\"paid_ts\") >= current_date - expr(\"INTERVAL 90 DAYS\"))\n",
    "\n",
    "# Step 2: Get latest payment per order using window function\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"paid_ts\").desc())\n",
    "df_latest_payments = df_payments_filtered.withColumn(\n",
    "    \"rn\", \n",
    "    row_number().over(window_spec)\n",
    ").filter(col(\"rn\") == 1).select(\"order_id\", col(\"amount\").alias(\"payment_amount\"), \"paid_ts\")\n",
    "\n",
    "# Step 3: Join with orders and customers\n",
    "df_result = df_latest_payments.join(\n",
    "    df_orders, \n",
    "    \"order_id\", \n",
    "    \"inner\"\n",
    ").join(\n",
    "    df_customers, \n",
    "    \"customer_id\", \n",
    "    \"inner\"\n",
    ").select(\n",
    "    date_format(\"paid_ts\", \"yyyy-MM\").alias(\"month\"),\n",
    "    \"region\",\n",
    "    \"payment_amount\"\n",
    ")\n",
    "\n",
    "# Step 4: Group by month and region, sum revenue\n",
    "df_final = df_result.groupBy(\"month\", \"region\").agg(\n",
    "    spark_sum(\"payment_amount\").alias(\"revenue\")\n",
    ").orderBy(col(\"month\").desc(), \"region\")\n",
    "\n",
    "print(\"Monthly Revenue by Region (Last 90 Days) - PySpark API:\")\n",
    "df_final.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f594f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Detailed Explanations\n",
    "\n",
    "### Key Concepts and Scenarios\n",
    "\n",
    "#### 1. **Why \"Latest Payment per Order\"?**\n",
    "\n",
    "In real-world scenarios, an order can have multiple payments:\n",
    "- **Partial payments**: Customer pays in installments\n",
    "- **Refunds and adjustments**: Payments may be refunded and re-processed\n",
    "- **Payment retries**: Failed payments that are retried later\n",
    "\n",
    "The business requirement asks for revenue based on the **latest payment**, which represents the final state of the order payment. This ensures:\n",
    "- We don't double-count if there are multiple payments\n",
    "- We use the most up-to-date payment status\n",
    "- We handle refunds correctly (if latest payment is negative, it's a refund)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Order 1:\n",
    "  Payment 1: $500 on 2024-01-15\n",
    "  Payment 2: $300 on 2024-01-20 (latest)\n",
    "  \n",
    "We use: $300 (not $800, not $500)\n",
    "```\n",
    "\n",
    "#### 2. **Date Filtering: Last 90 Days**\n",
    "\n",
    "The requirement says \"for the last 90 days\". This can be interpreted in two ways:\n",
    "- **Option A**: Payments made in the last 90 days (using `paid_ts`)\n",
    "- **Option B**: Orders placed in the last 90 days (using `order_ts`)\n",
    "\n",
    "**We use Option A** (payment date) because:\n",
    "- Revenue is recognized when payment is received\n",
    "- The question specifically mentions \"based on the latest payment\"\n",
    "- This aligns with accounting principles (cash basis)\n",
    "\n",
    "**SQL Filter:**\n",
    "```sql\n",
    "WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "```\n",
    "\n",
    "#### 3. **Window Function: ROW_NUMBER()**\n",
    "\n",
    "We use `ROW_NUMBER()` to identify the latest payment per order:\n",
    "\n",
    "```sql\n",
    "ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY paid_ts DESC) as rn\n",
    "```\n",
    "\n",
    "- `PARTITION BY order_id`: Creates separate windows for each order\n",
    "- `ORDER BY paid_ts DESC`: Orders payments by date (newest first)\n",
    "- `rn = 1`: Selects the first row in each window (the latest payment)\n",
    "\n",
    "**Alternative approaches:**\n",
    "- `RANK()` or `DENSE_RANK()`: Not suitable here (they handle ties differently)\n",
    "- Subquery with MAX: Less efficient, more complex\n",
    "- Self-join: More complex and potentially slower\n",
    "\n",
    "#### 4. **Monthly Aggregation**\n",
    "\n",
    "We use `DATE_FORMAT(paid_ts, 'yyyy-MM')` to extract year-month:\n",
    "- Groups all payments in the same calendar month together\n",
    "- Format \"yyyy-MM\" ensures proper sorting (e.g., \"2024-01\" < \"2024-02\")\n",
    "- Alternative: `TRUNC(paid_ts, 'MONTH')` or `DATE_TRUNC('month', paid_ts)`\n",
    "\n",
    "#### 5. **Join Strategy**\n",
    "\n",
    "The join order matters for performance:\n",
    "1. Filter payments first (reduces data size)\n",
    "2. Get latest payment per order (further reduces data)\n",
    "3. Join with orders (adds customer_id)\n",
    "4. Join with customers (adds region)\n",
    "\n",
    "**Why INNER JOIN?**\n",
    "- We only want orders that have payments (revenue)\n",
    "- Orders without payments don't contribute to revenue\n",
    "- If you need all orders (including unpaid), use LEFT JOIN\n",
    "\n",
    "#### 6. **Edge Cases to Consider**\n",
    "\n",
    "**Scenario 1: Order with no payments**\n",
    "- These orders are excluded (INNER JOIN)\n",
    "- If business requires including them, use LEFT JOIN with COALESCE\n",
    "\n",
    "**Scenario 2: Multiple payments with same timestamp**\n",
    "- `ROW_NUMBER()` will arbitrarily pick one\n",
    "- If business requires summing all payments at latest timestamp, use a different approach:\n",
    "  ```sql\n",
    "  WITH max_paid_ts AS (\n",
    "    SELECT order_id, MAX(paid_ts) as latest_ts\n",
    "    FROM payments\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT p.order_id, SUM(p.amount) as total_amount\n",
    "  FROM payments p\n",
    "  JOIN max_paid_ts m ON p.order_id = m.order_id AND p.paid_ts = m.latest_ts\n",
    "  GROUP BY p.order_id\n",
    "  ```\n",
    "\n",
    "**Scenario 3: Negative payments (refunds)**\n",
    "- Current solution handles this correctly (sums negative amounts)\n",
    "- If you want to exclude refunds, add: `WHERE amount > 0`\n",
    "\n",
    "**Scenario 4: Orders outside 90-day window but payments inside**\n",
    "- Current solution includes these (filters on payment date)\n",
    "- If you want to exclude orders older than 90 days, add:\n",
    "  ```sql\n",
    "  WHERE o.order_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "  ```\n",
    "\n",
    "#### 7. **Performance Optimization Tips**\n",
    "\n",
    "1. **Filter early**: Apply date filter before window function\n",
    "2. **Partition wisely**: If data is large, consider partitioning by date\n",
    "3. **Index/Stats**: Ensure proper statistics on join columns\n",
    "4. **Broadcast small tables**: If customers table is small, use broadcast join\n",
    "5. **Caching**: If query is run multiple times, cache intermediate results\n",
    "\n",
    "```python\n",
    "# Example: Broadcast join for small customers table\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_result = df_latest_payments.join(\n",
    "    broadcast(df_customers), \n",
    "    \"customer_id\", \n",
    "    \"inner\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1802353",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20b9a6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verification Queries\n",
    "\n",
    "Run these queries to verify your understanding and check intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f677769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: How many payments per order (to verify multiple payments exist)\n",
    "print(\"Payments per order (sample):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        COUNT(*) as payment_count,\n",
    "        MIN(paid_ts) as first_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        SUM(amount) as total_paid\n",
    "    FROM payments\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    "    ORDER BY payment_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74986d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: Latest payment per order (intermediate result)\n",
    "print(\"Latest payment per order (sample):\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH latest_payments AS (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            amount,\n",
    "            paid_ts,\n",
    "            ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY paid_ts DESC) as rn\n",
    "        FROM payments\n",
    "        WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "    )\n",
    "    SELECT \n",
    "        order_id,\n",
    "        amount,\n",
    "        paid_ts\n",
    "    FROM latest_payments\n",
    "    WHERE rn = 1\n",
    "    ORDER BY paid_ts DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb65791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: Date range of payments in last 90 days\n",
    "print(\"Payment date range (last 90 days):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(paid_ts) as earliest_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        COUNT(DISTINCT DATE_FORMAT(paid_ts, 'yyyy-MM')) as distinct_months,\n",
    "        COUNT(*) as total_payments\n",
    "    FROM payments\n",
    "    WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e122d6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This exercise covered:\n",
    "\n",
    "✅ **Window Functions**: Using `ROW_NUMBER()` to get latest record per group\n",
    "\n",
    "✅ **Date Filtering**: Filtering data based on date ranges (last 90 days)\n",
    "\n",
    "✅ **Multi-table Joins**: Joining customers, orders, and payments tables\n",
    "\n",
    "✅ **Monthly Aggregation**: Grouping by month and region\n",
    "\n",
    "✅ **Business Logic**: Understanding \"latest payment per order\" requirement\n",
    "\n",
    "✅ **CTEs (Common Table Expressions)**: Breaking down complex queries into readable steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always clarify business requirements**: \"Latest payment\" vs \"all payments\" makes a big difference\n",
    "2. **Filter early**: Apply date filters before expensive operations like window functions\n",
    "3. **Use window functions wisely**: `ROW_NUMBER()` is perfect for \"latest per group\" scenarios\n",
    "4. **Test edge cases**: Multiple payments, missing data, date boundaries\n",
    "5. **Consider performance**: Join order and filtering strategy impact query performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Try modifying the solution to:\n",
    "- Include orders with no payments (using LEFT JOIN)\n",
    "- Exclude refunds (negative payments)\n",
    "- Show daily revenue instead of monthly\n",
    "- Add year-over-year comparison\n",
    "- Include order count alongside revenue"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
