{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3797b3b2-382b-4ea6-9599-826dc9b62207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise: Advanced Delta Lake Operations with Analytics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this exercise, you will practice:\n",
    "- Creating and managing multiple Delta tables\n",
    "- Performing MERGE operations with complex scenarios\n",
    "- Handling schema evolution across multiple tables\n",
    "- Using time travel for data auditing\n",
    "- Performing joins between Delta tables\n",
    "- Using aggregations and window functions\n",
    "- Creating pivot tables for analytics\n",
    "- Building a complete data pipeline with Delta Lake\n",
    "\n",
    "## Scenario: E-Commerce Sales Analytics System\n",
    "\n",
    "You are building a sales analytics system for an e-commerce platform. The system needs to:\n",
    "1. Track customer orders and product sales\n",
    "2. Handle updates to order status and product information\n",
    "3. Evolve schemas as new attributes are added\n",
    "4. Generate analytics reports using joins, aggregations, and window functions\n",
    "5. Create pivot tables for business intelligence\n",
    "\n",
    "## Data Model\n",
    "\n",
    "- **customers_delta**: Customer master data\n",
    "- **products_delta**: Product catalog\n",
    "- **orders_delta**: Order transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5511a588-5799-4d6f-a165-6a30c745d860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Initial Setup\n",
    "\n",
    "Run the cells below to set up the initial environment and create sample data for all three tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd76ee61-5428-426f-9e69-0df03ee9b632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, sum, count, row_number, rank, dense_rank, window, year, month, when\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Define Delta table paths\n",
    "base_path = \"/Volumes/workspace/default/databricks_demo/analytics_exercise\"\n",
    "customers_path = f\"{base_path}/customers_delta\"\n",
    "products_path = f\"{base_path}/products_delta\"\n",
    "orders_path = f\"{base_path}/orders_delta\"\n",
    "\n",
    "# Clean up any existing tables (for fresh start)\n",
    "for path in [customers_path, products_path, orders_path]:\n",
    "    try:\n",
    "        dbutils.fs.rm(path, True)\n",
    "        print(f\"Cleaned up existing data at {path}\")\n",
    "    except:\n",
    "        print(f\"No existing data to clean up at {path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e4cd32-c4c9-4b64-99fe-e8987496beb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create initial customers data\n",
    "customers_data = [\n",
    "    (1, \"John Smith\", \"john.smith@email.com\", \"New York\", \"USA\"),\n",
    "    (2, \"Emma Johnson\", \"emma.j@email.com\", \"London\", \"UK\"),\n",
    "    (3, \"Michael Chen\", \"m.chen@email.com\", \"San Francisco\", \"USA\"),\n",
    "    (4, \"Sarah Williams\", \"sarah.w@email.com\", \"Toronto\", \"Canada\"),\n",
    "    (5, \"David Brown\", \"david.b@email.com\", \"Sydney\", \"Australia\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, customers_schema)\n",
    "\n",
    "print(\"Initial customers data:\")\n",
    "customers_df.show()\n",
    "print(f\"\\nTotal customers: {customers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff5bf1d-76a9-45ea-82de-6e5752c48aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create initial products data\n",
    "products_data = [\n",
    "    (101, \"Laptop Pro\", \"Electronics\", 1299.99, 50),\n",
    "    (102, \"Wireless Mouse\", \"Accessories\", 29.99, 200),\n",
    "    (103, \"Mechanical Keyboard\", \"Accessories\", 149.99, 75),\n",
    "    (104, \"USB-C Cable\", \"Accessories\", 19.99, 300),\n",
    "    (105, \"Monitor 27inch\", \"Electronics\", 399.99, 30),\n",
    "    (106, \"Gaming Headset\", \"Electronics\", 199.99, 60)\n",
    "]\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"stock_quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "products_df = spark.createDataFrame(products_data, products_schema)\n",
    "\n",
    "print(\"Initial products data:\")\n",
    "products_df.show()\n",
    "print(f\"\\nTotal products: {products_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e1b701-9bae-46ea-9574-8d3723a36d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create initial orders data\n",
    "orders_data = [\n",
    "    (1001, 1, 101, date(2024, 1, 15), 2, 1299.99, \"Completed\"),\n",
    "    (1002, 2, 102, date(2024, 1, 16), 3, 29.99, \"Completed\"),\n",
    "    (1003, 1, 103, date(2024, 1, 17), 1, 149.99, \"Completed\"),\n",
    "    (1004, 3, 105, date(2024, 1, 18), 1, 399.99, \"Processing\"),\n",
    "    (1005, 4, 104, date(2024, 1, 19), 5, 19.99, \"Completed\"),\n",
    "    (1006, 2, 106, date(2024, 1, 20), 1, 199.99, \"Processing\"),\n",
    "    (1007, 5, 101, date(2024, 1, 21), 1, 1299.99, \"Pending\")\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema)\n",
    "\n",
    "print(\"Initial orders data:\")\n",
    "orders_df.show()\n",
    "print(f\"\\nTotal orders: {orders_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "603f6efd-02ee-45ca-a87e-9f4caa5c80a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 1: Create Delta Tables for All Entities\n",
    "\n",
    "**Task:** Create Delta tables for customers, products, and orders.\n",
    "\n",
    "**Requirements:**\n",
    "- Create three separate Delta tables\n",
    "- Use overwrite mode for initial creation\n",
    "- Verify all tables are created successfully\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89382a1b-36b6-4513-96a8-41d0b646fefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Create Delta tables for customers, products, and orders\n",
    "# Hint: Use .write.format(\"delta\").mode(\"overwrite\").save() for each DataFrame\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.delta_demo.customers_delta\")\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.delta_demo.products_delta\")\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.delta_demo.orders_delta\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e013c55-12b1-4d2a-a6d7-34f566c9c0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 1\n",
    "try:\n",
    "    customers_delta = spark.read.format(\"delta\").load(customers_path)\n",
    "    products_delta = spark.read.format(\"delta\").load(products_path)\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    \n",
    "    print(\"‚úÖ All Delta tables created successfully!\")\n",
    "    print(f\"\\nCustomers: {customers_delta.count()} records\")\n",
    "    print(f\"Products: {products_delta.count()} records\")\n",
    "    print(f\"Orders: {orders_delta.count()} records\")\n",
    "    \n",
    "    print(\"\\nCustomers table sample:\")\n",
    "    customers_delta.show(3)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 1 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0e60f97-d9a5-467c-8841-82d5990918c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 2: Perform MERGE Operations on Orders\n",
    "\n",
    "**Task:** Handle order updates and new orders using MERGE.\n",
    "\n",
    "**Scenario:** You receive a batch of order updates:\n",
    "- Order 1004: Status changed from \"Processing\" to \"Completed\"\n",
    "- Order 1006: Status changed from \"Processing\" to \"Completed\"\n",
    "- Order 1007: Status changed from \"Pending\" to \"Processing\"\n",
    "- Order 1008: New order - Customer 3, Product 102, Date 2024-01-22, Quantity 2, Price 29.99, Status \"Pending\"\n",
    "- Order 1009: New order - Customer 1, Product 105, Date 2024-01-22, Quantity 1, Price 399.99, Status \"Pending\"\n",
    "\n",
    "**Requirements:**\n",
    "- Create a DataFrame with the updates/new orders\n",
    "- Use MERGE to update existing orders and insert new ones\n",
    "- Match on `order_id`\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b35d8c-ba88-4a3b-b07b-4ab3715f9efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Create updates DataFrame for orders\n",
    "\n",
    "\n",
    "# Step 2: Perform MERGE operation on orders_delta\n",
    "# Hint: Use DeltaTable.forPath() and .merge() with whenMatchedUpdateAll() and whenNotMatchedInsertAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79613aae-e776-4327-babe-bf9cc61a66de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 2\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    \n",
    "    print(\"‚úÖ MERGE operation completed!\")\n",
    "    print(f\"\\nTotal orders: {orders_delta.count()}\")\n",
    "    \n",
    "    # Check specific updates\n",
    "    print(\"\\nVerifying updates:\")\n",
    "    order_1004 = orders_delta.filter(col(\"order_id\") == 1004).collect()\n",
    "    if order_1004 and order_1004[0][\"status\"] == \"Completed\":\n",
    "        print(\"‚úÖ Order 1004 status updated correctly\")\n",
    "    \n",
    "    order_1008 = orders_delta.filter(col(\"order_id\") == 1008).collect()\n",
    "    if order_1008:\n",
    "        print(\"‚úÖ Order 1008 (new order) inserted correctly\")\n",
    "    \n",
    "    print(\"\\nUpdated orders table:\")\n",
    "    orders_delta.orderBy(\"order_id\").show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b1a920-bad4-4d79-8b19-a87044db57c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 3: Join Operations - Create Enriched Order View\n",
    "\n",
    "**Task:** Create an enriched view of orders by joining with customers and products tables.\n",
    "\n",
    "**Requirements:**\n",
    "- Join orders with customers on `customer_id`\n",
    "- Join the result with products on `product_id`\n",
    "- Select: order_id, customer_name, email, product_name, category, order_date, quantity, unit_price, status\n",
    "- Calculate total_amount as quantity * unit_price\n",
    "- Show only completed orders\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b68a5e7-ed20-4fec-82f1-75315114935f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Read all three Delta tables\n",
    "\n",
    "\n",
    "# Step 2: Join orders with customers\n",
    "\n",
    "\n",
    "# Step 3: Join with products and select required columns\n",
    "# Hint: Use .join() with join condition, then .select() and .withColumn() for calculated fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a855280-bea9-4d57-9064-ef9d1bdcbc6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 3\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    customers_delta = spark.read.format(\"delta\").load(customers_path)\n",
    "    products_delta = spark.read.format(\"delta\").load(products_path)\n",
    "    \n",
    "    # Expected join result\n",
    "    enriched = orders_delta.join(customers_delta, \"customer_id\") \\\n",
    "                           .join(products_delta, \"product_id\") \\\n",
    "                           .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
    "                           .filter(col(\"status\") == \"Completed\") \\\n",
    "                           .select(\"order_id\", \"customer_name\", \"email\", \"product_name\", \n",
    "                                  \"category\", \"order_date\", \"quantity\", \"unit_price\", \n",
    "                                  \"total_amount\", \"status\")\n",
    "    \n",
    "    print(\"‚úÖ Join operation completed!\")\n",
    "    print(f\"\\nCompleted orders: {enriched.count()}\")\n",
    "    print(\"\\nEnriched order view:\")\n",
    "    enriched.orderBy(\"order_id\").show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b06288cd-2cce-4b79-bdfd-2c2d1440b59a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 4: Aggregations - Sales Summary by Category\n",
    "\n",
    "**Task:** Calculate sales aggregations by product category.\n",
    "\n",
    "**Requirements:**\n",
    "- Join orders with products\n",
    "- Filter for completed orders only\n",
    "- Calculate total_amount for each order (quantity * unit_price)\n",
    "- Group by category\n",
    "- Calculate: total_sales, order_count, avg_order_value, total_quantity_sold\n",
    "- Order by total_sales descending\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96468dec-93b5-4bf8-bb55-a9bc37258f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Join orders with products and calculate total_amount\n",
    "\n",
    "\n",
    "# Step 2: Group by category and calculate aggregations\n",
    "# Hint: Use .groupBy() with .agg() for sum(), count(), avg(), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b2e2c9-83b7-4722-8307-265b3c4b84a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 4\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    products_delta = spark.read.format(\"delta\").load(products_path)\n",
    "    \n",
    "    # Expected aggregation result\n",
    "    sales_summary = orders_delta.join(products_delta, \"product_id\") \\\n",
    "                                .filter(col(\"status\") == \"Completed\") \\\n",
    "                                .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
    "                                .groupBy(\"category\") \\\n",
    "                                .agg(\n",
    "                                    sum(\"total_amount\").alias(\"total_sales\"),\n",
    "                                    count(\"order_id\").alias(\"order_count\"),\n",
    "                                    (sum(\"total_amount\") / count(\"order_id\")).alias(\"avg_order_value\"),\n",
    "                                    sum(\"quantity\").alias(\"total_quantity_sold\")\n",
    "                                ) \\\n",
    "                                .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    print(\"‚úÖ Aggregation completed!\")\n",
    "    print(\"\\nSales summary by category:\")\n",
    "    sales_summary.show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 4 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b19cf7a-0b1d-425f-a49c-1e5bf0a86fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 5: Window Functions - Customer Ranking and Running Totals\n",
    "\n",
    "**Task:** Use window functions to calculate customer rankings and running totals.\n",
    "\n",
    "**Requirements:**\n",
    "- Join orders with customers\n",
    "- Filter for completed orders\n",
    "- Calculate total_amount per order\n",
    "- For each customer, calculate:\n",
    "  - Total sales (sum of all orders)\n",
    "  - Rank of customers by total sales (use rank())\n",
    "  - Running total of sales ordered by order_date\n",
    "- Show customer_id, customer_name, order_id, order_date, total_amount, customer_total_sales, sales_rank, running_total\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e16c5328-68ed-4259-b519-69e04893e121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Join orders with customers and calculate total_amount\n",
    "\n",
    "\n",
    "# Step 2: Use window functions for ranking and running totals\n",
    "# Hint: \n",
    "# - Use Window.partitionBy() for customer-level aggregations\n",
    "# - Use Window.orderBy() for running totals\n",
    "# - Use rank() or dense_rank() for rankings\n",
    "# - Use sum().over() for running totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d397f7-5c9f-4f31-8338-a05445ebc6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 5\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    customers_delta = spark.read.format(\"delta\").load(customers_path)\n",
    "    \n",
    "    # Expected window function result\n",
    "    window_spec_customer = Window.partitionBy(\"customer_id\")\n",
    "    window_spec_rank = Window.orderBy(col(\"customer_total_sales\").desc())\n",
    "    window_spec_running = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "    \n",
    "    result = orders_delta.join(customers_delta, \"customer_id\") \\\n",
    "                         .filter(col(\"status\") == \"Completed\") \\\n",
    "                         .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
    "                         .withColumn(\"customer_total_sales\", sum(\"total_amount\").over(window_spec_customer)) \\\n",
    "                         .withColumn(\"sales_rank\", rank().over(window_spec_rank)) \\\n",
    "                         .withColumn(\"running_total\", sum(\"total_amount\").over(window_spec_running)) \\\n",
    "                         .select(\"customer_id\", \"customer_name\", \"order_id\", \"order_date\", \n",
    "                                \"total_amount\", \"customer_total_sales\", \"sales_rank\", \"running_total\") \\\n",
    "                         .orderBy(\"customer_id\", \"order_date\")\n",
    "    \n",
    "    print(\"‚úÖ Window functions completed!\")\n",
    "    print(\"\\nCustomer rankings and running totals:\")\n",
    "    result.show(20)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1a3931-622f-4e11-8ed1-ace04d350f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Schema Evolution Setup\n",
    "\n",
    "**Important:** Complete Exercises 1-5 before running the cells below!\n",
    "\n",
    "The following cells simulate a real-world scenario where:\n",
    "1. New attributes are added to the orders table (discount_percentage, shipping_cost)\n",
    "2. You need to evolve the schema to accommodate these new fields\n",
    "3. You'll need to update existing orders and handle new orders with the evolved schema\n",
    "\n",
    "Run the setup cells below to prepare for the schema evolution exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6a9c4e7-c967-4d27-b214-37ccd5694ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate incoming orders with new schema (includes discount_percentage and shipping_cost)\n",
    "# This represents data from an updated order system that tracks discounts and shipping\n",
    "\n",
    "new_orders_data = [\n",
    "    (1010, 2, 103, date(2024, 1, 23), 2, 149.99, \"Pending\", 10.0, 5.99),  # 10% discount\n",
    "    (1011, 3, 104, date(2024, 1, 23), 10, 19.99, \"Pending\", 15.0, 8.99),  # 15% discount\n",
    "    (1012, 4, 106, date(2024, 1, 24), 1, 199.99, \"Pending\", 0.0, 12.99)  # No discount\n",
    "]\n",
    "\n",
    "evolved_orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"discount_percentage\", DoubleType(), True),  # New column\n",
    "    StructField(\"shipping_cost\", DoubleType(), True)  # New column\n",
    "])\n",
    "\n",
    "new_orders_df = spark.createDataFrame(new_orders_data, evolved_orders_schema)\n",
    "\n",
    "print(\"New orders with evolved schema (includes discount_percentage and shipping_cost):\")\n",
    "new_orders_df.show()\n",
    "print(\"\\nNew schema:\")\n",
    "new_orders_df.printSchema()\n",
    "print(\"\\n‚ö†Ô∏è  Note: This data has 2 additional columns that don't exist in the current orders Delta table!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1600e609-2de1-41d2-a697-fad92b05c12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 6: Schema Evolution with mergeSchema\n",
    "\n",
    "**Task:** Evolve the orders Delta table schema to accommodate the new columns (discount_percentage and shipping_cost).\n",
    "\n",
    "**Requirements:**\n",
    "- Append the new orders to the orders Delta table\n",
    "- Use the `mergeSchema` option to allow schema evolution\n",
    "- Verify that existing orders have NULL values for the new columns\n",
    "- Verify that new orders have values for all columns including the new ones\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4d5021-fbaf-4a2c-b99f-7960d1148039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Evolve schema by appending new orders with mergeSchema option\n",
    "# Hint: Use .option(\"mergeSchema\", \"true\") when writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce2c767-cf61-4f91-95dc-08e2c52b3350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 6\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    \n",
    "    print(\"‚úÖ Schema evolution completed!\")\n",
    "    print(\"\\nUpdated schema:\")\n",
    "    orders_delta.printSchema()\n",
    "    \n",
    "    print(\"\\nSample data (showing new columns):\")\n",
    "    orders_delta.select(\"order_id\", \"customer_id\", \"product_id\", \"discount_percentage\", \"shipping_cost\").show(10)\n",
    "    \n",
    "    # Check if schema has new columns\n",
    "    columns = orders_delta.columns\n",
    "    if \"discount_percentage\" in columns and \"shipping_cost\" in columns:\n",
    "        print(\"\\n‚úÖ New columns 'discount_percentage' and 'shipping_cost' added successfully!\")\n",
    "        \n",
    "        # Check that old records have NULL for new columns\n",
    "        old_order = orders_delta.filter(col(\"order_id\") == 1001).collect()[0]\n",
    "        if old_order[\"discount_percentage\"] is None:\n",
    "            print(\"‚úÖ Old orders correctly have NULL for new columns\")\n",
    "        \n",
    "        # Check that new records have values\n",
    "        new_order = orders_delta.filter(col(\"order_id\") == 1010).collect()[0]\n",
    "        if new_order[\"discount_percentage\"] is not None:\n",
    "            print(\"‚úÖ New orders correctly have values for new columns\")\n",
    "    else:\n",
    "        print(\"‚ùå Schema evolution did not work as expected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 6 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d02b9cf9-5f9a-422c-ba6e-e27030075ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: MERGE with Evolved Schema and Source Data Updates\n",
    "\n",
    "Now that the schema has evolved, you'll receive updates that include the new columns.\n",
    "Run the setup cells below to prepare for the final exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc76083-cd63-43e6-96da-c1997dc9e23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate updates to existing orders with evolved schema\n",
    "# Some orders need discount and shipping information added\n",
    "# Some orders need status updates\n",
    "\n",
    "updates_with_schema = [\n",
    "    # Update existing order with new columns\n",
    "    (1001, 1, 101, date(2024, 1, 15), 2, 1299.99, \"Completed\", 5.0, 15.99),  # Add discount and shipping\n",
    "    (1002, 2, 102, date(2024, 1, 16), 3, 29.99, \"Completed\", 0.0, 5.99),  # Add discount and shipping\n",
    "    # New order with full schema\n",
    "    (1013, 5, 105, date(2024, 1, 25), 1, 399.99, \"Pending\", 20.0, 10.99)  # New order with discount\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates_with_schema, evolved_orders_schema)\n",
    "\n",
    "print(\"Updates with evolved schema:\")\n",
    "updates_df.show()\n",
    "print(\"\\n‚ö†Ô∏è  Note: These updates include discount_percentage and shipping_cost for existing orders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4efa2e6-0bcb-48d6-bc59-20acb0f8ce96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 7: MERGE with Evolved Schema\n",
    "\n",
    "**Task:** Perform a MERGE operation that updates existing orders with discount/shipping information and inserts new orders.\n",
    "\n",
    "**Requirements:**\n",
    "- Use MERGE to update existing orders (1001, 1002) with their discount_percentage and shipping_cost\n",
    "- Insert new order (1013)\n",
    "- Match on `order_id`\n",
    "- Update all columns when matched\n",
    "- Insert all columns when not matched\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7548012b-ac4b-4a8a-ab47-8375d8fc65cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Perform MERGE with evolved schema on orders_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6127b04-483d-417a-a9e1-573466110d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 7\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    \n",
    "    print(\"‚úÖ MERGE with evolved schema completed!\")\n",
    "    print(f\"\\nTotal orders: {orders_delta.count()}\")\n",
    "    \n",
    "    # Check order 1001 has discount now\n",
    "    order_1001 = orders_delta.filter(col(\"order_id\") == 1001).collect()[0]\n",
    "    if order_1001[\"discount_percentage\"] == 5.0:\n",
    "        print(\"‚úÖ Order 1001 updated with discount correctly\")\n",
    "    \n",
    "    # Check new order 1013\n",
    "    order_1013 = orders_delta.filter(col(\"order_id\") == 1013).collect()\n",
    "    if order_1013:\n",
    "        print(\"‚úÖ Order 1013 inserted correctly\")\n",
    "    \n",
    "    print(\"\\nSample of updated orders:\")\n",
    "    orders_delta.select(\"order_id\", \"customer_id\", \"product_id\", \"status\", \n",
    "                        \"discount_percentage\", \"shipping_cost\").orderBy(\"order_id\").show(10)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 7 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bad3f21e-7047-45b5-94ae-8463ae736278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 8: Pivot Operation - Sales by Month and Category\n",
    "\n",
    "**Task:** Create a pivot table showing sales by month and product category.\n",
    "\n",
    "**Requirements:**\n",
    "- Join orders with products\n",
    "- Filter for completed orders only\n",
    "- Calculate total_amount (considering discount: quantity * unit_price * (1 - discount_percentage/100) + shipping_cost)\n",
    "- Extract year and month from order_date\n",
    "- Create a pivot table with months as columns and categories as rows\n",
    "- Show total sales for each category-month combination\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f5946b5-1927-48e7-9303-2e11bb9efd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Join orders with products and calculate total_amount with discount\n",
    "\n",
    "\n",
    "# Step 2: Extract year and month, then create pivot table\n",
    "# Hint: \n",
    "# - Use year() and month() functions to extract date parts\n",
    "# - Use .groupBy() with .pivot() to create pivot table\n",
    "# - Use .agg() with sum() for aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586dd122-b133-464a-bb09-b410e3549efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 8\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    products_delta = spark.read.format(\"delta\").load(products_path)\n",
    "    \n",
    "    # Calculate total amount with discount and shipping\n",
    "    orders_with_amount = orders_delta.join(products_delta, \"product_id\") \\\n",
    "                                     .filter(col(\"status\") == \"Completed\") \\\n",
    "                                     .withColumn(\"discount_amount\", \n",
    "                                                when(col(\"discount_percentage\").isNotNull(),\n",
    "                                                     col(\"quantity\") * col(\"unit_price\") * (1 - col(\"discount_percentage\") / 100))\n",
    "                                                .otherwise(col(\"quantity\") * col(\"unit_price\"))) \\\n",
    "                                     .withColumn(\"shipping\", \n",
    "                                                when(col(\"shipping_cost\").isNotNull(), col(\"shipping_cost\"))\n",
    "                                                .otherwise(0.0)) \\\n",
    "                                     .withColumn(\"total_amount\", col(\"discount_amount\") + col(\"shipping\")) \\\n",
    "                                     .withColumn(\"order_year\", year(\"order_date\")) \\\n",
    "                                     .withColumn(\"order_month\", month(\"order_date\"))\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot_table = orders_with_amount.groupBy(\"category\") \\\n",
    "                                   .pivot(\"order_month\") \\\n",
    "                                   .agg(sum(\"total_amount\").alias(\"total_sales\")) \\\n",
    "                                   .orderBy(\"category\")\n",
    "    \n",
    "    print(\"‚úÖ Pivot operation completed!\")\n",
    "    print(\"\\nSales by month and category (pivot table):\")\n",
    "    pivot_table.show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 8 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d18680-3afb-4ae6-afae-862c2cf753b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 9: Time Travel - Compare Sales Before and After Schema Evolution\n",
    "\n",
    "**Task:** Use time travel to compare order data before and after schema evolution.\n",
    "\n",
    "**Requirements:**\n",
    "- Get the history of the orders Delta table\n",
    "- Query a version before schema evolution (version 0 or 1)\n",
    "- Query the current version\n",
    "- Compare the number of orders and total sales between versions\n",
    "- Show how schema evolution affected the data structure\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2493b9c7-8df3-485f-aed6-e121cc4097e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Get table history\n",
    "\n",
    "\n",
    "# Step 2: Query an earlier version (before schema evolution)\n",
    "\n",
    "\n",
    "# Step 3: Query current version and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c923d23-ce40-462e-96d4-e8adfd840318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 9\n",
    "try:\n",
    "    orders_delta = spark.read.format(\"delta\").load(orders_path)\n",
    "    delta_table = DeltaTable.forPath(spark, orders_path)\n",
    "    history = delta_table.history()\n",
    "    \n",
    "    if history.count() > 0:\n",
    "        print(\"‚úÖ History retrieved successfully!\")\n",
    "        print(f\"\\nTotal versions: {history.count()}\")\n",
    "        \n",
    "        # Find version before schema evolution (typically version 0 or 1)\n",
    "        version_before = 0\n",
    "        version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", version_before).load(orders_path)\n",
    "        current = spark.read.format(\"delta\").load(orders_path)\n",
    "        \n",
    "        print(f\"\\nVersion {version_before} (before schema evolution):\")\n",
    "        print(f\"  - Orders: {version_0.count()}\")\n",
    "        print(f\"  - Columns: {len(version_0.columns)}\")\n",
    "        version_0.printSchema()\n",
    "        \n",
    "        print(f\"\\nCurrent version (after schema evolution):\")\n",
    "        print(f\"  - Orders: {current.count()}\")\n",
    "        print(f\"  - Columns: {len(current.columns)}\")\n",
    "        current.printSchema()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Schema evolution added {len(current.columns) - len(version_0.columns)} new columns!\")\n",
    "    else:\n",
    "        print(\"‚ùå No history found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please complete Exercise 9 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a180dad5-5c39-4842-a582-65464e42b092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary and Reflection\n",
    "\n",
    "Congratulations on completing the Advanced Delta Lake Operations exercise! üéâ\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "‚úÖ **Multi-Table Delta Operations** - Created and managed multiple Delta tables\n",
    "\n",
    "‚úÖ **Complex MERGE Operations** - Handled updates and inserts across different scenarios\n",
    "\n",
    "‚úÖ **Join Operations** - Joined multiple Delta tables to create enriched views\n",
    "\n",
    "‚úÖ **Aggregations** - Calculated sales summaries and statistics\n",
    "\n",
    "‚úÖ **Window Functions** - Used rankings and running totals for analytics\n",
    "\n",
    "‚úÖ **Schema Evolution** - Evolved table schemas to accommodate new attributes\n",
    "\n",
    "‚úÖ **Pivot Operations** - Created pivot tables for business intelligence\n",
    "\n",
    "‚úÖ **Time Travel** - Compared data across different versions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Delta Lake supports complex analytics** - You can combine Delta operations with SQL analytics\n",
    "\n",
    "2. **Schema evolution is flexible** - Add new columns without breaking existing queries\n",
    "\n",
    "3. **MERGE handles complex scenarios** - Update existing records and insert new ones efficiently\n",
    "\n",
    "4. **Time travel enables data auditing** - Track changes and compare versions\n",
    "\n",
    "5. **Delta tables work seamlessly with Spark SQL** - All Spark SQL operations work on Delta tables\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Practice with larger, more complex datasets\n",
    "- Explore Delta table optimization (OPTIMIZE, Z-ORDER, partitioning)\n",
    "- Learn about Delta table maintenance (VACUUM, history retention)\n",
    "- Experiment with streaming Delta tables\n",
    "- Build end-to-end data pipelines with Delta Lake"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_exercise_advanced_delta_analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
