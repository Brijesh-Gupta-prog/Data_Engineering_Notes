{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042f6996-5fe8-4f63-88e9-d3a327214af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise: Delta Lake Operations - MERGE, Schema Evolution, and Time Travel\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this exercise, you will practice:\n",
    "- Creating and managing Delta tables\n",
    "- Performing MERGE operations (upserts)\n",
    "- Handling schema evolution\n",
    "- Using time travel to query historical data\n",
    "- Working with data updates and inserts\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are working with a product inventory system. You need to:\n",
    "1. Create an initial Delta table with product data\n",
    "2. Handle incoming updates and new products using MERGE\n",
    "3. Evolve the schema when new attributes are added\n",
    "4. Use time travel to audit changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c0bf8f5-bf5d-44ed-9b8c-75c2e8c8d8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Initial Setup\n",
    "\n",
    "Run the cells below to set up the initial environment and create sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c016033-727d-4532-bd38-c8b6f3cffb05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define the Delta table path\n",
    "delta_path = \"/Volumes/workspace/default/databricks_demo/exercise_products_delta\"\n",
    "\n",
    "# Clean up any existing table (for fresh start)\n",
    "try:\n",
    "    dbutils.fs.rm(delta_path, True)\n",
    "    print(f\"Cleaned up existing data at {delta_path}\")\n",
    "except:\n",
    "    print(f\"No existing data to clean up at {delta_path}\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbfceb8f-466e-47a0-b1a7-cd1266f6e811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create initial product data\n",
    "# Schema: id, product_name, price, quantity_in_stock\n",
    "initial_data = [\n",
    "    (101, \"Laptop Pro\", 1299.99, 50),\n",
    "    (102, \"Wireless Mouse\", 29.99, 200),\n",
    "    (103, \"Mechanical Keyboard\", 149.99, 75),\n",
    "    (104, \"USB-C Cable\", 19.99, 300),\n",
    "    (105, \"Monitor 27inch\", 399.99, 30)\n",
    "]\n",
    "\n",
    "initial_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity_in_stock\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "initial_df = spark.createDataFrame(initial_data, initial_schema)\n",
    "\n",
    "print(\"Initial product data:\")\n",
    "initial_df.show()\n",
    "print(f\"\\nTotal products: {initial_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdce890f-04f5-4357-b0a6-17f6a3b63a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 1: Create Initial Delta Table\n",
    "\n",
    "**Task:** Create a Delta table from the initial DataFrame and save it to `delta_path`.\n",
    "\n",
    "**Requirements:**\n",
    "- Write the DataFrame as Delta format\n",
    "- Use overwrite mode (since we cleaned up earlier)\n",
    "- Verify the table was created by reading it back\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350ce805-625f-40e4-bde8-f39bc0fe1dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Hint: Use .write.format(\"delta\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bcda7fd-9cc2-442c-bff1-f851a0447d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 1\n",
    "try:\n",
    "    verify_df = spark.read.format(\"delta\").load(delta_path)\n",
    "    print(\"✅ Delta table created successfully!\")\n",
    "    print(f\"\\nTable contains {verify_df.count()} products:\")\n",
    "    verify_df.show()\n",
    "    print(\"\\nSchema:\")\n",
    "    verify_df.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please complete Exercise 1 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38dd070b-9d5a-4f72-a0c1-9a698ce465dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 2: Perform MERGE Operation (Upsert)\n",
    "\n",
    "**Task:** Use MERGE to handle updates and inserts.\n",
    "\n",
    "**Scenario:** You receive a batch of product updates:\n",
    "- Product ID 101: Price updated to 1199.99, quantity updated to 45\n",
    "- Product ID 102: Quantity updated to 180\n",
    "- Product ID 106: New product \"Gaming Headset\" with price 199.99 and quantity 60\n",
    "- Product ID 107: New product \"Webcam HD\" with price 89.99 and quantity 100\n",
    "\n",
    "**Requirements:**\n",
    "- Create a DataFrame with the updates/new products\n",
    "- Use DeltaTable.merge() to:\n",
    "  - UPDATE existing records when product_id matches\n",
    "  - INSERT new records when product_id doesn't exist\n",
    "- Match on `product_id`\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37e5d90e-6a32-4535-a89e-d4294c45d317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Create updates DataFrame\n",
    "\n",
    "\n",
    "# Step 2: Perform MERGE operation\n",
    "# Hint: Use DeltaTable.forPath() and .merge() with whenMatchedUpdateAll() and whenNotMatchedInsertAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c88e3d2f-bd43-4eb6-adaa-e499da6e2b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 2\n",
    "try:\n",
    "    verify_df = spark.read.format(\"delta\").load(delta_path)\n",
    "    print(\"✅ MERGE operation completed!\")\n",
    "    print(f\"\\nTable now contains {verify_df.count()} products:\")\n",
    "    verify_df.show()\n",
    "    \n",
    "    # Check specific updates\n",
    "    print(\"\\nVerifying updates:\")\n",
    "    product_101 = verify_df.filter(col(\"product_id\") == 101).collect()\n",
    "    if product_101 and product_101[0][\"price\"] == 1199.99:\n",
    "        print(\"✅ Product 101 price updated correctly\")\n",
    "    \n",
    "    product_106 = verify_df.filter(col(\"product_id\") == 106).collect()\n",
    "    if product_106:\n",
    "        print(\"✅ Product 106 (new product) inserted correctly\")\n",
    "    \n",
    "    product_107 = verify_df.filter(col(\"product_id\") == 107).collect()\n",
    "    if product_107:\n",
    "        print(\"✅ Product 107 (new product) inserted correctly\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please complete Exercise 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04ca10b2-b495-4185-a613-2c65eb3ba23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 3: Time Travel - Query Historical Version\n",
    "\n",
    "**Task:** Use time travel to view the state of the table before the MERGE operation.\n",
    "\n",
    "**Requirements:**\n",
    "- Get the history of the Delta table\n",
    "- Query version 0 (the initial version before MERGE)\n",
    "- Compare it with the current version\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a4a83f-9274-43bb-8c15-9d58173c124e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Step 1: Get table history\n",
    "\n",
    "\n",
    "# Step 2: Query version 0 (initial version)\n",
    "\n",
    "\n",
    "# Step 3: Compare with current version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbd6f1e-4963-468a-9ffa-be38e5695d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 3\n",
    "try:\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "    history = delta_table.history()\n",
    "    \n",
    "    if history.count() > 0:\n",
    "        print(\"✅ History retrieved successfully!\")\n",
    "        print(f\"\\nTotal versions: {history.count()}\")\n",
    "        \n",
    "        version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "        current = spark.read.format(\"delta\").load(delta_path)\n",
    "        \n",
    "        print(f\"\\nVersion 0 had {version_0.count()} products\")\n",
    "        print(f\"Current version has {current.count()} products\")\n",
    "        \n",
    "        if current.count() > version_0.count():\n",
    "            print(\"✅ Time travel working correctly - new products detected!\")\n",
    "    else:\n",
    "        print(\"❌ No history found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please complete Exercise 3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d744c1-63a7-48c8-9b41-5e95f17433a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Schema Evolution Setup\n",
    "\n",
    "**Important:** Complete Exercises 1-3 before running the cells below!\n",
    "\n",
    "The following cells simulate a real-world scenario where:\n",
    "1. New data arrives with additional columns (category, supplier_name)\n",
    "2. You need to evolve the schema to accommodate these new fields\n",
    "3. You'll need to handle MERGE operations with the evolved schema\n",
    "\n",
    "Run the setup cells below to prepare for the schema evolution exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ecabff-0f83-474f-9d25-fd591268cf30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate incoming data with new schema (includes category and supplier_name)\n",
    "# This represents data from a new source system that tracks additional attributes\n",
    "\n",
    "new_schema_data = [\n",
    "    (108, \"SSD 1TB\", 129.99, 150, \"Storage\", \"TechSupplier Inc\"),\n",
    "    (109, \"RAM 16GB\", 89.99, 200, \"Memory\", \"TechSupplier Inc\"),\n",
    "    (110, \"Graphics Card RTX\", 599.99, 25, \"Graphics\", \"GamingTech Ltd\")\n",
    "]\n",
    "\n",
    "evolved_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity_in_stock\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True),  # New column\n",
    "    StructField(\"supplier_name\", StringType(), True)  # New column\n",
    "])\n",
    "\n",
    "new_schema_df = spark.createDataFrame(new_schema_data, evolved_schema)\n",
    "\n",
    "print(\"New data with evolved schema (includes category and supplier_name):\")\n",
    "new_schema_df.show()\n",
    "print(\"\\nNew schema:\")\n",
    "new_schema_df.printSchema()\n",
    "print(\"\\n⚠️  Note: This data has 2 additional columns that don't exist in the current Delta table!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7700865a-c1c5-468b-afa7-bf6ad05baff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 4: Schema Evolution with mergeSchema\n",
    "\n",
    "**Task:** Evolve the Delta table schema to accommodate the new columns (category and supplier_name).\n",
    "\n",
    "**Requirements:**\n",
    "- Append the new data to the Delta table\n",
    "- Use the `mergeSchema` option to allow schema evolution\n",
    "- Verify that existing records have NULL values for the new columns\n",
    "- Verify that new records have values for all columns including the new ones\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2042a4-3248-400b-96f5-08d957ecfeb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Hint: Use .option(\"mergeSchema\", \"true\") when writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecc96de6-b27d-4766-adff-8af639c6bb22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 4\n",
    "try:\n",
    "    verify_df = spark.read.format(\"delta\").load(delta_path)\n",
    "    \n",
    "    print(\"✅ Schema evolution completed!\")\n",
    "    print(\"\\nUpdated schema:\")\n",
    "    verify_df.printSchema()\n",
    "    \n",
    "    print(\"\\nSample data (showing new columns):\")\n",
    "    verify_df.select(\"product_id\", \"product_name\", \"category\", \"supplier_name\").show(10)\n",
    "    \n",
    "    # Check if schema has new columns\n",
    "    columns = verify_df.columns\n",
    "    if \"category\" in columns and \"supplier_name\" in columns:\n",
    "        print(\"\\n✅ New columns 'category' and 'supplier_name' added successfully!\")\n",
    "        \n",
    "        # Check that old records have NULL for new columns\n",
    "        old_record = verify_df.filter(col(\"product_id\") == 101).collect()[0]\n",
    "        if old_record[\"category\"] is None:\n",
    "            print(\"✅ Old records correctly have NULL for new columns\")\n",
    "        \n",
    "        # Check that new records have values\n",
    "        new_record = verify_df.filter(col(\"product_id\") == 108).collect()[0]\n",
    "        if new_record[\"category\"] is not None:\n",
    "            print(\"✅ New records correctly have values for new columns\")\n",
    "    else:\n",
    "        print(\"❌ Schema evolution did not work as expected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please complete Exercise 4 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cad7e99-b3ee-4691-901c-381eb2244bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: MERGE with Evolved Schema\n",
    "\n",
    "Now that the schema has evolved, you'll receive updates that include the new columns.\n",
    "Run the setup cell below to prepare for the final exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97a00815-293c-4ed7-920a-bd55ae4d6604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate updates with evolved schema\n",
    "# Some products need category and supplier information added\n",
    "# Some products need price/quantity updates\n",
    "\n",
    "updates_with_schema = [\n",
    "    # Update existing product with new columns\n",
    "    (101, \"Laptop Pro\", 1199.99, 45, \"Computers\", \"TechSupplier Inc\"),  # Update with category\n",
    "    (102, \"Wireless Mouse\", 29.99, 180, \"Accessories\", \"TechSupplier Inc\"),  # Update with category\n",
    "    # New product with full schema\n",
    "    (111, \"USB Hub\", 34.99, 120, \"Accessories\", \"TechSupplier Inc\")\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates_with_schema, evolved_schema)\n",
    "\n",
    "print(\"Updates with evolved schema:\")\n",
    "updates_df.show()\n",
    "print(\"\\n⚠️  Note: These updates include category and supplier_name for existing products!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae9af7bd-87ea-4b5c-a62f-4b89b9335414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 5: MERGE with Evolved Schema\n",
    "\n",
    "**Task:** Perform a MERGE operation that updates existing products with category/supplier information and inserts new products.\n",
    "\n",
    "**Requirements:**\n",
    "- Use MERGE to update existing products (101, 102) with their category and supplier_name\n",
    "- Insert new product (111)\n",
    "- Match on `product_id`\n",
    "- Update all columns when matched\n",
    "- Insert all columns when not matched\n",
    "\n",
    "**Write your code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffb29daf-8566-4a98-8b3f-c566e3944c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write your code here\n",
    "# Perform MERGE with evolved schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4591225b-3e35-4096-92d8-706b0e8447c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification cell - Run this after completing Exercise 5\n",
    "try:\n",
    "    verify_df = spark.read.format(\"delta\").load(delta_path)\n",
    "    \n",
    "    print(\"✅ MERGE with evolved schema completed!\")\n",
    "    print(f\"\\nTotal products: {verify_df.count()}\")\n",
    "    \n",
    "    # Check product 101 has category now\n",
    "    product_101 = verify_df.filter(col(\"product_id\") == 101).collect()[0]\n",
    "    if product_101[\"category\"] == \"Computers\":\n",
    "        print(\"✅ Product 101 updated with category correctly\")\n",
    "    \n",
    "    # Check product 102 has category now\n",
    "    product_102 = verify_df.filter(col(\"product_id\") == 102).collect()[0]\n",
    "    if product_102[\"category\"] == \"Accessories\":\n",
    "        print(\"✅ Product 102 updated with category correctly\")\n",
    "    \n",
    "    # Check new product 111\n",
    "    product_111 = verify_df.filter(col(\"product_id\") == 111).collect()\n",
    "    if product_111:\n",
    "        print(\"✅ Product 111 inserted correctly\")\n",
    "    \n",
    "    print(\"\\nFinal table state:\")\n",
    "    verify_df.orderBy(\"product_id\").show(15)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please complete Exercise 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d52ae0-7ef1-4a58-8c48-441d68e5614a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary and Reflection\n",
    "\n",
    "Congratulations on completing the Delta Lake operations exercise! \uD83C\uDF89\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "✅ **Delta Table Creation** - Created and managed Delta tables\n",
    "\n",
    "✅ **MERGE Operations** - Performed upserts (update existing, insert new)\n",
    "\n",
    "✅ **Time Travel** - Queried historical versions of data\n",
    "\n",
    "✅ **Schema Evolution** - Evolved table schema to accommodate new columns\n",
    "\n",
    "✅ **Complex MERGE** - Handled MERGE operations with evolved schemas\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Delta Lake provides ACID transactions** - All operations are atomic and consistent\n",
    "\n",
    "2. **MERGE is powerful** - It handles both updates and inserts in a single operation\n",
    "\n",
    "3. **Schema evolution is flexible** - You can add new columns without breaking existing data\n",
    "\n",
    "4. **Time travel enables auditing** - You can always query previous versions of your data\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Practice with larger datasets\n",
    "- Explore Delta table optimization (OPTIMIZE, Z-ORDER)\n",
    "- Learn about Delta table maintenance (VACUUM)\n",
    "- Experiment with partitioning strategies"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_exercise_delta_operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}