{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae04eae4-abdc-4cf1-a583-9c5fc650637e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Compute & Clusters: A Comprehensive Guide\n",
    "\n",
    "This guide provides a comprehensive overview of Databricks compute resources, cluster types, configuration, and best practices for optimizing your data processing workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Fundamentals: Compute vs. Clusters](#1-fundamentals-compute-vs-clusters)\n",
    "2. [Storage and Data Handling](#2-storage-and-data-handling)\n",
    "3. [Cluster Types](#3-cluster-types)\n",
    "4. [Compute Types: Serverless vs. Classic](#4-compute-types-serverless-vs-classic)\n",
    "5. [Cluster Access Modes](#5-cluster-access-modes)\n",
    "6. [High-Performance Engines: Photon & AQE](#6-high-performance-engines-photon--aqe)\n",
    "7. [Cluster Sizing and Configuration](#7-cluster-sizing-and-configuration)\n",
    "8. [Cluster Pools](#8-cluster-pools)\n",
    "9. [Autoscaling](#9-autoscaling)\n",
    "10. [Instance Types and Selection](#10-instance-types-and-selection)\n",
    "11. [Cluster Lifecycle Management](#11-cluster-lifecycle-management)\n",
    "12. [Cost Optimization Strategies](#12-cost-optimization-strategies)\n",
    "13. [Monitoring and Performance](#13-monitoring-and-performance)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Fundamentals: Compute vs. Clusters\n",
    "\n",
    "### What is Compute?\n",
    "\n",
    "**Compute** in Databricks refers to the underlying hardware resources required to process data and run code. It encompasses:\n",
    "- **CPU**: Processing power for executing tasks\n",
    "- **Memory (RAM)**: Temporary storage for data processing\n",
    "- **Storage**: Local disk space for temporary files\n",
    "- **Networking**: Bandwidth for data transfer between nodes and external storage\n",
    "\n",
    "### How is a Cluster Different from Compute?\n",
    "\n",
    "Think of **Compute** as the raw infrastructure (the Virtual Machines) and a **Cluster** as the organized, configured team that uses that infrastructure.\n",
    "\n",
    "**Key Differences:**\n",
    "- **Compute**: The abstract concept of processing resources\n",
    "- **Cluster**: A specific, configured set of compute resources working together as a single unit using Apache Spark\n",
    "\n",
    "**In Practice:**\n",
    "- You **provision compute** by **creating a cluster**\n",
    "- A cluster is a managed Spark environment that coordinates multiple compute nodes\n",
    "- Each cluster has a driver node (coordinates work) and worker nodes (execute tasks)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Storage and Data Handling\n",
    "\n",
    "### Do Clusters Have Storage?\n",
    "\n",
    "Yes, but cluster storage is primarily **ephemeral (temporary)**. Understanding storage architecture is crucial for data engineering workflows.\n",
    "\n",
    "### Local Storage (Ephemeral)\n",
    "\n",
    "Each cluster node has local storage used for:\n",
    "\n",
    "1. **Operating System**: Base OS files and system libraries\n",
    "2. **Application Libraries**: Python packages, JAR files, and other dependencies\n",
    "3. **Spark Shuffling**: Temporary data exchange during Spark operations (joins, aggregations, repartitions)\n",
    "4. **Spill Files**: When memory is full, Spark writes intermediate data to disk\n",
    "5. **Logs**: Application and system logs\n",
    "\n",
    "**Important Notes:**\n",
    "- Local storage is **not persistent** - data is lost when the cluster terminates\n",
    "- Size depends on the instance type (typically 50-200 GB per node)\n",
    "- Fast SSD storage is used for better performance\n",
    "\n",
    "### Persistent Storage (External)\n",
    "\n",
    "For actual data storage, Databricks connects to external cloud object storage:\n",
    "\n",
    "- **AWS**: Amazon S3\n",
    "- **Azure**: Azure Data Lake Storage (ADLS Gen1/Gen2) or Azure Blob Storage\n",
    "- **GCP**: Google Cloud Storage (GCS)\n",
    "\n",
    "**How It Works:**\n",
    "- Clusters **read from** and **write to** cloud storage\n",
    "- Data remains in cloud storage even after cluster termination\n",
    "- Databricks uses optimized connectors (e.g., DBIO) for efficient data access\n",
    "- Delta Lake tables are stored in cloud storage, not on cluster nodes\n",
    "\n",
    "**Best Practice:** Always store your data in cloud storage, not on cluster local storage.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cluster Types\n",
    "\n",
    "Databricks offers different cluster types optimized for different use cases:\n",
    "\n",
    "### 3.1 All-Purpose Clusters\n",
    "\n",
    "**Purpose:** Interactive development and ad-hoc analysis\n",
    "\n",
    "**Characteristics:**\n",
    "- Used for notebooks, interactive queries, and exploratory data analysis\n",
    "- Can be shared among multiple users\n",
    "- Supports multiple languages (Python, R, Scala, SQL)\n",
    "- Customizable Spark configuration\n",
    "- Typically runs for extended periods\n",
    "\n",
    "**Use Cases:**\n",
    "- Data exploration and analysis\n",
    "- Interactive development\n",
    "- Collaborative work on notebooks\n",
    "- Ad-hoc queries\n",
    "\n",
    "### 3.2 Job Clusters\n",
    "\n",
    "**Purpose:** Automated, scheduled, or one-time workloads\n",
    "\n",
    "**Characteristics:**\n",
    "- Created automatically when a job runs\n",
    "- Terminated automatically after job completion\n",
    "- More cost-effective for scheduled workloads\n",
    "- Can be configured with different settings than all-purpose clusters\n",
    "- Supports retry policies and notifications\n",
    "\n",
    "**Use Cases:**\n",
    "- Scheduled ETL pipelines\n",
    "- Automated data processing jobs\n",
    "- Batch transformations\n",
    "- Data quality checks\n",
    "\n",
    "**Key Difference:** Job clusters are ephemeral and created on-demand, while all-purpose clusters are long-lived.\n",
    "\n",
    "### 3.3 SQL Warehouses (formerly SQL Endpoints)\n",
    "\n",
    "**Purpose:** Optimized for SQL queries and BI tool connectivity\n",
    "\n",
    "**Characteristics:**\n",
    "- Specialized architecture for SQL workloads\n",
    "- Low-latency query execution\n",
    "- High concurrency support\n",
    "- Can be \"Always On\" or serverless\n",
    "- Optimized for BI tools (Tableau, Power BI, Looker, etc.)\n",
    "- Does not support Python/Scala/R - SQL only\n",
    "\n",
    "**Use Cases:**\n",
    "- Business intelligence dashboards\n",
    "- Ad-hoc SQL queries\n",
    "- Reporting and analytics\n",
    "- BI tool connectivity\n",
    "\n",
    "**When to Use SQL Warehouse vs. All-Purpose:**\n",
    "- **SQL Warehouse**: Pure SQL workloads, BI tools, high concurrency SQL queries\n",
    "- **All-Purpose**: Multi-language development, data engineering, custom Spark configurations\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Compute Types: Serverless vs. Classic\n",
    "\n",
    "### 4.1 Classic Compute (All-Purpose)\n",
    "\n",
    "**Management:** User-managed infrastructure\n",
    "\n",
    "**Characteristics:**\n",
    "- You select VM instance types\n",
    "- You choose Spark version\n",
    "- You configure scaling limits\n",
    "- You manage cluster lifecycle\n",
    "- Full control over configuration\n",
    "\n",
    "**Startup Time:** ~3-5 minutes (VM provisioning time)\n",
    "\n",
    "**Configuration:**\n",
    "- Instance types (e.g., `Standard_DS3_v2`, `Standard_L8s_v2`)\n",
    "- Spark version selection\n",
    "- Custom Spark configurations\n",
    "- Environment variables\n",
    "- Init scripts\n",
    "\n",
    "**Use Cases:**\n",
    "- When you need specific VM types\n",
    "- Custom Spark configurations\n",
    "- Long-running interactive sessions\n",
    "- Full control over infrastructure\n",
    "\n",
    "### 4.2 Serverless Compute\n",
    "\n",
    "**Management:** Databricks-managed infrastructure\n",
    "\n",
    "**Characteristics:**\n",
    "- Databricks automatically selects and manages VMs\n",
    "- No VM type selection needed\n",
    "- Automatic optimization\n",
    "- Instant startup\n",
    "- Automatic scaling\n",
    "- Pay only for compute time used\n",
    "\n",
    "**Startup Time:** ~10-30 seconds (much faster than classic)\n",
    "\n",
    "**Configuration:**\n",
    "- Simplified configuration\n",
    "- Databricks optimizes resource allocation\n",
    "- Automatic instance type selection\n",
    "- Less control, more convenience\n",
    "\n",
    "**Use Cases:**\n",
    "- Unpredictable workloads\n",
    "- Short-running tasks\n",
    "- Quick iterations\n",
    "- When you want to minimize management overhead\n",
    "- Cost optimization for sporadic workloads\n",
    "\n",
    "**Availability:** Serverless is available for both SQL Warehouses and All-Purpose clusters (depending on your Databricks plan).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Cluster Access Modes\n",
    "\n",
    "Access modes determine how users and applications interact with clusters and what level of isolation is provided.\n",
    "\n",
    "### 5.1 Single User Access Mode\n",
    "\n",
    "**Isolation:** Full isolation per user\n",
    "\n",
    "**Characteristics:**\n",
    "- One user per cluster\n",
    "- Complete isolation of data and libraries\n",
    "- Best for security-sensitive workloads\n",
    "- User-specific environment variables and secrets\n",
    "- No resource sharing conflicts\n",
    "\n",
    "**Use Cases:**\n",
    "- Production workloads with strict security requirements\n",
    "- When users need different library versions\n",
    "- Compliance requirements\n",
    "- Sensitive data processing\n",
    "\n",
    "**Limitations:**\n",
    "- Cannot share clusters among users\n",
    "- Higher cost (one cluster per user)\n",
    "\n",
    "### 5.2 Shared Access Mode\n",
    "\n",
    "**Isolation:** Shared resources with fair scheduling\n",
    "\n",
    "**Characteristics:**\n",
    "- Multiple users can attach to the same cluster\n",
    "- Fair scheduling ensures resource distribution\n",
    "- Shared libraries and environment\n",
    "- Cost-effective for teams\n",
    "- Automatic resource allocation\n",
    "\n",
    "**Use Cases:**\n",
    "- Collaborative development\n",
    "- Team environments\n",
    "- Cost optimization\n",
    "- General-purpose analytics\n",
    "\n",
    "**Best Practices:**\n",
    "- Use when users have similar library requirements\n",
    "- Enable autoscaling for better resource utilization\n",
    "- Monitor for resource contention\n",
    "\n",
    "### 5.3 No Isolation Shared Access Mode\n",
    "\n",
    "**Isolation:** Minimal isolation (legacy mode)\n",
    "\n",
    "**Characteristics:**\n",
    "- Multiple users share the same Spark session\n",
    "- Less isolation than Shared mode\n",
    "- Legacy option (not recommended for new clusters)\n",
    "- Potential security and stability concerns\n",
    "\n",
    "**Recommendation:** Avoid this mode for new clusters. Use Shared Access Mode instead.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. High-Performance Engines: Photon & AQE\n",
    "\n",
    "### 6.1 Photon Engine\n",
    "\n",
    "**What is Photon?**\n",
    "\n",
    "Photon is Databricks' high-performance, vectorized query engine written in **C++**. It's a drop-in replacement for traditional Spark JVM execution that provides significant performance improvements.\n",
    "\n",
    "**Key Features:**\n",
    "- **Vectorized Execution**: Processes data in batches (vectors) rather than row-by-row\n",
    "- **Native Code**: Written in C++ for better CPU utilization\n",
    "- **Optimized Operators**: Faster joins, aggregations, and filters\n",
    "- **Cost Reduction**: More efficient resource usage = lower costs\n",
    "- **Automatic Fallback**: Falls back to JVM Spark for unsupported operations\n",
    "\n",
    "**Performance Benefits:**\n",
    "- 2-10x faster for SQL and DataFrame operations\n",
    "- Better CPU cache utilization\n",
    "- Reduced memory overhead\n",
    "- Lower cloud costs\n",
    "\n",
    "**When Photon is Used:**\n",
    "- SQL queries\n",
    "- DataFrame operations\n",
    "- Most Spark SQL operations\n",
    "- Automatically enabled on supported clusters\n",
    "\n",
    "**Enabling Photon:**\n",
    "- Available on Databricks Runtime 9.1 LTS and above\n",
    "- Enable in cluster configuration: `spark.databricks.photon.enabled = true`\n",
    "- Or use Photon-optimized runtime versions\n",
    "\n",
    "**Limitations:**\n",
    "- Some Spark operations may fall back to JVM\n",
    "- Custom UDFs (User Defined Functions) may not benefit\n",
    "- Some advanced Spark features may not be supported\n",
    "\n",
    "### 6.2 Adaptive Query Execution (AQE)\n",
    "\n",
    "**What is AQE?**\n",
    "\n",
    "Adaptive Query Execution (AQE) is a Spark optimization feature that **dynamically re-optimizes** query execution plans based on real-time statistics collected during query execution.\n",
    "\n",
    "**How AQE Works:**\n",
    "\n",
    "1. **Initial Plan**: Spark creates an initial query execution plan based on table statistics\n",
    "2. **Runtime Statistics**: During execution, AQE collects real-time data about:\n",
    "   - Actual data sizes\n",
    "   - Skew in data distribution\n",
    "   - Join output sizes\n",
    "3. **Dynamic Re-optimization**: AQE adjusts the plan mid-execution:\n",
    "   - Switches join strategies (e.g., Sort-Merge Join → Broadcast Join)\n",
    "   - Adjusts partition counts\n",
    "   - Handles data skew automatically\n",
    "   - Optimizes shuffle partitions\n",
    "\n",
    "**Key Optimizations:**\n",
    "\n",
    "1. **Dynamic Coalescing of Shuffle Partitions**\n",
    "   - Automatically reduces the number of shuffle partitions if data is smaller than expected\n",
    "   - Reduces overhead and improves performance\n",
    "\n",
    "2. **Dynamic Join Selection**\n",
    "   - Switches from expensive Sort-Merge Join to fast Broadcast Join when appropriate\n",
    "   - Example: If a table is smaller than expected, broadcast it instead of doing a sort-merge\n",
    "\n",
    "3. **Dynamic Skew Join Handling**\n",
    "   - Automatically detects and handles skewed data in joins\n",
    "   - Splits large partitions to balance workload\n",
    "\n",
    "**Is AQE in Open Source Spark?**\n",
    "\n",
    "Yes! AQE was introduced in **Apache Spark 3.0** and is available in the open-source version. However:\n",
    "- Databricks includes proprietary enhancements\n",
    "- Better defaults and tuning in Databricks Runtime\n",
    "- More aggressive optimizations\n",
    "- Better integration with Photon\n",
    "\n",
    "**Enabling AQE:**\n",
    "- Enabled by default in Spark 3.0+\n",
    "- Configuration: `spark.sql.adaptive.enabled = true`\n",
    "- Additional settings available for fine-tuning\n",
    "\n",
    "**Benefits:**\n",
    "- Automatic optimization without manual tuning\n",
    "- Handles data skew automatically\n",
    "- Adapts to actual data characteristics\n",
    "- Reduces need for manual partition tuning\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Cluster Sizing and Configuration\n",
    "\n",
    "### 7.1 Understanding Cluster Size\n",
    "\n",
    "Cluster size depends on:\n",
    "- **Data volume**: How much data you're processing\n",
    "- **Query complexity**: Simple aggregations vs. complex joins\n",
    "- **Concurrency**: Number of simultaneous users/queries\n",
    "- **Performance requirements**: Latency vs. throughput needs\n",
    "- **Budget constraints**: Cost considerations\n",
    "\n",
    "### 7.2 Sizing Scenarios\n",
    "\n",
    "#### Scenario A: Small Exploratory Data (1-10 GB)\n",
    "\n",
    "**Characteristics:**\n",
    "- Small datasets\n",
    "- Ad-hoc analysis\n",
    "- Single user or small team\n",
    "- Interactive queries\n",
    "\n",
    "**Recommendation:**\n",
    "- **Single Node Cluster** or small 2-node cluster\n",
    "- Instance type: `Standard_DS3_v2` (4 vCPUs, 14 GB RAM)\n",
    "- No autoscaling needed\n",
    "- Cost-effective for exploration\n",
    "\n",
    "**Configuration Example:**\n",
    "```\n",
    "Cluster Mode: Single Node or Standard\n",
    "Min Workers: 0 (single node) or 1\n",
    "Max Workers: 2\n",
    "Instance Type: Standard_DS3_v2\n",
    "```\n",
    "\n",
    "#### Scenario B: Large ETL/Batch (500 GB - 1 TB)\n",
    "\n",
    "**Characteristics:**\n",
    "- Large data volumes\n",
    "- Scheduled batch processing\n",
    "- Complex transformations\n",
    "- Heavy joins and aggregations\n",
    "\n",
    "**Recommendation:**\n",
    "- **Multi-node cluster with Autoscaling**\n",
    "- Min: 4 nodes, Max: 20 nodes (adjust based on workload)\n",
    "- **Memory-Optimized instances** (r-series) for heavy joins\n",
    "- Instance type: `Standard_L8s_v2` or `Standard_L16s_v2`\n",
    "- Consider job clusters for scheduled workloads\n",
    "\n",
    "**Configuration Example:**\n",
    "```\n",
    "Cluster Mode: Standard\n",
    "Min Workers: 4\n",
    "Max Workers: 20\n",
    "Instance Type: Standard_L8s_v2 (memory-optimized)\n",
    "Autoscaling: Enabled\n",
    "```\n",
    "\n",
    "#### Scenario C: High-Concurrency (Many Users/BI)\n",
    "\n",
    "**Characteristics:**\n",
    "- Multiple simultaneous users\n",
    "- BI tool connections\n",
    "- Mixed workload types\n",
    "- Need for fair resource distribution\n",
    "\n",
    "**Recommendation:**\n",
    "- **Shared Access Mode cluster** with autoscaling\n",
    "- Larger cluster size to handle concurrent queries\n",
    "- Consider SQL Warehouse for pure SQL workloads\n",
    "- Instance type: `Standard_DS4_v2` or larger\n",
    "- Enable fair scheduling\n",
    "\n",
    "**Configuration Example:**\n",
    "```\n",
    "Cluster Mode: Standard\n",
    "Access Mode: Shared\n",
    "Min Workers: 8\n",
    "Max Workers: 50\n",
    "Instance Type: Standard_DS4_v2\n",
    "Autoscaling: Enabled\n",
    "```\n",
    "\n",
    "#### Scenario D: Real-Time Streaming\n",
    "\n",
    "**Characteristics:**\n",
    "- Continuous data processing\n",
    "- Low latency requirements\n",
    "- Always-on workloads\n",
    "\n",
    "**Recommendation:**\n",
    "- **Always-on cluster** (don't auto-terminate)\n",
    "- Stable cluster size (limited autoscaling)\n",
    "- Consider Structured Streaming optimizations\n",
    "- Instance type: Balanced (compute-optimized)\n",
    "\n",
    "### 7.3 Cluster Configuration Best Practices\n",
    "\n",
    "**Driver Node:**\n",
    "- Usually same instance type as workers\n",
    "- For large clusters, consider larger driver node\n",
    "- Driver needs memory for:\n",
    "  - Spark application state\n",
    "  - Broadcast variables\n",
    "  - Result collection\n",
    "\n",
    "**Worker Nodes:**\n",
    "- Choose based on workload:\n",
    "  - **Compute-optimized**: CPU-intensive workloads\n",
    "  - **Memory-optimized**: Large joins, caching\n",
    "  - **Storage-optimized**: I/O-intensive workloads\n",
    "  - **General-purpose**: Balanced workloads\n",
    "\n",
    "**Spark Configuration:**\n",
    "- `spark.sql.shuffle.partitions`: Default 200, adjust based on data size\n",
    "- `spark.executor.memory`: Allocate based on instance type\n",
    "- `spark.executor.cores`: Match to instance vCPUs\n",
    "- Enable Photon and AQE for better performance\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Cluster Pools\n",
    "\n",
    "### What are Cluster Pools?\n",
    "\n",
    "**Cluster Pools** (also called Instance Pools) are a collection of idle, ready-to-use cloud instances that Databricks maintains to reduce cluster start times.\n",
    "\n",
    "### How Cluster Pools Work\n",
    "\n",
    "1. **Pre-provisioned Instances**: Databricks keeps a pool of instances running\n",
    "2. **Fast Cluster Start**: When you create a cluster from a pool, it starts in **~1-2 minutes** instead of 3-5 minutes\n",
    "3. **Cost Efficiency**: Idle instances in pools are cheaper than running full clusters\n",
    "4. **Automatic Management**: Databricks manages the pool size automatically\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Faster Startup**: Reduced cluster start time\n",
    "- **Cost Savings**: Idle pool instances cost less than idle clusters\n",
    "- **Better Resource Utilization**: Instances are reused across clusters\n",
    "- **Reduced Cold Starts**: Instances are \"warm\" and ready\n",
    "\n",
    "### When to Use Cluster Pools\n",
    "\n",
    "- **Frequent cluster creation/termination**: Jobs that start/stop often\n",
    "- **Interactive development**: When you need quick cluster starts\n",
    "- **Cost optimization**: When you have predictable workload patterns\n",
    "- **Team environments**: Shared pools for multiple users\n",
    "\n",
    "### Configuration\n",
    "\n",
    "- **Min Idle Instances**: Minimum instances to keep warm\n",
    "- **Max Capacity**: Maximum instances in the pool\n",
    "- **Instance Type**: Choose based on your typical workload\n",
    "- **Preloaded Spark Version**: Pre-install common Spark versions\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Autoscaling\n",
    "\n",
    "### What is Autoscaling?\n",
    "\n",
    "**Autoscaling** automatically adds or removes worker nodes from a cluster based on workload demand.\n",
    "\n",
    "### How Autoscaling Works\n",
    "\n",
    "1. **Monitor Workload**: Databricks monitors cluster utilization\n",
    "2. **Scale Up**: Adds workers when:\n",
    "   - Tasks are queued\n",
    "   - High CPU/memory utilization\n",
    "   - Long-running tasks\n",
    "3. **Scale Down**: Removes workers when:\n",
    "   - Low utilization\n",
    "   - Idle workers\n",
    "   - Tasks complete\n",
    "\n",
    "### Autoscaling Configuration\n",
    "\n",
    "**Min Workers:** Minimum number of worker nodes (always running)\n",
    "\n",
    "**Max Workers:** Maximum number of worker nodes (scaling limit)\n",
    "\n",
    "**Scaling Behavior:**\n",
    "- **Conservative**: Scales slowly, good for stable workloads\n",
    "- **Standard**: Balanced scaling (default)\n",
    "- **Aggressive**: Scales quickly, good for variable workloads\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Cost Optimization**: Pay only for resources you use\n",
    "- **Performance**: Automatically handles workload spikes\n",
    "- **Flexibility**: Adapts to changing data volumes\n",
    "- **Efficiency**: Removes idle workers automatically\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Set **min workers** based on baseline workload\n",
    "- Set **max workers** based on peak workload and budget\n",
    "- Use autoscaling for variable workloads\n",
    "- Monitor scaling behavior and adjust if needed\n",
    "- Consider cluster pools for faster scaling\n",
    "\n",
    "### When Not to Use Autoscaling\n",
    "\n",
    "- **Fixed workloads**: If workload is always the same size\n",
    "- **Very short jobs**: Scaling overhead may not be worth it\n",
    "- **Strict SLAs**: Fixed size provides predictable performance\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Instance Types and Selection\n",
    "\n",
    "### Understanding Instance Types\n",
    "\n",
    "Instance types determine the CPU, memory, and storage characteristics of your cluster nodes.\n",
    "\n",
    "### Instance Type Categories\n",
    "\n",
    "#### General Purpose (D-series)\n",
    "- **Use Case**: Balanced workloads, general data processing\n",
    "- **Examples**: `Standard_DS3_v2`, `Standard_DS4_v2`\n",
    "- **Characteristics**: Balanced CPU, memory, and storage\n",
    "\n",
    "#### Compute Optimized (F-series)\n",
    "- **Use Case**: CPU-intensive workloads, high-performance computing\n",
    "- **Examples**: `Standard_F4s_v2`, `Standard_F8s_v2`\n",
    "- **Characteristics**: High CPU-to-memory ratio\n",
    "\n",
    "#### Memory Optimized (L-series, r-series)\n",
    "- **Use Case**: Large joins, caching, in-memory processing\n",
    "- **Examples**: `Standard_L8s_v2`, `Standard_L16s_v2`\n",
    "- **Characteristics**: High memory-to-CPU ratio\n",
    "- **Best For**: Spark operations that require large memory (joins, aggregations, caching)\n",
    "\n",
    "#### Storage Optimized (Ls-series)\n",
    "- **Use Case**: I/O-intensive workloads, large local storage needs\n",
    "- **Examples**: `Standard_L8s_v2`, `Standard_L16s_v2`\n",
    "- **Characteristics**: High local SSD storage, high I/O performance\n",
    "\n",
    "### Selecting the Right Instance Type\n",
    "\n",
    "**Consider:**\n",
    "1. **Workload Type**:\n",
    "   - CPU-bound → Compute optimized\n",
    "   - Memory-bound → Memory optimized\n",
    "   - I/O-bound → Storage optimized\n",
    "   - Mixed → General purpose\n",
    "\n",
    "2. **Data Size**:\n",
    "   - Small data → Smaller instances\n",
    "   - Large data → Larger instances or more nodes\n",
    "\n",
    "3. **Cost**:\n",
    "   - Balance performance needs with budget\n",
    "   - Consider spot instances for cost savings\n",
    "\n",
    "4. **Spark Operations**:\n",
    "   - Joins and aggregations → Memory optimized\n",
    "   - Transformations → General purpose\n",
    "   - Shuffling → Storage optimized\n",
    "\n",
    "### Instance Type Examples\n",
    "\n",
    "| Instance Type | vCPUs | Memory | Local Storage | Best For |\n",
    "|--------------|-------|--------|---------------|----------|\n",
    "| Standard_DS3_v2 | 4 | 14 GB | 28 GB | Small workloads, development |\n",
    "| Standard_DS4_v2 | 8 | 28 GB | 56 GB | Medium workloads, general purpose |\n",
    "| Standard_L8s_v2 | 8 | 64 GB | 128 GB | Memory-intensive, large joins |\n",
    "| Standard_L16s_v2 | 16 | 128 GB | 256 GB | Very memory-intensive workloads |\n",
    "| Standard_F4s_v2 | 4 | 8 GB | 16 GB | CPU-intensive computations |\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Cluster Lifecycle Management\n",
    "\n",
    "### Cluster States\n",
    "\n",
    "1. **Pending**: Cluster is being created\n",
    "2. **Running**: Cluster is active and ready\n",
    "3. **Restarting**: Cluster is restarting (after failure or manual restart)\n",
    "4. **Terminated**: Cluster has been stopped\n",
    "5. **Error**: Cluster failed to start or encountered an error\n",
    "\n",
    "### Termination Policies\n",
    "\n",
    "**Auto-Termination:**\n",
    "- Automatically terminates cluster after specified idle time\n",
    "- Default: 120 minutes of inactivity\n",
    "- Configurable per cluster\n",
    "- **Best Practice**: Enable for all-purpose clusters to save costs\n",
    "\n",
    "**Manual Termination:**\n",
    "- User manually stops the cluster\n",
    "- Immediate termination\n",
    "- Data in local storage is lost\n",
    "\n",
    "**Job Completion:**\n",
    "- Job clusters terminate automatically after job completes\n",
    "- No manual intervention needed\n",
    "\n",
    "### Cluster Restart\n",
    "\n",
    "**When to Restart:**\n",
    "- After configuration changes\n",
    "- After library installations\n",
    "- To clear cached data\n",
    "- To resolve performance issues\n",
    "\n",
    "**Restart Types:**\n",
    "- **Full Restart**: Restarts all nodes (clears all state)\n",
    "- **Driver Restart**: Restarts only driver node (faster)\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Enable auto-termination** for all-purpose clusters\n",
    "- **Use job clusters** for scheduled workloads\n",
    "- **Monitor cluster health** regularly\n",
    "- **Restart clusters** after significant changes\n",
    "- **Terminate unused clusters** to save costs\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Cost Optimization Strategies\n",
    "\n",
    "### Understanding Databricks Costs\n",
    "\n",
    "Costs come from:\n",
    "1. **Compute (DBUs)**: Databricks Unit consumption\n",
    "2. **Cloud Infrastructure**: VM instance costs\n",
    "3. **Storage**: Cloud storage costs (separate from Databricks)\n",
    "\n",
    "### Cost Optimization Techniques\n",
    "\n",
    "#### 1. Right-Size Clusters\n",
    "- **Don't over-provision**: Use appropriately sized instances\n",
    "- **Monitor utilization**: Adjust based on actual usage\n",
    "- **Use autoscaling**: Scale down when not needed\n",
    "\n",
    "#### 2. Use Job Clusters\n",
    "- **For scheduled workloads**: Job clusters are more cost-effective\n",
    "- **Automatic termination**: No idle time costs\n",
    "- **Optimized for batch**: Better resource utilization\n",
    "\n",
    "#### 3. Enable Auto-Termination\n",
    "- **All-purpose clusters**: Set reasonable idle timeout\n",
    "- **Default 120 minutes**: Adjust based on usage patterns\n",
    "- **Saves costs**: No charges for idle clusters\n",
    "\n",
    "#### 4. Use Cluster Pools\n",
    "- **Faster starts**: Reduces wasted time\n",
    "- **Lower idle costs**: Pool instances cost less than idle clusters\n",
    "- **Better utilization**: Shared resources\n",
    "\n",
    "#### 5. Choose Appropriate Instance Types\n",
    "- **Match workload**: Don't use memory-optimized for CPU-bound tasks\n",
    "- **Consider spot instances**: For fault-tolerant workloads (if available)\n",
    "- **Right size**: Avoid unnecessarily large instances\n",
    "\n",
    "#### 6. Optimize Spark Configuration\n",
    "- **Enable Photon**: Faster execution = lower costs\n",
    "- **Enable AQE**: Better resource utilization\n",
    "- **Tune partitions**: Reduce shuffle overhead\n",
    "- **Cache strategically**: Only cache frequently used data\n",
    "\n",
    "#### 7. Use Serverless (When Available)\n",
    "- **Instant start**: No waiting time costs\n",
    "- **Automatic optimization**: Better resource utilization\n",
    "- **Pay per use**: Only charged for actual compute time\n",
    "\n",
    "#### 8. Monitor and Analyze\n",
    "- **Review cluster usage**: Identify underutilized clusters\n",
    "- **Analyze costs**: Use Databricks cost analysis tools\n",
    "- **Set budgets**: Configure spending limits\n",
    "- **Optimize regularly**: Review and adjust monthly\n",
    "\n",
    "### Cost Monitoring\n",
    "\n",
    "- **Databricks Cost Analysis**: Built-in cost tracking\n",
    "- **Cloud Provider Billing**: Monitor VM costs\n",
    "- **Usage Reports**: Track DBU consumption\n",
    "- **Budget Alerts**: Set up spending notifications\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Monitoring and Performance\n",
    "\n",
    "### Cluster Monitoring\n",
    "\n",
    "**Key Metrics to Monitor:**\n",
    "\n",
    "1. **CPU Utilization**\n",
    "   - High CPU → May need more workers or larger instances\n",
    "   - Low CPU → May be over-provisioned\n",
    "\n",
    "2. **Memory Utilization**\n",
    "   - High memory → Consider memory-optimized instances\n",
    "   - Memory pressure → May cause spills to disk\n",
    "\n",
    "3. **Network I/O**\n",
    "   - Monitor data transfer rates\n",
    "   - High network usage → Consider data locality optimizations\n",
    "\n",
    "4. **Storage I/O**\n",
    "   - Disk read/write rates\n",
    "   - High I/O → May need storage-optimized instances\n",
    "\n",
    "5. **Task Execution**\n",
    "   - Task duration\n",
    "   - Failed tasks\n",
    "   - Queued tasks (indicates need for scaling)\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**Spark UI:**\n",
    "- Access via cluster details page\n",
    "- View job execution plans\n",
    "- Identify bottlenecks\n",
    "- Analyze task distribution\n",
    "\n",
    "**Query Optimization:**\n",
    "- Use EXPLAIN to view query plans\n",
    "- Identify expensive operations\n",
    "- Optimize joins and aggregations\n",
    "- Use appropriate partitioning\n",
    "\n",
    "**Data Locality:**\n",
    "- Co-locate data and compute when possible\n",
    "- Use Delta Lake optimizations (Z-ordering, compaction)\n",
    "- Minimize data shuffling\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Regular Monitoring**: Check cluster metrics weekly\n",
    "- **Performance Baselines**: Establish expected performance\n",
    "- **Optimize Incrementally**: Make small, measured changes\n",
    "- **Document Changes**: Track configuration changes and their impact\n",
    "- **Use Photon and AQE**: Enable for automatic optimizations\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Compute vs. Cluster**: Compute is the infrastructure, clusters are the configured Spark environments\n",
    "2. **Storage**: Clusters use ephemeral local storage; persistent data lives in cloud storage\n",
    "3. **Cluster Types**: Choose all-purpose for development, job clusters for automation, SQL warehouses for BI\n",
    "4. **Access Modes**: Single user for isolation, shared for collaboration\n",
    "5. **Performance**: Enable Photon and AQE for automatic optimizations\n",
    "6. **Sizing**: Right-size based on workload characteristics\n",
    "7. **Cost**: Use autoscaling, auto-termination, and job clusters to optimize costs\n",
    "8. **Monitoring**: Regularly monitor metrics and optimize based on actual usage\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Use Case | Cluster Type | Access Mode | Instance Type | Autoscaling |\n",
    "|----------|--------------|-------------|---------------|-------------|\n",
    "| Development | All-Purpose | Shared | General Purpose | Optional |\n",
    "| Scheduled ETL | Job Cluster | N/A | Memory-Optimized | Recommended |\n",
    "| BI/Reporting | SQL Warehouse | N/A | Auto-selected | Auto |\n",
    "| Large Joins | All-Purpose | Single User | Memory-Optimized | Recommended |\n",
    "| High Concurrency | All-Purpose | Shared | General Purpose | Required |\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Cluster Configuration Guide](https://docs.databricks.com/clusters/)\n",
    "- [Photon Engine Documentation](https://docs.databricks.com/runtime/photon.html)\n",
    "- [Cost Optimization Best Practices](https://docs.databricks.com/administration-guide/account-settings/billing.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_databricks_compute_and_clusters",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}