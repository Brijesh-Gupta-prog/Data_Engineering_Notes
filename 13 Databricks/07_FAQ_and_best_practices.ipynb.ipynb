{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc37c65a-2abd-43b4-9548-d1fb7a7df735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68631d9-1912-41f8-baf5-04f46faa271c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì Is the driver node ‚Äúcompute‚Äù?\n",
    "\n",
    "Yes. Period.\n",
    "\n",
    "The driver:\n",
    "\n",
    "- Runs your SparkSession\n",
    "- Builds the DAG\n",
    "- Plans stages & tasks\n",
    "- Tracks metadata for every partition\n",
    "- Collects results for collect(), toPandas(), count(), etc.\n",
    "- Hosts the Spark UI\n",
    "- Manages job scheduling & heartbeats\n",
    "\n",
    "That is real CPU + real RAM usage. If your driver dies, the job dies.\n",
    "\n",
    "![](/Workspace/Users/roityadav@gmail.com/databricks-training/_src_img/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0567ab4c-b16f-4557-995d-7fea0ac88a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Does the driver have storage?\n",
    "\n",
    "Yes ‚Äî but don‚Äôt overestimate it.\n",
    "\n",
    "Driver storage includes:\n",
    "\n",
    "- Local disk (ephemeral)\n",
    "- Shuffle metadat\n",
    "- Temporary spill files\n",
    "- Logs\n",
    "\n",
    "It is **NOT** for data processing at scale.\n",
    "Anything ‚Äúlarge‚Äù that lands on the driver is a design mistake.\n",
    "\n",
    "If you do:\n",
    "\n",
    "`df.collect()`\n",
    "\n",
    "You are explicitly asking the driver to hold everything in memory. That‚Äôs on you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e38ac4-f814-4d4a-958b-f608cd211263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì Why would df.collect() break the application?\n",
    "\n",
    "When you call `.collect()`, you are triggering a massive migration of data from a distributed environment into a single, centralized process. Here is the step-by-step breakdown of what happens behind the scenes:\n",
    "\n",
    "- The Triggering of an Action\n",
    "Spark uses lazy evaluation. Up until you call `.collect()`, Spark has only been building a logical plan (a recipe). `.collect()` is an action, which tells Spark: \"Stop planning, execute the transformations, and give me the final result.\"\n",
    "\n",
    "- The Multi-Stage Transfer\n",
    "Once the executors finish processing their individual partitions of data, the transfer begins:\n",
    "\n",
    "- - Serialization: Each executor takes the data sitting in its memory and converts it into a format that can be sent over the network (ByteBuffers).\n",
    "- - Network Transfer: Every executor simultaneously opens a connection to the Driver.\n",
    "- - The \"Bottleneck\" Gathering: All those bytes are sent over the network to the Driver's IP address.\n",
    "- - Deserialization & Reassembly: The Driver receives these chunks and must turn them back into objects (like a Python list or a Java array).\n",
    "\n",
    "### Does every operation send data to the driver?\n",
    "No. This is the most important distinction in Spark. Only Actions that return data to the client/driver move data this way\n",
    "\n",
    "| Operation Type       | Examples                          | Where the data stays                                      |\n",
    "|----------------------|-----------------------------------|-----------------------------------------------------------|\n",
    "| Transformations      | filter, select, join              | Distributed across Executors.                             |\n",
    "| Distributed Actions  | saveAsTextFile, write.parquet     | Distributed (moves from Executors to Storage/S3/HDFS).    |\n",
    "| Driver Actions       | collect, take(n), show()          | Moves to the Driver.                                      |\n",
    "\n",
    "\n",
    "### Why .collect() is risky\n",
    "\n",
    "When you run a distributed job, you might have 100 executors with 16GB of RAM each (1.6TB total). If your driver only has 8GB of RAM and you call .collect() on a dataset that is 20GB:\n",
    "\n",
    "- The executors will successfully process the data.\n",
    "- The network will become saturated as 20GB tries to move at once.\n",
    "- The Driver will attempt to load all 20GB into its 8GB heap.\n",
    "- Result: java.lang.OutOfMemoryError: Java heap space. The driver crashes, and the entire Spark session dies.\n",
    "\n",
    "### When should you actually use it?\n",
    "You generally only need .collect() in two specific scenarios:\n",
    "\n",
    "- Unit Testing: Running a small sample of data to verify logic.\n",
    "- Final Results: After you have aggregated, filtered, and reduced your data down to something tiny (e.g., a summary table or a single count) that can easily fit in a standard laptop's memory.\n",
    "- Rule of Thumb: If you find yourself needing to \"loop\" through data in Python or Scala after a Spark job, try to rewrite that loop using Spark SQL functions instead. Keep the data distributed as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54e398f-6a72-43eb-80d4-8f3fdf6d4fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì What is the practice around df.collect() in production?\n",
    "\n",
    "In production environments, the goal is to keep data distributed for as long as possible. Moving data to the driver is essentially \"exiting\" the Spark engine and losing all the benefits of parallel computing\n",
    "\n",
    "### Production Alternatives to .collect()\n",
    "If you need to see or use your data, choose the \"least invasive\" method based on your goal:\n",
    "| Technique                  | Operation/Example                          | Benefit/Behavior                                                                 |\n",
    "|----------------------------|--------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| Inspection                 | df.show(n)                                 | Only pulls n rows (default 20) to the driver, not the whole set.                 |\n",
    "| Sampling                   | df.take(n) or df.limit(n).collect()        | Fetches a specific small subset rather than the entire partition.                |\n",
    "| Processing                 | df.foreach() or df.mapPartitions()         | Keeps the logic on the Executors. The driver just coordinates.                   |\n",
    "| Storage                    | df.write.parquet(\"path\")                   | Data moves directly from Executors to S3/HDFS/Blob Storage.                      |\n",
    "| Iterating                  | df.toLocalIterator()                       | Pulls one partition at a time to the driver instead of all at once.              |\n",
    "\n",
    "The `toLocalIterator() `Trick\n",
    "If you absolutely must loop through a large dataset on the driver (e.g., for a specific sequential API call), use df.toLocalIterator(). Instead of flooding the driver with everything at once, it consumes one partition, processes it, and then moves to the next, significantly reducing the risk of an Out of Memory (OOM) error.\n",
    "\n",
    "### How it works in Production Environments\n",
    "\n",
    "In a production cluster (like Databricks, EMR, or Google Cloud Dataproc), the setup usually involves a Cluster Manager (YARN, Kubernetes, or Standalone).\n",
    "\n",
    "The **\"Driver Wall\"**\n",
    "In production, your Driver is often a much smaller machine than your total Executor pool.\n",
    "\n",
    "- Executors: Might have 512GB of total RAM combined.\n",
    "- Driver: Might only have 8GB or 16GB.\n",
    "- The Risk: If you have a 100GB dataset, the executors handle it easily. If you call .collect(), you are trying to shove 100GB into a 16GB pipe.\n",
    "\n",
    "**Common Production Guardrails**\n",
    "To prevent one developer from crashing a shared production cluster, admins often set these Spark configurations:\n",
    "\n",
    "- `spark.driver.maxResultSize`: Limits the total size of results returned to the driver. If you try to .collect() more than this (e.g., 2GB), the job will fail safely rather than crashing the driver.\n",
    "- `spark.driver.memory`: Explicitly defines how much room the driver has.\n",
    "\n",
    "## Best Practices\n",
    "- Filter and Aggregate Early: Always use .filter() and .groupBy() to shrink your data while it is still on the executors. Only collect the \"answer,\" never the \"source data.\"\n",
    "- Use External Storage: Instead of collecting data to your driver to write a CSV, use the df.write API. This allows the executors to write their own chunks of data in parallel directly to your cloud bucket or database.\n",
    "- Use Broadcast Variables for Small Data: If you need data from one DataFrame to be available to all executors (like a small lookup table), don't collect it and then join it. Use broadcast(small_df), which sends the data from the driver out to the executors efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f8c7b22-89d5-4d16-87f1-6a7832e7f61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚ùì Executors vs Driver: They are not symmetrical\n",
    "\n",
    "Executors:\n",
    "\n",
    "- Execute tasks\n",
    "- Use cores heavily\n",
    "- Consume RAM proportional to partitions\n",
    "- Scale horizontally\n",
    "\n",
    "Driver:\n",
    "\n",
    "- Mostly single-threaded planning\n",
    "- Needs memory, not many cores\n",
    "- Becomes a choke point for:\n",
    "- - Too many partitions\n",
    "- - Wide schemas\n",
    "- - Massive task counts\n",
    "- - `collect, groupByKey, toPandas`\n",
    "\n",
    "This is why ‚Äúmore executors‚Äù does not save a weak driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4ca9ff4-345e-4881-a9d3-75dbf396b4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì If I have 4 executors with 4 cores each, does this mean that my driver will also have same memory and cores?\n",
    "\n",
    "Let‚Äôs say:\n",
    "\n",
    "- 4 executors\n",
    "- 4 cores each\n",
    "- Total executor cores = 16\n",
    "\n",
    "**Wrong assumption**:\n",
    "Driver will have 4 cores because executors have 4 cores\n",
    "\n",
    "No. That‚Äôs not how it works.\n",
    "\n",
    "### Reality\n",
    "\n",
    "Driver configuration is separate.\n",
    "\n",
    "Typical Databricks default might look like:\n",
    "\n",
    "- Driver: 1‚Äì2 cores, 4‚Äì8 GB RAM\n",
    "- Executors: 4 cores each, larger RAM\n",
    "\n",
    "That can be terrible for:\n",
    "\n",
    "- Large joins\n",
    "- Many partitions (10k+)\n",
    "- Large shuffles\n",
    "- Heavy metadata operations\n",
    "\n",
    "### When your driver becomes the silent killer\n",
    "\n",
    "You‚Äôll see:\n",
    "\n",
    "- Job stuck at ‚ÄúSubmitting tasks‚Äù\n",
    "- Spark UI slow or unresponsive\n",
    "- Random Driver OOM\n",
    "- Executor CPUs idle while job ‚Äúruns‚Äù\n",
    "\n",
    "And you may wrongly blame:\n",
    "\n",
    "- Partitioning\n",
    "- Shuffle\n",
    "- Network\n",
    "- Databricks bugs\n",
    "\n",
    "But in actual, your driver is underpowered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2935e392-12f0-4f49-9dbe-70288a78b954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Executors fail ‚Üí Spark retries\n",
    "\n",
    "Driver fails ‚Üí job is dead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8177e4b1-4765-460d-a5e1-f436f2907805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì Going beyond ‚Äú4 executors √ó 4 cores‚Äù thinking pattern\n",
    "\n",
    "In Databricks terms, the correct translation is:\n",
    "\n",
    "> ‚ÄúI have 4 worker nodes, each with 4 vCPUs‚Äù\n",
    "\n",
    "Not:\n",
    "\n",
    "> ‚ÄúI configured 4 executors manually‚Äù\n",
    "\n",
    "Executors are an implementation detail Databricks abstracts unless you force Spark configs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05194863-c92b-41e0-b6c4-c95582f9678f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì Databricks UI terminology:\n",
    "\n",
    "- Driver node type\n",
    "- Worker node type\n",
    "- Autoscaling (min workers / max workers)\n",
    "- DBU (Databricks Units) ‚Äì billing abstraction\n",
    "- Runtime version (e.g., 13.3 LTS)\n",
    "- Photon enabled (changes executor behavior)\n",
    "- Single Node cluster (driver = worker)\n",
    "\n",
    "Not:\n",
    "\n",
    "- ‚ÄúHow many executors should I take?‚Äù ‚ùå\n",
    "- ‚ÄúHow many cores does Spark give me?‚Äù ‚ùå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45af3c7f-dab0-483c-9f41-1c0e011337ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/roityadav@gmail.com/databricks-training/_src_img/cluster-config.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07adb9cb-6903-4cff-968e-21c23f7fd929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùìWhat is `r6id.large` in the image in above cell?\n",
    "\n",
    "r6id.xlarge is an AWS EC2 instance type, not a Spark concept, not a Databricks abstraction.\n",
    "\n",
    "- r ‚Üí Memory-optimized family\n",
    "- 6 ‚Üí 6th generation (Graviton? No‚Äîthis one is Intel-based)\n",
    "- i ‚Üí Intel CPU\n",
    "- d ‚Üí Local NVMe SSD attached\n",
    "- xlarge ‚Üí size class\n",
    "\n",
    "Concrete specs\n",
    "\n",
    "- 4 vCPUs\n",
    "- 32 GB RAM\n",
    "- Local NVMe disk (fast scratch storage)\n",
    "- Good for memory-heavy Spark workloads and shuffle-intensive jobs\n",
    "\n",
    "Databricks is simply letting you pick which EC2 box your executor runs on.\n",
    "\n",
    "### Common Databricks instance options (AWS)\n",
    " \n",
    "\n",
    "| Category              | Instance Types                     | Characteristics                  | Use When                                                                 | Avoid When / Notes                                                      |\n",
    "|-----------------------|------------------------------------|----------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| General Purpose       | m6i.large, m6i.xlarge              | Balanced CPU / RAM               | Light ETL, moderate data sizes, unclear workload                          | Not optimal for big Spark shuffles                                      |\n",
    "| Memory Optimized      | r6i.xlarge, r6id.xlarge (NVMe)     | High RAM per core                | Large joins, wide transformations, caching DataFrames, OOM risk           | Costs more than general purpose                                         |\n",
    "| Compute Optimized     | c6i.xlarge                         | High CPU, low RAM                | Heavy UDFs, CPU-bound transforms, ML feature engineering (not training)   | Shuffle-heavy workloads, caching                                        |\n",
    "| Storage Optimized     | i3.xlarge, i4i.xlarge              | Massive local NVMe               | Massive shuffle spill, sort-heavy workloads, temp-heavy pipelines         | Overkill for most cases, expensive if misused                            |\n",
    "| ARM / Graviton        | r6g, m6g, c6g                      | Cheaper, often faster            | Cost-optimized workloads with compatible libraries                        | Native libs, Python wheels, JVM compatibility can break workloads        |\n",
    "\n",
    "\n",
    "### Other common Databricks instance options (Azure)\n",
    "\n",
    "| Category              | Azure VM Series / Examples                 | Characteristics                               | Use When                                                                 | Avoid When / Notes                                                                 |\n",
    "|-----------------------|--------------------------------------------|-----------------------------------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------------------|\n",
    "| General Purpose       | Dsv5, Dasv5 (e.g., D4s_v5, D8as_v5)        | Balanced CPU / RAM                            | Light ETL, moderate data, default Spark workloads                         | Weak for heavy shuffles; memory is the first bottleneck                             |\n",
    "| Memory Optimized      | Esv5, Easv5 (e.g., E8s_v5, E16as_v5)       | High RAM per core                             | Large joins, wide transformations, caching, OOM issues                    | Expensive; wasteful if your job isn‚Äôt memory-bound                                  |\n",
    "| Compute Optimized     | Fsv2 (e.g., F8s_v2, F16s_v2)               | High CPU, low RAM                             | CPU-heavy UDFs, transformations, feature engineering                      | Shuffle-heavy jobs, joins, caching‚Äîwill fall apart fast                              |\n",
    "| Storage Optimized     | Lsv3 (e.g., L8s_v3, L16s_v3)               | Massive local NVMe                            | Heavy shuffle spill, sort-intensive pipelines, temp-heavy Spark workloads | Overkill for most jobs; expensive and often misused                                  |\n",
    "| AMD / Cost Optimized  | Dasv5, Easv5                              | Cheaper than Intel, good performance          | Cost-sensitive workloads with standard Spark/Python stacks                | Slightly weaker single-core performance vs Intel                                     |\n",
    "| ARM / Ampere          | Dpsv5, Epsv5                              | ARM-based, cheaper, energy efficient          | Controlled environments with verified ARM compatibility                   | Python wheels, native libs, JVM edge cases‚Äîeasy way to break pipelines               |\n",
    "\n",
    "\n",
    "### How could we decide which one to use?\n",
    "\n",
    "| Workload Type                     | Driver VM (Azure) | Worker VM (Azure) | Why This Works                                                             |\n",
    "|----------------------------------|-------------------|-------------------|----------------------------------------------------------------------------|\n",
    "| Unknown / Mixed workload         | D8s_v5            | D8s_v5            | Balanced; safe starting point                                              |\n",
    "| Join-heavy / Wide transformations| E8s_v5            | E8s_v5 or E16s_v5 | Driver needs RAM for query plans & broadcasts; workers need RAM for shuffle |\n",
    "| Large broadcast joins            | E16s_v5           | D8s_v5 or E8s_v5  | Driver holds broadcast; workers don‚Äôt need excessive RAM                   |\n",
    "| CPU-heavy UDFs / parsing         | D8s_v5            | F8s_v2            | Driver coordination only; workers burn CPU                                 |\n",
    "| Shuffle spill / sort-heavy       | E8s_v5            | L8s_v3            | Driver tracks shuffle; workers need NVMe                                   |\n",
    "| Cost-optimized production ETL    | D8s_v5            | Dasv5             | Driver stability; cheaper workers                                          |\n",
    "| ARM (only if verified)           | Dpsv5             | Dpsv5             | Homogeneous ARM avoids JVM & wheel mismatch                                 |\n",
    "\n",
    "\n",
    "**Stop guessing**\n",
    "\n",
    "- Check Spark UI ‚Üí Executors ‚Üí Memory / Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "683d552b-31e2-4019-aefd-b1e228ac0d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b945fb62-13b5-4abd-9433-d79152af7ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùìHow SQL Warehouse Is Used for BI Workloads (Databricks + Power BI)\n",
    "## \n",
    "\n",
    "Assume the following data flow:\n",
    "\n",
    "1. Bronze data stored in ADLS  \n",
    "2. Transformed into Silver and written back to ADLS in Delta format  \n",
    "3. Further transformed into Gold and written back to ADLS in Delta format  \n",
    "\n",
    "The question:\n",
    "\n",
    "> How does Power BI use Databricks SQL Warehouse to query Gold data stored in ADLS and perform analytics?\n",
    "\n",
    "---\n",
    "\n",
    "## Role of SQL Warehouse\n",
    "\n",
    "The SQL Warehouse acts as a **query compute layer** between:\n",
    "\n",
    "- **Storage** ‚Üí ADLS (Delta tables)\n",
    "- **Consumption** ‚Üí Power BI\n",
    "\n",
    "Key facts:\n",
    "\n",
    "- SQL Warehouse does **not store data**\n",
    "- It reads Delta files **on demand**\n",
    "- It executes SQL queries and streams results to BI tools\n",
    "\n",
    "---\n",
    "\n",
    "## Connectivity Flow\n",
    "\n",
    "### 1. Registering Gold Data (Metadata Layer)\n",
    "\n",
    "Even though Gold data physically resides in ADLS as Delta files, SQL tools require metadata.\n",
    "\n",
    "Register the table using Unity Catalog (or Hive Metastore):\n",
    "\n",
    "```sql\n",
    "CREATE TABLE catalog.schema.gold_table\n",
    "USING DELTA\n",
    "LOCATION 'abfss://container@storage.dfs.core.windows.net/gold_path';\n",
    "```\n",
    "\n",
    "## The SQL Warehouse Can Now See the Table\n",
    "\n",
    "Once the Gold table is registered in Unity Catalog or the Hive Metastore,  \n",
    "the SQL Warehouse can discover and query it.\n",
    "\n",
    "---\n",
    "\n",
    "## Power BI Connection\n",
    "\n",
    "Power BI connects to Databricks SQL Warehouse as follows:\n",
    "\n",
    "- Connect using the **Server Hostname** and **HTTP Path** from SQL Warehouse settings\n",
    "- **Authentication:** Typically Microsoft Entra ID (SSO)\n",
    "- Power BI sends **SQL queries** to the SQL Warehouse, not directly to ADLS\n",
    "\n",
    "---\n",
    "\n",
    "## Import Mode vs. DirectQuery Mode\n",
    "\n",
    "| Feature | Import Mode | DirectQuery Mode |\n",
    "|------|------------|------------------|\n",
    "| Data Location | Loaded into Power BI memory (RAM) | Stays in ADLS; queried on-the-fly |\n",
    "| Performance | Extremely fast (pre-loaded) | Depends on SQL Warehouse speed |\n",
    "| Data Freshness | Only as fresh as last refresh | Near real-time |\n",
    "| SQL Warehouse Usage | Runs only during refresh | Runs on every user interaction (filters, slicers, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use SQL Warehouse Instead of a Standard Spark Cluster?\n",
    "\n",
    "| Benefit | Explanation |\n",
    "|------|------------|\n",
    "| Instant Compute (Serverless) | Starts in seconds (vs. 3‚Äì5 minutes for All-Purpose clusters), avoiding Power BI timeouts |\n",
    "| High Concurrency | Designed to scale horizontally for many simultaneous BI users |\n",
    "| Disk Caching | Proactively caches hot data on fast local SSDs for faster repeated queries |\n",
    "\n",
    "---\n",
    "\n",
    "## Production Best Practice: The Semantic Layer (Views)\n",
    "\n",
    "Instead of letting Power BI query raw Gold tables directly, create **views** in Databricks.\n",
    "\n",
    "### Example\n",
    "\n",
    "```sql\n",
    "CREATE VIEW catalog.schema.v_sales_report AS\n",
    "SELECT\n",
    "    region,\n",
    "    SUM(sales) AS total_sales\n",
    "FROM catalog.schema.gold_table\n",
    "GROUP BY region;\n",
    "```\n",
    "Power BI connects **only to the views**, not the underlying tables.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- Centralize business logic\n",
    "- Change calculations without republishing Power BI reports\n",
    "- Add security (e.g., column masking, restricted columns)\n",
    "\n",
    "---\n",
    "\n",
    "## Clarification: How Views Work with SQL Warehouse\n",
    "\n",
    "SQL Warehouse is **compute only** ‚Äî it has no persistent storage.\n",
    "\n",
    "---\n",
    "\n",
    "## The Three Layers\n",
    "\n",
    "| Layer | Component | Role |\n",
    "|------|----------|------|\n",
    "| Storage | ADLS | Holds actual Delta Parquet files and transaction logs |\n",
    "| Metadata | Unity Catalog / Hive Metastore | Stores table and view definitions (schema, location, view SQL text) |\n",
    "| Compute | SQL Warehouse | Executes queries using temporary RAM; forgets data when query ends |\n",
    "\n",
    "---\n",
    "\n",
    "## What Happens When You Create a View?\n",
    "\n",
    "1. You run `CREATE VIEW` using a SQL Warehouse\n",
    "2. SQL Warehouse forwards the command to Unity Catalog\n",
    "3. Unity Catalog stores the view definition as text (a saved query)\n",
    "4. No new files are created in ADLS\n",
    "\n",
    "---\n",
    "\n",
    "## Query Execution Flow\n",
    "\n",
    "### Power BI sends:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM v_sales_report;\n",
    "```\n",
    "\n",
    "### SQL Warehouse asks Unity Catalog\n",
    "\n",
    "> What is `v_sales_report`?\n",
    "\n",
    "---\n",
    "\n",
    "### Unity Catalog response\n",
    "\n",
    "- Returns the stored query definition\n",
    "\n",
    "---\n",
    "\n",
    "### SQL Warehouse execution\n",
    "\n",
    "- Resolves the view to the underlying Gold table  \n",
    "- Reads data from ADLS  \n",
    "- Processes data in temporary RAM  \n",
    "- Streams results to Power BI  \n",
    "\n",
    "---\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "- Data is dropped from memory after query completion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd369a4c-e0ac-44c4-9854-4609e9bb7d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4682e5-cdee-4b60-950b-3df3a272c439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùì Is Photon a Replacement for the Spark Engine?\n",
    "\n",
    "\"How does Databricks Photon relate to the Apache Spark architecture? Is it a standalone distributed compute engine designed to replace Spark, or does it function as a specialized component within the Spark ecosystem to optimize performance?\"\n",
    "\n",
    "### Technical Deep Dive: Photon vs. Spark\n",
    "\n",
    "#### The Short Answer\n",
    "**No.** Photon is not a replacement for Spark; it is an accelerant for Spark.\n",
    "\n",
    "#### The Correct Mental Model\n",
    "To understand Photon, you must realize that Spark is not a monolithic \"engine\"‚Äîit is a stack of layers. Photon replaces only the **execution layer** of that stack.\n",
    "\n",
    "| Layer        | Component                              | Does Photon Replace This? |\n",
    "|--------------|----------------------------------------|---------------------------|\n",
    "| API          | SparkSession, DataFrame API, Spark SQL | No                        |\n",
    "| Planning     | Catalyst Optimizer, Logical/Physical Plans | No                    |\n",
    "| Execution    | Task execution, Operator processing   | Yes (for SQL operators)   |\n",
    "| Coordination | Cluster management, Scheduling, Shuffling | No                    |\n",
    "\n",
    "#### What Spark Still Manages\n",
    "Even when Photon is enabled, Apache Spark remains the \"brain\" and the \"skeleton\" of the operation. Photon would be useless without Spark to handle:\n",
    "\n",
    "- **Query Planning**: The Catalyst optimizer still decides how to join tables.\n",
    "- **Distribution**: Spark still breaks data into partitions and manages tasks.\n",
    "- **Fault Tolerance**: If a node fails, Spark (not Photon) manages the retry logic.\n",
    "- **Cluster Coordination**: The Driver and Executors are still Spark-native components.\n",
    "\n",
    "#### What Photon Actually Replaces\n",
    "Photon replaces the JVM-based execution (Java/Scala bytecode) with a high-performance C++ implementation.\n",
    "\n",
    "- **The Old Way (Standard Spark)**: Uses Whole-Stage Code Generation to create Java bytecode, which runs on the JVM and is subject to Garbage Collection (GC) overhead.\n",
    "- **The Photon Way**: Uses Vectorized Execution written in C++. It leverages SIMD (Single Instruction, Multiple Data) to process multiple data points at once directly at the CPU level.\n",
    "\n",
    "**Key Difference**: Instead of moving Java objects around, Photon operates on a column-oriented, off-heap memory layout that is much friendlier to modern CPU caches.\n",
    "\n",
    "Photon is a high-performance turbo-charged engine. You don't get a \"new car\"; you just upgraded the engine under the hood. You still drive it using the same steering wheel (DataFrames/SQL).\n",
    "\n",
    "#### When Photon Does (and Doesn't) Help\n",
    "\n",
    "| Photon Shines In ‚ú®                             | Photon Has No Effect On üõë                     |\n",
    "|------------------------------------------------|------------------------------------------------|\n",
    "| Parquet/Delta Scans: Rapid data ingestion      | Python UDFs: Row-by-row Python logic           |\n",
    "| Aggregations & Joins: High-volume SQL ops      | RDD-based Logic: Legacy Spark code             |\n",
    "| SQL Warehouses: BI-style workloads             | Custom Scala/Java Code: Outside SQL APIs       |\n",
    "| Vectorized Operations: SIMD-friendly tasks     | Pandas UDFs (unless using specific optimizations) |\n",
    "\n",
    "#### Final Summary\n",
    "Photon is a native vectorized execution engine that accelerates Spark SQL workloads by replacing the JVM-based execution path. Spark continues to handle the high-level APIs, query planning, scheduling, and distributed resource management.\n",
    "\n",
    "![](/Workspace/Users/roityadav@gmail.com/databricks-training/_src_img/photon.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b25702df-09e1-4c66-a366-35e7c466de5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e443b7de-fecf-4cad-bbf6-cbdc5b094634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Adaptive Query Execution (AQE) in Apache Spark\n",
    "\n",
    "How does Adaptive Query Execution (AQE) work in Apache Spark, given that Spark traditionally generates a physical query plan before execution?\n",
    "\n",
    "Is AQE part of open-source Apache Spark, or is it a Databricks-specific enhancement?\n",
    "\n",
    "Specifically, how does query planning and execution differ between open-source Spark without AQE and Spark with AQE enabled?\n",
    "\n",
    "## What is Adaptive Query Execution (AQE)?\n",
    "\n",
    "Adaptive Query Execution (AQE) allows Spark to **postpone certain optimization decisions** until runtime, when it can observe **actual data statistics** instead of relying on potentially stale or inaccurate estimates.\n",
    "\n",
    "### Traditional (Static) Spark Planning\n",
    "- Builds **one static physical plan** before execution begins\n",
    "- Commits to that plan entirely\n",
    "- If assumptions were wrong ‚Üí performance suffers (no recovery)\n",
    "\n",
    "### AQE Approach\n",
    "- Builds an **initial physical plan**\n",
    "- Starts execution\n",
    "- Collects **real runtime statistics** (partition sizes, row counts, skew patterns)\n",
    "- **Re-optimizes downstream stages** while the query is running\n",
    "\n",
    "This is Spark acknowledging:  \n",
    "> \"My optimizer is essentially blind at compile time.\"\n",
    "\n",
    "## Is AQE Part of Open-Source Apache Spark?\n",
    "\n",
    "**Yes** ‚Äî AQE is a core open-source feature, **not** Databricks-specific.\n",
    "\n",
    "- Introduced in **Apache Spark 3.0** (2020)\n",
    "- Enabled by **default** starting in **Spark 3.2+**\n",
    "- Databricks:\n",
    "  - Turned it on earlier\n",
    "  - Added better heuristics & skew detection\n",
    "  - Integrated it tightly with Photon\n",
    "  - Uses more aggressive defaults\n",
    "\n",
    "## How Planning & Execution Differ\n",
    "\n",
    "### Without AQE (Static Planning)\n",
    "\n",
    "1. Logical plan ‚Üí optimized logical plan\n",
    "2. Physical plan generated **once**\n",
    "3. Execution follows that plan **rigidly**\n",
    "   - Wrong join strategy? ‚Üí Stuck with it\n",
    "   - Bad shuffle partition count? ‚Üí Many tiny tasks or huge spills\n",
    "\n",
    "### With AQE Enabled (Adaptive Planning)\n",
    "\n",
    "1. Spark creates an **initial physical plan**\n",
    "2. Execution begins\n",
    "3. As stages complete ‚Üí Spark collects **real runtime stats**\n",
    "4. Spark **re-optimizes** subsequent stages based on actual data\n",
    "   - Happens at **stage boundaries** (not arbitrary mid-operator)\n",
    "\n",
    "## Core AQE Features in Open-Source Apache Spark\n",
    "\n",
    "1. **Dynamic Join Strategy Switching**  \n",
    "   - Planned: Sort-Merge Join  \n",
    "   - Runtime: one side is tiny ‚Üí switches to **Broadcast Hash Join**  \n",
    "   ‚Üí Massive performance gain in many cases\n",
    "\n",
    "2. **Dynamic Shuffle Partition Coalescing**  \n",
    "   - Static: `spark.sql.shuffle.partitions = 200` ‚Üí 200 tiny tasks  \n",
    "   - AQE: sees small partitions ‚Üí **coalesces** them into fewer, larger tasks  \n",
    "   ‚Üí Less scheduling overhead, better resource utilization\n",
    "\n",
    "3. **Skew Join Mitigation**  \n",
    "   - Detects heavily skewed partitions  \n",
    "   - Automatically **splits** them into sub-tasks  \n",
    "   ‚Üí Prevents single executor from being overwhelmed\n",
    "\n",
    "## What Databricks Adds on Top\n",
    "\n",
    "Databricks enhances (but does **not** invent) AQE:\n",
    "\n",
    "- More precise skew detection heuristics\n",
    "- Smarter auto-broadcast thresholds\n",
    "- Tighter integration with **Photon** engine\n",
    "- More aggressive & production-tuned defaults\n",
    "\n",
    "**Analogy**  \n",
    "Apache Spark AQE = solid adaptive engine  \n",
    "Databricks AQE = same engine + racing tires & tuning\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Aspect                     | Without AQE (Static)               | With AQE (Adaptive)                          |\n",
    "|----------------------------|------------------------------------|----------------------------------------------|\n",
    "| Plan generation            | One-time, before execution         | Initial plan + runtime re-optimizations      |\n",
    "| Join choice                | Fixed based on estimates           | Can switch dynamically (e.g., to broadcast)  |\n",
    "| Shuffle partitions         | Fixed (usually 200)                | Dynamically coalesced based on actual size   |\n",
    "| Skew handling              | Manual (user must fix)             | Automatic detection & splitting (to a degree)|\n",
    "| Typical failure mode       | Silent inefficiency                | Late but smarter correction                  |\n",
    "| Overhead                   | None                               | Small re-planning cost (usually worth it)    |\n",
    "\n",
    "## Important Caveat\n",
    "\n",
    "**AQE is not magic.**  \n",
    "It **cannot** fix:\n",
    "\n",
    "- Terrible data modeling\n",
    "- Extremely skewed keys without any mitigation\n",
    "- Joining two massive tables without filters\n",
    "- Relying on AQE instead of good partitioning strategy\n",
    "\n",
    "**AQE is a seatbelt ‚Äî not autopilot.**\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "- AQE is **real open-source Apache Spark** (since 3.0, default since 3.2)\n",
    "- Spark still creates a plan **before** execution begins\n",
    "- AQE allows **parts** of that plan to be rewritten **during** execution\n",
    "- Databricks enhances AQE ‚Äî but doesn't own it\n",
    "- Biggest wins come when data is unpredictable and statistics are unreliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef95f83-69ae-4e03-b919-4632d54c8f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ec3f69-b303-4448-95c2-f517f328a469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## If Adaptive Query Execution can dynamically change join strategies at runtime, what is the practical impact of explicitly using broadcast() joins in Spark code?\n",
    "\n",
    "In which scenarios does AQE reliably replace manual join hints, and where does explicit join strategy still matter for performance?\n",
    "\n",
    "> Can I write inefficient code and rely completely on Databricks AQE for performance optimization?\n",
    "\n",
    "### Flawed Assumption\n",
    "This question has an implicit belief:\n",
    "\n",
    "‚ÄúIf AQE can re-optimize joins at runtime, then my explicit join hints (like broadcast) don‚Äôt really matter.‚Äù\n",
    "\n",
    "That sounds logical. It‚Äôs also **dangerously incomplete**.\n",
    "\n",
    "**AQE is reactive, bounded, and late.**  \n",
    "**Your code is proactive, unbounded, and early.**\n",
    "\n",
    "Those are not interchangeable.\n",
    "\n",
    "### What AQE Can Actually Do (and When)\n",
    "AQE can:\n",
    "\n",
    "- Switch Sort-Merge ‚Üí Broadcast **only if**\n",
    "  - The table ends up below broadcast threshold\n",
    "  - Stats are collected before the join stage\n",
    "- Coalesce shuffle partitions **after** shuffle\n",
    "- Mitigate skew **after** it has already detected it\n",
    "\n",
    "**Key word: after.**\n",
    "\n",
    "AQE does not prevent bad execution paths from starting.  \n",
    "It only corrects some of them once Spark realizes it made a bad guess.\n",
    "\n",
    "### Why Explicit `broadcast()` Still Matters\n",
    "\n",
    "1. **AQE only adapts at stage boundaries**  \n",
    "   If your join happens early and triggers:\n",
    "   - A massive shuffle\n",
    "   - Disk spill\n",
    "   - Executor memory pressure\n",
    "\n",
    "   AQE cannot rewind time and undo that cost.\n",
    "\n",
    "   By contrast, an explicit broadcast:\n",
    "   - Avoids shuffle entirely\n",
    "   - Prevents spill\n",
    "   - Keeps the DAG narrow\n",
    "\n",
    "   **Preventing work > fixing work after damage is done.**\n",
    "\n",
    "2. **AQE depends on runtime statistics ‚Äî which are often unavailable**  \n",
    "   AQE can only switch to broadcast if:\n",
    "   - The small table is fully materialized\n",
    "   - Accurate size stats are known\n",
    "   - The join hasn‚Äôt already been planned into a wide stage\n",
    "\n",
    "   Cases where AQE often fails to broadcast:\n",
    "   - UDFs\n",
    "   - Complex subqueries\n",
    "   - Views over views\n",
    "   - DataFrame caching\n",
    "   - Inaccurate file-level stats (very common in data lakes)\n",
    "\n",
    "   When you write `broadcast(dim_df)`, you bypass all that uncertainty.\n",
    "\n",
    "3. **Broadcast hint is a constraint, not a suggestion**  \n",
    "   Your code says:  \n",
    ">    ‚ÄúThis table is small. Treat it as such.‚Äù\n",
    "\n",
    "   AQE says:  \n",
    ">    ‚ÄúLet me see‚Ä¶ maybe it‚Äôs small‚Ä¶ if conditions allow‚Ä¶ later.‚Äù\n",
    "\n",
    "   If you know something the optimizer doesn‚Äôt, and you don‚Äôt encode it, that‚Äôs not clean code ‚Äî that‚Äôs negligence.\n",
    "\n",
    "4. **AQE has safety limits (on purpose)**  \n",
    "   Databricks and OSS Spark cap:\n",
    "   - Auto broadcast thresholds\n",
    "   - Skew splitting aggressiveness\n",
    "   - Partition coalescing\n",
    "\n",
    "   Why? Because aggressive adaptation can crash clusters.\n",
    "\n",
    "   Your manual broadcast can exceed these thresholds safely when you know the data.\n",
    "\n",
    "   **AQE plays defense. You‚Äôre supposed to play offense.**\n",
    "\n",
    "### In short\n",
    "AQE is designed to **rescue reasonable code from unpredictable data**.  \n",
    "It is **not** designed to **rescue lazy engineers from bad decisions**.\n",
    "\n",
    "If you rely on AQE to fix:\n",
    "- Cartesian joins\n",
    "- Late filters\n",
    "- Fact‚Äìfact joins\n",
    "- Skewed keys you already know about\n",
    "\n",
    "You‚Äôre pushing cost, not eliminating it.\n",
    "\n",
    "### Concrete Comparison\n",
    "\n",
    "**Scenario**: small dimension (50 MB), large fact (2 TB)\n",
    "\n",
    "**You write bad code**:\n",
    "```python\n",
    "fact.join(dim, \"id\")\n",
    "```\n",
    "\n",
    "What happens:\n",
    "\n",
    "- Spark plans sort-merge\n",
    "- Shuffle starts\n",
    "- Spill begins\n",
    "- AQE may switch later\n",
    "\n",
    "Damage already done.\n",
    "\n",
    "You write intentional code:\n",
    "`fact.join(broadcast(dim), \"id\")`\n",
    "\n",
    "What happens:\n",
    "\n",
    "- No shuffle\n",
    "- No spill\n",
    "- No AQE intervention needed\n",
    "\n",
    "This will always be faster and more predictable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_FAQ_and_best_practices.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
