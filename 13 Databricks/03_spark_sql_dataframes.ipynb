{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f6814481-5fb1-4173-8073-fdd8433af62b","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"Y84mq7X_Z5FP"},"source":["# Module 03 - Spark SQL and DataFrames in Databricks\n","\n","## Overview\n","\n","This module focuses on Spark SQL and advanced DataFrame operations in Databricks. Since you already know PySpark, we'll focus on Databricks-specific features and optimizations.\n","\n","## Learning Objectives\n","\n","By the end of this module, you will understand:\n","- Spark SQL in Databricks environment\n","- Working with temporary views and global views\n","- Advanced DataFrame operations\n","- Window functions and aggregations\n","- Performance optimization techniques\n","- Integration between SQL and Python cells\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8066e4bd-58cc-4185-b43c-2acf83cf3b7c","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"grSZYuH2Z5FT"},"source":["## Creating Sample Datasets\n","\n","Let's create comprehensive sample datasets for our demonstrations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2b05a3ad-baee-40e7-a3db-5dee7409506e","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"XjzBIzhbZ5FU","outputId":"5e6f2cf8-126d-44f3-b9be-39a94231fa83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Employees DataFrame:\n+-----------+-------+-----------+------+----------+\n|employee_id|   name| department|salary| hire_date|\n+-----------+-------+-----------+------+----------+\n|          1|  Alice|Engineering| 75000|2020-01-15|\n|          2|    Bob|      Sales| 65000|2019-03-20|\n|          3|Charlie|Engineering| 80000|2018-06-10|\n|          4|  Diana|  Marketing| 60000|2021-02-05|\n|          5|    Eve|      Sales| 70000|2020-11-12|\n|          6|  Frank|Engineering| 85000|2017-09-01|\n|          7|  Grace|         HR| 55000|2022-01-10|\n|          8|  Henry|Engineering| 90000|2016-04-15|\n+-----------+-------+-----------+------+----------+\n\n\nSales DataFrame:\n+----------+---------+------+--------+-----------+\n| sale_date|  product|amount|quantity|employee_id|\n+----------+---------+------+--------+-----------+\n|2024-01-01|Product A| 100.0|      10|          1|\n|2024-01-02|Product B| 150.0|      15|          2|\n|2024-01-03|Product A| 120.0|      12|          1|\n|2024-01-04|Product C| 200.0|      20|          3|\n|2024-01-05|Product B| 180.0|      18|          2|\n|2024-01-06|Product A| 110.0|      11|          1|\n|2024-01-07|Product C| 220.0|      22|          3|\n|2024-01-08|Product B| 160.0|      16|          2|\n+----------+---------+------+--------+-----------+\n\n"]}],"source":["# Create sample datasets\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n","from pyspark.sql.functions import col, lit, rand, when\n","from datetime import datetime, timedelta\n","\n","# Employees dataset\n","employees_data = [\n","    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n","    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\"),\n","    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\"),\n","    (4, \"Diana\", \"Marketing\", 60000, \"2021-02-05\"),\n","    (5, \"Eve\", \"Sales\", 70000, \"2020-11-12\"),\n","    (6, \"Frank\", \"Engineering\", 85000, \"2017-09-01\"),\n","    (7, \"Grace\", \"HR\", 55000, \"2022-01-10\"),\n","    (8, \"Henry\", \"Engineering\", 90000, \"2016-04-15\"),\n","]\n","\n","employees_schema = StructType([\n","    StructField(\"employee_id\", IntegerType(), True),\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"department\", StringType(), True),\n","    StructField(\"salary\", IntegerType(), True),\n","    StructField(\"hire_date\", StringType(), True)\n","])\n","\n","employees_df = spark.createDataFrame(employees_data, employees_schema)\n","employees_df.createOrReplaceTempView(\"employees\")\n","\n","print(\"Employees DataFrame:\")\n","employees_df.show()\n","\n","# Sales dataset\n","sales_data = [\n","    (\"2024-01-01\", \"Product A\", 100.0, 10, 1),\n","    (\"2024-01-02\", \"Product B\", 150.0, 15, 2),\n","    (\"2024-01-03\", \"Product A\", 120.0, 12, 1),\n","    (\"2024-01-04\", \"Product C\", 200.0, 20, 3),\n","    (\"2024-01-05\", \"Product B\", 180.0, 18, 2),\n","    (\"2024-01-06\", \"Product A\", 110.0, 11, 1),\n","    (\"2024-01-07\", \"Product C\", 220.0, 22, 3),\n","    (\"2024-01-08\", \"Product B\", 160.0, 16, 2),\n","]\n","\n","sales_schema = StructType([\n","    StructField(\"sale_date\", StringType(), True),\n","    StructField(\"product\", StringType(), True),\n","    StructField(\"amount\", DoubleType(), True),\n","    StructField(\"quantity\", IntegerType(), True),\n","    StructField(\"employee_id\", IntegerType(), True)\n","])\n","\n","sales_df = spark.createDataFrame(sales_data, sales_schema)\n","sales_df.createOrReplaceTempView(\"sales\")\n","\n","print(\"\\nSales DataFrame:\")\n","sales_df.show()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"eaa6b8ba-dfe1-4d8a-8cde-dee4c70ae76c","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"3R6BvKLrZ5FV"},"source":["## Working with Temporary Views\n","\n","Temporary views allow you to share DataFrames between Python and SQL cells. They exist only for the current Spark session.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f6399574-9100-4466-bc1c-19cb37aafbe7","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"FrTldac_Z5FV","outputId":"45a8e102-3605-40c3-878b-982bc60d57b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Temporary view 'employees_view' created\nYou can now use it in SQL cells with: SELECT * FROM employees_view\n"]}],"source":["# Create a temporary view from DataFrame\n","employees_df.createOrReplaceTempView(\"employees_view\")\n","\n","# Now you can query it in SQL cells\n","print(\"Temporary view 'employees_view' created\")\n","print(\"You can now use it in SQL cells with: SELECT * FROM employees_view\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"c922d3da-1d35-40a2-950b-ef0d691fdebb","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"xwj9fbdfZ5FV","outputId":"e8e99be1-6d63-48c3-e70f-4a8a73e4247a"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employee_count</th><th>avg_salary</th><th>max_salary</th><th>min_salary</th></tr></thead><tbody><tr><td>Engineering</td><td>4</td><td>82500.0</td><td>90000</td><td>75000</td></tr><tr><td>Sales</td><td>2</td><td>67500.0</td><td>70000</td><td>65000</td></tr><tr><td>Marketing</td><td>1</td><td>60000.0</td><td>60000</td><td>60000</td></tr><tr><td>HR</td><td>1</td><td>55000.0</td><td>55000</td><td>55000</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Engineering",4,82500,90000,75000],["Sales",2,67500,70000,65000],["Marketing",1,60000,60000,60000],["HR",1,55000,55000,55000]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"department","nullable":true,"type":"string"},{"metadata":{},"name":"employee_count","nullable":false,"type":"long"},{"metadata":{},"name":"avg_salary","nullable":true,"type":"double"},{"metadata":{},"name":"max_salary","nullable":true,"type":"integer"},{"metadata":{},"name":"min_salary","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":4},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"employee_count","type":"\"long\""},{"metadata":"{}","name":"avg_salary","type":"\"double\""},{"metadata":"{}","name":"max_salary","type":"\"integer\""},{"metadata":"{}","name":"min_salary","type":"\"integer\""}],"type":"table"}}}],"source":["%sql\n","-- Query the temporary view using SQL\n","SELECT\n","    department,\n","    COUNT(*) as employee_count,\n","    AVG(salary) as avg_salary,\n","    MAX(salary) as max_salary,\n","    MIN(salary) as min_salary\n","FROM employees_view\n","GROUP BY department\n","ORDER BY avg_salary DESC\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"50ce1d59-a927-4200-acc1-9f2ca6fdb471","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"tC2viwn6Z5FW"},"source":["## Global Temporary Views\n","\n","Global temporary views are accessible across all Spark sessions in the same cluster. They are stored in the `global_temp` database.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"193474c3-cf19-49d3-9007-3b3ed0b2fbb2","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"Ok_N_ynZZ5FW","outputId":"9fd354a4-cc17-41b3-aa32-56e1c7742e6d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n","File \u001b[0;32m<command-5296116087534835>, line 2\u001b[0m\n","\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a global temporary view\u001b[39;00m\n","\u001b[0;32m----> 2\u001b[0m employees_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceGlobalTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_employees\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal temporary view \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_employees\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess it in SQL with: SELECT * FROM global_temp.global_employees\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2157\u001b[0m, in \u001b[0;36mDataFrame.createOrReplaceGlobalTempView\u001b[0;34m(self, name)\u001b[0m\n","\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplaceGlobalTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;32m   2154\u001b[0m     command \u001b[38;5;241m=\u001b[39m plan\u001b[38;5;241m.\u001b[39mCreateView(\n","\u001b[1;32m   2155\u001b[0m         child\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan, name\u001b[38;5;241m=\u001b[39mname, is_global\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[1;32m   2156\u001b[0m     )\u001b[38;5;241m.\u001b[39mcommand(session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n","\u001b[0;32m-> 2157\u001b[0m     _, _, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(command, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mobservations)\n","\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_info \u001b[38;5;241m=\u001b[39m ei\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1306\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n","\u001b[1;32m   1304\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n","\u001b[1;32m   1305\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n","\u001b[0;32m-> 1306\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n","\u001b[1;32m   1307\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n","\u001b[1;32m   1308\u001b[0m )\n","\u001b[1;32m   1309\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n","\u001b[1;32m   1310\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1764\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n","\u001b[1;32m   1761\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n","\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n","\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n","\u001b[1;32m   1765\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n","\u001b[1;32m   1766\u001b[0m     ):\n","\u001b[1;32m   1767\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n","\u001b[1;32m   1768\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1740\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n","\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n","\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n","\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2056\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n","\u001b[1;32m   2054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n","\u001b[0;32m-> 2056\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n","\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n","\u001b[1;32m   2058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2137\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n","\u001b[1;32m   2133\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n","\u001b[1;32m   2135\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error_with_error_info(info, status\u001b[38;5;241m.\u001b[39mmessage, status_code)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n","\u001b[0;32m-> 2137\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n","\u001b[1;32m   2138\u001b[0m                 info,\n","\u001b[1;32m   2139\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n","\u001b[1;32m   2140\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n","\u001b[1;32m   2141\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n","\u001b[1;32m   2142\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[1;32m   2144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n","\u001b[1;32m   2145\u001b[0m         message\u001b[38;5;241m=\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmessage,\n","\u001b[1;32m   2146\u001b[0m         sql_state\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n","\u001b[1;32m   2147\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\n","\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_WITH_SERVERLESS] GLOBAL TEMPORARY VIEW is not supported on serverless compute. SQLSTATE: 0A000;\n","CreateViewCommand `global_employees`, false, true, GlobalTempView, UNSUPPORTED, false\n","+- LocalRelation [employee_id#14691, name#14692, department#14693, salary#14694, hire_date#14695]\n","\n","\n","JVM stacktrace:\n","org.apache.spark.sql.catalyst.ExtendedAnalysisException\n","\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:485)\n","\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError$(SQLGatewayEdgeCheck.scala:477)\n","\tat com.databricks.sqlgateway.BlockGlobalTempView.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:207)\n","\tat com.databricks.sqlgateway.BlockGlobalTempView.throwException(SQLGatewayEdgeCheck.scala:220)\n","\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2(SQLGatewayEdgeCheck.scala:231)\n","\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2$adapted(SQLGatewayEdgeCheck.scala:225)\n","\tat org.apache.spark.sql.catalyst.plans.QueryPlan.actualFunc$1(QueryPlan.scala:600)\n","\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2(QueryPlan.scala:603)\n","\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2$adapted(QueryPlan.scala:603)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:315)\n","\tat org.apache.spark.sql.catalyst.plans.QueryPlan.foreachWithSubqueries(QueryPlan.scala:603)\n","\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:225)\n","\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:207)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63(CheckAnalysis.scala:1055)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63$adapted(CheckAnalysis.scala:1055)\n","\tat scala.collection.immutable.List.foreach(List.scala:334)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:1055)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n","\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5625)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5623)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n","\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5623)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5621)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n","\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n","\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n","\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n","\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n","\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n","\tat scala.collection.immutable.List.foreach(List.scala:334)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n","\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n","\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n","\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n","\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n","\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n","\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n","\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n","\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n","\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n","\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n","\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n","\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n","\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n","\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n","\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n","\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n","\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n","\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n","\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n","\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n","\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n","\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n","\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n","\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n","\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n","\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n","\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n","\tat scala.util.Try$.apply(Try.scala:217)\n","\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n","\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n","\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n","\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n","\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n","\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n","\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$17(Dataset.scala:282)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n","\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:279)\n","\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleCreateViewCommand(SparkConnectPlanner.scala:4010)\n","\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3452)\n","\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n","\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n","\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n","\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n","\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n","\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n","\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n","\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n","\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n","\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n","\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n","\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n","\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n","\tat scala.Option.map(Option.scala:242)\n","\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:217)\n","\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n","\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n","\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n","\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n","\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n","\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n","\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n","\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n","\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n","\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n","\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n","\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n","\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n","\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n","\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n","\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n","\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n","\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n","\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n","\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n","\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n","\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n","\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n","\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n","\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"datasetInfos":[],"jupyterProps":{"ename":"AnalysisException","evalue":"[NOT_SUPPORTED_WITH_SERVERLESS] GLOBAL TEMPORARY VIEW is not supported on serverless compute. SQLSTATE: 0A000;\nCreateViewCommand `global_employees`, false, true, GlobalTempView, UNSUPPORTED, false\n+- LocalRelation [employee_id#14691, name#14692, department#14693, salary#14694, hire_date#14695]\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:485)\n\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError$(SQLGatewayEdgeCheck.scala:477)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:207)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.throwException(SQLGatewayEdgeCheck.scala:220)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2(SQLGatewayEdgeCheck.scala:231)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2$adapted(SQLGatewayEdgeCheck.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.actualFunc$1(QueryPlan.scala:600)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2(QueryPlan.scala:603)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2$adapted(QueryPlan.scala:603)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:315)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.foreachWithSubqueries(QueryPlan.scala:603)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:225)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63(CheckAnalysis.scala:1055)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63$adapted(CheckAnalysis.scala:1055)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:1055)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5625)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5623)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5621)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$17(Dataset.scala:282)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:279)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleCreateViewCommand(SparkConnectPlanner.scala:4010)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3452)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"},"metadata":{"errorSummary":"[NOT_SUPPORTED_WITH_SERVERLESS] GLOBAL TEMPORARY VIEW is not supported on serverless compute. SQLSTATE: 0A000"},"removedWidgets":[],"sqlProps":{"breakingChangeInfo":null,"errorClass":"NOT_SUPPORTED_WITH_SERVERLESS","pysparkCallSite":"","pysparkFragment":"","pysparkSummary":null,"sqlState":"0A000","stackTrace":"org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:485)\n\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError$(SQLGatewayEdgeCheck.scala:477)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:207)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.throwException(SQLGatewayEdgeCheck.scala:220)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2(SQLGatewayEdgeCheck.scala:231)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2$adapted(SQLGatewayEdgeCheck.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.actualFunc$1(QueryPlan.scala:600)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2(QueryPlan.scala:603)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2$adapted(QueryPlan.scala:603)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:315)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.foreachWithSubqueries(QueryPlan.scala:603)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:225)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63(CheckAnalysis.scala:1055)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63$adapted(CheckAnalysis.scala:1055)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:1055)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5625)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5623)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5621)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$17(Dataset.scala:282)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:279)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleCreateViewCommand(SparkConnectPlanner.scala:4010)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3452)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)","startIndex":null,"stopIndex":null},"stackFrames":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<command-5296116087534835>, line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a global temporary view\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m employees_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceGlobalTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_employees\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal temporary view \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_employees\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess it in SQL with: SELECT * FROM global_temp.global_employees\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2157\u001b[0m, in \u001b[0;36mDataFrame.createOrReplaceGlobalTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplaceGlobalTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2154\u001b[0m     command \u001b[38;5;241m=\u001b[39m plan\u001b[38;5;241m.\u001b[39mCreateView(\n\u001b[1;32m   2155\u001b[0m         child\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan, name\u001b[38;5;241m=\u001b[39mname, is_global\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m     )\u001b[38;5;241m.\u001b[39mcommand(session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 2157\u001b[0m     _, _, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(command, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mobservations)\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_info \u001b[38;5;241m=\u001b[39m ei\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1306\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m   1305\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1306\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n\u001b[1;32m   1307\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n\u001b[1;32m   1308\u001b[0m )\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1764\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n\u001b[1;32m   1761\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[1;32m   1765\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[1;32m   1766\u001b[0m     ):\n\u001b[1;32m   1767\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1768\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1740\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2056\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 2056\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2137\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   2133\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[1;32m   2135\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error_with_error_info(info, status\u001b[38;5;241m.\u001b[39mmessage, status_code)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[0;32m-> 2137\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   2138\u001b[0m                 info,\n\u001b[1;32m   2139\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2140\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   2141\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   2142\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[1;32m   2145\u001b[0m         message\u001b[38;5;241m=\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2146\u001b[0m         sql_state\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_WITH_SERVERLESS] GLOBAL TEMPORARY VIEW is not supported on serverless compute. SQLSTATE: 0A000;\nCreateViewCommand `global_employees`, false, true, GlobalTempView, UNSUPPORTED, false\n+- LocalRelation [employee_id#14691, name#14692, department#14693, salary#14694, hire_date#14695]\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:485)\n\tat com.databricks.sqlgateway.UserFacingEndpointNameProvider.notSupportedWithDbSqlError$(SQLGatewayEdgeCheck.scala:477)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.notSupportedWithDbSqlError(SQLGatewayEdgeCheck.scala:207)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.throwException(SQLGatewayEdgeCheck.scala:220)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2(SQLGatewayEdgeCheck.scala:231)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.$anonfun$apply$2$adapted(SQLGatewayEdgeCheck.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.actualFunc$1(QueryPlan.scala:600)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2(QueryPlan.scala:603)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$foreachWithSubqueries$2$adapted(QueryPlan.scala:603)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:315)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.foreachWithSubqueries(QueryPlan.scala:603)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:225)\n\tat com.databricks.sqlgateway.BlockGlobalTempView.apply(SQLGatewayEdgeCheck.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63(CheckAnalysis.scala:1055)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$63$adapted(CheckAnalysis.scala:1055)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:1055)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5625)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$62.applyOrElse(Analyzer.scala:5623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5623)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$.apply(Analyzer.scala:5621)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$17(Dataset.scala:282)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:279)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleCreateViewCommand(SparkConnectPlanner.scala:4010)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3452)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"],"type":"baseError"}}}],"source":["# Create a global temporary view\n","employees_df.createOrReplaceGlobalTempView(\"global_employees\")\n","\n","print(\"Global temporary view 'global_employees' created\")\n","print(\"Access it in SQL with: SELECT * FROM global_temp.global_employees\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"8c13f11b-f164-448b-a723-296e7a675876","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"SbK8f2bHZ5FW","executionInfo":{"status":"error","timestamp":1767940459101,"user_tz":-330,"elapsed":28,"user":{"displayName":"Rana Nandy","userId":"16026419471031214450"}},"outputId":"fb98498d-5b77-47b8-9395-5e6628845305","colab":{"base_uri":"https://localhost:8080/","height":106}},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-1771545864.py, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1771545864.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -- Query global temporary view\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["%sql\n","-- Query global temporary view\n","SELECT * FROM global_temp.global_employees\n","LIMIT 5\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"56e7660a-61ad-4d73-8eef-efd12f18613b","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"3c3PA_bHZ5FX"},"source":["## Advanced SQL Queries\n","\n","Let's explore advanced SQL features in Databricks.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"710c79b8-9c18-4d28-a789-23adbe05774d","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"A638DihLZ5FX","outputId":"56778ae9-9186-4ef3-e073-dcfd9c03d2df"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>department</th><th>salary</th><th>product</th><th>amount</th><th>sale_date</th></tr></thead><tbody><tr><td>Charlie</td><td>Engineering</td><td>80000</td><td>Product C</td><td>220.0</td><td>2024-01-07</td></tr><tr><td>Charlie</td><td>Engineering</td><td>80000</td><td>Product C</td><td>200.0</td><td>2024-01-04</td></tr><tr><td>Bob</td><td>Sales</td><td>65000</td><td>Product B</td><td>180.0</td><td>2024-01-05</td></tr><tr><td>Bob</td><td>Sales</td><td>65000</td><td>Product B</td><td>160.0</td><td>2024-01-08</td></tr><tr><td>Bob</td><td>Sales</td><td>65000</td><td>Product B</td><td>150.0</td><td>2024-01-02</td></tr><tr><td>Alice</td><td>Engineering</td><td>75000</td><td>Product A</td><td>120.0</td><td>2024-01-03</td></tr><tr><td>Alice</td><td>Engineering</td><td>75000</td><td>Product A</td><td>110.0</td><td>2024-01-06</td></tr><tr><td>Alice</td><td>Engineering</td><td>75000</td><td>Product A</td><td>100.0</td><td>2024-01-01</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Charlie","Engineering",80000,"Product C",220,"2024-01-07"],["Charlie","Engineering",80000,"Product C",200,"2024-01-04"],["Bob","Sales",65000,"Product B",180,"2024-01-05"],["Bob","Sales",65000,"Product B",160,"2024-01-08"],["Bob","Sales",65000,"Product B",150,"2024-01-02"],["Alice","Engineering",75000,"Product A",120,"2024-01-03"],["Alice","Engineering",75000,"Product A",110,"2024-01-06"],["Alice","Engineering",75000,"Product A",100,"2024-01-01"]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"name","nullable":true,"type":"string"},{"metadata":{},"name":"department","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"integer"},{"metadata":{},"name":"product","nullable":true,"type":"string"},{"metadata":{},"name":"amount","nullable":true,"type":"double"},{"metadata":{},"name":"sale_date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":7},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"salary","type":"\"integer\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"amount","type":"\"double\""},{"metadata":"{}","name":"sale_date","type":"\"string\""}],"type":"table"}}}],"source":["%sql\n","-- JOIN operations\n","SELECT\n","    e.name,\n","    e.department,\n","    e.salary,\n","    s.product,\n","    s.amount,\n","    s.sale_date\n","FROM employees e\n","INNER JOIN sales s ON e.employee_id = s.employee_id\n","ORDER BY s.amount DESC\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"dc2b8706-0a89-40bc-b670-7618d01f5621","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"avJHw8toZ5FX","outputId":"851a264f-2e9d-47d1-8539-3c5fb51c735d"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>total_employees</th><th>total_sales</th><th>total_revenue</th><th>avg_sale_amount</th></tr></thead><tbody><tr><td>Engineering</td><td>4</td><td>5</td><td>750.0</td><td>150.0</td></tr><tr><td>Sales</td><td>2</td><td>3</td><td>490.0</td><td>163.33333333333334</td></tr><tr><td>HR</td><td>1</td><td>0</td><td>0.0</td><td>0.0</td></tr><tr><td>Marketing</td><td>1</td><td>0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Engineering",4,5,750,150],["Sales",2,3,490,163.33333333333334],["HR",1,0,0,0],["Marketing",1,0,0,0]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"department","nullable":true,"type":"string"},{"metadata":{},"name":"total_employees","nullable":false,"type":"long"},{"metadata":{},"name":"total_sales","nullable":false,"type":"long"},{"metadata":{},"name":"total_revenue","nullable":false,"type":"double"},{"metadata":{},"name":"avg_sale_amount","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":8},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"total_employees","type":"\"long\""},{"metadata":"{}","name":"total_sales","type":"\"long\""},{"metadata":"{}","name":"total_revenue","type":"\"double\""},{"metadata":"{}","name":"avg_sale_amount","type":"\"double\""}],"type":"table"}}}],"source":["%sql\n","-- LEFT JOIN with aggregation\n","SELECT\n","    e.department,\n","    COUNT(DISTINCT e.employee_id) as total_employees,\n","    COUNT(s.sale_date) as total_sales,\n","    COALESCE(SUM(s.amount), 0) as total_revenue,\n","    COALESCE(AVG(s.amount), 0) as avg_sale_amount\n","FROM employees e\n","LEFT JOIN sales s ON e.employee_id = s.employee_id\n","GROUP BY e.department\n","ORDER BY total_revenue DESC\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"167323be-4eb0-4e0b-812b-c1495f1b0ffc","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"45ZedFauZ5FX"},"source":["## Window Functions\n","\n","Window functions are powerful for analytical queries. They allow you to perform calculations across rows related to the current row.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"3df0a53c-b72f-4220-85eb-4b37878e0020","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"nGSTYsBxZ5FX","outputId":"236ef95d-3640-4880-f7e7-dcba1ead6396"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>department</th><th>salary</th><th>row_num</th><th>rank_salary</th><th>dense_rank_salary</th><th>prev_salary</th><th>next_salary</th></tr></thead><tbody><tr><td>Henry</td><td>Engineering</td><td>90000</td><td>1</td><td>1</td><td>1</td><td>null</td><td>85000</td></tr><tr><td>Frank</td><td>Engineering</td><td>85000</td><td>2</td><td>2</td><td>2</td><td>90000</td><td>80000</td></tr><tr><td>Charlie</td><td>Engineering</td><td>80000</td><td>3</td><td>3</td><td>3</td><td>85000</td><td>75000</td></tr><tr><td>Alice</td><td>Engineering</td><td>75000</td><td>4</td><td>4</td><td>4</td><td>80000</td><td>null</td></tr><tr><td>Grace</td><td>HR</td><td>55000</td><td>1</td><td>1</td><td>1</td><td>null</td><td>null</td></tr><tr><td>Diana</td><td>Marketing</td><td>60000</td><td>1</td><td>1</td><td>1</td><td>null</td><td>null</td></tr><tr><td>Eve</td><td>Sales</td><td>70000</td><td>1</td><td>1</td><td>1</td><td>null</td><td>65000</td></tr><tr><td>Bob</td><td>Sales</td><td>65000</td><td>2</td><td>2</td><td>2</td><td>70000</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Henry","Engineering",90000,1,1,1,null,85000],["Frank","Engineering",85000,2,2,2,90000,80000],["Charlie","Engineering",80000,3,3,3,85000,75000],["Alice","Engineering",75000,4,4,4,80000,null],["Grace","HR",55000,1,1,1,null,null],["Diana","Marketing",60000,1,1,1,null,null],["Eve","Sales",70000,1,1,1,null,65000],["Bob","Sales",65000,2,2,2,70000,null]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"name","nullable":true,"type":"string"},{"metadata":{},"name":"department","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"integer"},{"metadata":{},"name":"row_num","nullable":false,"type":"integer"},{"metadata":{},"name":"rank_salary","nullable":false,"type":"integer"},{"metadata":{},"name":"dense_rank_salary","nullable":false,"type":"integer"},{"metadata":{},"name":"prev_salary","nullable":true,"type":"integer"},{"metadata":{},"name":"next_salary","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":9},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"salary","type":"\"integer\""},{"metadata":"{}","name":"row_num","type":"\"integer\""},{"metadata":"{}","name":"rank_salary","type":"\"integer\""},{"metadata":"{}","name":"dense_rank_salary","type":"\"integer\""},{"metadata":"{}","name":"prev_salary","type":"\"integer\""},{"metadata":"{}","name":"next_salary","type":"\"integer\""}],"type":"table"}}}],"source":["%sql\n","-- Window functions: ROW_NUMBER, RANK, DENSE_RANK\n","SELECT\n","    name,\n","    department,\n","    salary,\n","    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num,\n","    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank_salary,\n","    DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank_salary,\n","    LAG(salary, 1) OVER (PARTITION BY department ORDER BY salary DESC) as prev_salary,\n","    LEAD(salary, 1) OVER (PARTITION BY department ORDER BY salary DESC) as next_salary\n","FROM employees\n","ORDER BY department, salary DESC\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"f4cbad93-9121-4693-971d-16af74d55d8b","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"eyQJ4vKvZ5FY","outputId":"54faabf9-a7b1-4637-f91f-fdd0b6aeda69"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sale_date</th><th>product</th><th>amount</th><th>running_total</th><th>moving_avg_3days</th><th>product_running_total</th></tr></thead><tbody><tr><td>2024-01-01</td><td>Product A</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td></tr><tr><td>2024-01-02</td><td>Product B</td><td>150.0</td><td>250.0</td><td>150.0</td><td>150.0</td></tr><tr><td>2024-01-03</td><td>Product A</td><td>120.0</td><td>370.0</td><td>110.0</td><td>220.0</td></tr><tr><td>2024-01-04</td><td>Product C</td><td>200.0</td><td>570.0</td><td>200.0</td><td>200.0</td></tr><tr><td>2024-01-05</td><td>Product B</td><td>180.0</td><td>750.0</td><td>165.0</td><td>330.0</td></tr><tr><td>2024-01-06</td><td>Product A</td><td>110.0</td><td>860.0</td><td>110.0</td><td>330.0</td></tr><tr><td>2024-01-07</td><td>Product C</td><td>220.0</td><td>1080.0</td><td>210.0</td><td>420.0</td></tr><tr><td>2024-01-08</td><td>Product B</td><td>160.0</td><td>1240.0</td><td>163.33333333333334</td><td>490.0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["2024-01-01","Product A",100,100,100,100],["2024-01-02","Product B",150,250,150,150],["2024-01-03","Product A",120,370,110,220],["2024-01-04","Product C",200,570,200,200],["2024-01-05","Product B",180,750,165,330],["2024-01-06","Product A",110,860,110,330],["2024-01-07","Product C",220,1080,210,420],["2024-01-08","Product B",160,1240,163.33333333333334,490]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"sale_date","nullable":true,"type":"string"},{"metadata":{},"name":"product","nullable":true,"type":"string"},{"metadata":{},"name":"amount","nullable":true,"type":"double"},{"metadata":{},"name":"running_total","nullable":true,"type":"double"},{"metadata":{},"name":"moving_avg_3days","nullable":true,"type":"double"},{"metadata":{},"name":"product_running_total","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":10},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"sale_date","type":"\"string\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"amount","type":"\"double\""},{"metadata":"{}","name":"running_total","type":"\"double\""},{"metadata":"{}","name":"moving_avg_3days","type":"\"double\""},{"metadata":"{}","name":"product_running_total","type":"\"double\""}],"type":"table"}}}],"source":["%sql\n","-- Window functions: Running totals and averages\n","SELECT\n","    sale_date,\n","    product,\n","    amount,\n","    SUM(amount) OVER (ORDER BY sale_date) as running_total,\n","    AVG(amount) OVER (PARTITION BY product ORDER BY sale_date\n","                      ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3days,\n","    SUM(amount) OVER (PARTITION BY product ORDER BY sale_date) as product_running_total\n","FROM sales\n","ORDER BY sale_date\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f594dda9-2363-463a-8df4-9f5ecb5b5fef","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"8ARCIKt4Z5FY"},"source":["## Advanced DataFrame Operations\n","\n","Now let's explore advanced DataFrame operations in Python.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c6e9c720-9e96-4eaa-bf92-8b9344b467eb","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"cx5oeo9BZ5FY","outputId":"679cd551-a5aa-48be-9c5a-dc4dc6a68e8b"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>department</th><th>salary</th><th>hire_date</th><th>row_num</th><th>rank_salary</th><th>dense_rank_salary</th><th>prev_salary</th><th>next_salary</th></tr></thead><tbody><tr><td>8</td><td>Henry</td><td>Engineering</td><td>90000</td><td>2016-04-15</td><td>1</td><td>1</td><td>1</td><td>null</td><td>85000</td></tr><tr><td>6</td><td>Frank</td><td>Engineering</td><td>85000</td><td>2017-09-01</td><td>2</td><td>2</td><td>2</td><td>90000</td><td>80000</td></tr><tr><td>3</td><td>Charlie</td><td>Engineering</td><td>80000</td><td>2018-06-10</td><td>3</td><td>3</td><td>3</td><td>85000</td><td>75000</td></tr><tr><td>1</td><td>Alice</td><td>Engineering</td><td>75000</td><td>2020-01-15</td><td>4</td><td>4</td><td>4</td><td>80000</td><td>null</td></tr><tr><td>7</td><td>Grace</td><td>HR</td><td>55000</td><td>2022-01-10</td><td>1</td><td>1</td><td>1</td><td>null</td><td>null</td></tr><tr><td>4</td><td>Diana</td><td>Marketing</td><td>60000</td><td>2021-02-05</td><td>1</td><td>1</td><td>1</td><td>null</td><td>null</td></tr><tr><td>5</td><td>Eve</td><td>Sales</td><td>70000</td><td>2020-11-12</td><td>1</td><td>1</td><td>1</td><td>null</td><td>65000</td></tr><tr><td>2</td><td>Bob</td><td>Sales</td><td>65000</td><td>2019-03-20</td><td>2</td><td>2</td><td>2</td><td>70000</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[8,"Henry","Engineering",90000,"2016-04-15",1,1,1,null,85000],[6,"Frank","Engineering",85000,"2017-09-01",2,2,2,90000,80000],[3,"Charlie","Engineering",80000,"2018-06-10",3,3,3,85000,75000],[1,"Alice","Engineering",75000,"2020-01-15",4,4,4,80000,null],[7,"Grace","HR",55000,"2022-01-10",1,1,1,null,null],[4,"Diana","Marketing",60000,"2021-02-05",1,1,1,null,null],[5,"Eve","Sales",70000,"2020-11-12",1,1,1,null,65000],[2,"Bob","Sales",65000,"2019-03-20",2,2,2,70000,null]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"employee_id","type":"\"integer\""},{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"salary","type":"\"integer\""},{"metadata":"{}","name":"hire_date","type":"\"string\""},{"metadata":"{}","name":"row_num","type":"\"integer\""},{"metadata":"{}","name":"rank_salary","type":"\"integer\""},{"metadata":"{}","name":"dense_rank_salary","type":"\"integer\""},{"metadata":"{}","name":"prev_salary","type":"\"integer\""},{"metadata":"{}","name":"next_salary","type":"\"integer\""}],"type":"table"}}}],"source":["# Window functions in PySpark\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, sum as spark_sum, avg as spark_avg\n","\n","# Define window specification\n","window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n","\n","# Apply window functions\n","result_df = employees_df.withColumn(\n","    \"row_num\", row_number().over(window_spec)\n",").withColumn(\n","    \"rank_salary\", rank().over(window_spec)\n",").withColumn(\n","    \"dense_rank_salary\", dense_rank().over(window_spec)\n",").withColumn(\n","    \"prev_salary\", lag(\"salary\", 1).over(window_spec)\n",").withColumn(\n","    \"next_salary\", lead(\"salary\", 1).over(window_spec)\n",")\n","\n","display(result_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"49a7262c-f604-4ff4-9f61-77f29eeb5724","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"adTT72MsZ5FY","outputId":"1ac96bb1-a76d-4fd7-d067-1f810e1add82"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>total_employees</th><th>unique_employees</th><th>avg_salary</th><th>total_salary</th><th>employee_names</th><th>unique_names</th></tr></thead><tbody><tr><td>Engineering</td><td>4</td><td>4</td><td>82500.0</td><td>330000</td><td>List(Alice, Charlie, Frank, Henry)</td><td>List(Alice, Charlie, Frank, Henry)</td></tr><tr><td>Sales</td><td>2</td><td>2</td><td>67500.0</td><td>135000</td><td>List(Bob, Eve)</td><td>List(Bob, Eve)</td></tr><tr><td>Marketing</td><td>1</td><td>1</td><td>60000.0</td><td>60000</td><td>List(Diana)</td><td>List(Diana)</td></tr><tr><td>HR</td><td>1</td><td>1</td><td>55000.0</td><td>55000</td><td>List(Grace)</td><td>List(Grace)</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Engineering",4,4,82500,330000,["Alice","Charlie","Frank","Henry"],["Alice","Charlie","Frank","Henry"]],["Sales",2,2,67500,135000,["Bob","Eve"],["Bob","Eve"]],["Marketing",1,1,60000,60000,["Diana"],["Diana"]],["HR",1,1,55000,55000,["Grace"],["Grace"]]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"total_employees","type":"\"long\""},{"metadata":"{}","name":"unique_employees","type":"\"long\""},{"metadata":"{}","name":"avg_salary","type":"\"double\""},{"metadata":"{}","name":"total_salary","type":"\"long\""},{"metadata":"{}","name":"employee_names","type":"{\"containsNull\":false,\"elementType\":\"string\",\"type\":\"array\"}"},{"metadata":"{}","name":"unique_names","type":"{\"containsNull\":false,\"elementType\":\"string\",\"type\":\"array\"}"}],"type":"table"}}}],"source":["# Complex aggregations\n","from pyspark.sql.functions import count, countDistinct, collect_list, collect_set\n","\n","agg_result = employees_df.groupBy(\"department\").agg(\n","    count(\"*\").alias(\"total_employees\"),\n","    countDistinct(\"employee_id\").alias(\"unique_employees\"),\n","    spark_avg(\"salary\").alias(\"avg_salary\"),\n","    spark_sum(\"salary\").alias(\"total_salary\"),\n","    collect_list(\"name\").alias(\"employee_names\"),\n","    collect_set(\"name\").alias(\"unique_names\")\n",")\n","\n","display(agg_result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"27cc62ab-f8cc-4f50-8db6-5593d84ad4f6","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"EhGLuKHJZ5FY","outputId":"d989ad52-7301-457f-964c-37795db2da58"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>department</th><th>product</th><th>amount</th><th>sale_date</th></tr></thead><tbody><tr><td>Alice</td><td>Engineering</td><td>Product A</td><td>100.0</td><td>2024-01-01</td></tr><tr><td>Bob</td><td>Sales</td><td>Product B</td><td>150.0</td><td>2024-01-02</td></tr><tr><td>Alice</td><td>Engineering</td><td>Product A</td><td>120.0</td><td>2024-01-03</td></tr><tr><td>Charlie</td><td>Engineering</td><td>Product C</td><td>200.0</td><td>2024-01-04</td></tr><tr><td>Bob</td><td>Sales</td><td>Product B</td><td>180.0</td><td>2024-01-05</td></tr><tr><td>Alice</td><td>Engineering</td><td>Product A</td><td>110.0</td><td>2024-01-06</td></tr><tr><td>Charlie</td><td>Engineering</td><td>Product C</td><td>220.0</td><td>2024-01-07</td></tr><tr><td>Bob</td><td>Sales</td><td>Product B</td><td>160.0</td><td>2024-01-08</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Alice","Engineering","Product A",100,"2024-01-01"],["Bob","Sales","Product B",150,"2024-01-02"],["Alice","Engineering","Product A",120,"2024-01-03"],["Charlie","Engineering","Product C",200,"2024-01-04"],["Bob","Sales","Product B",180,"2024-01-05"],["Alice","Engineering","Product A",110,"2024-01-06"],["Charlie","Engineering","Product C",220,"2024-01-07"],["Bob","Sales","Product B",160,"2024-01-08"]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"amount","type":"\"double\""},{"metadata":"{}","name":"sale_date","type":"\"string\""}],"type":"table"}}}],"source":["# Joins in PySpark\n","joined_df = employees_df.join(\n","    sales_df,\n","    employees_df.employee_id == sales_df.employee_id,\n","    \"inner\"\n",").select(\n","    employees_df[\"name\"],\n","    employees_df[\"department\"],\n","    sales_df[\"product\"],\n","    sales_df[\"amount\"],\n","    sales_df[\"sale_date\"]\n",")\n","\n","display(joined_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b448a19f-bdad-4132-ad41-154ba5042e9b","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"w8mZ0eBxZ5FY","outputId":"a80a8370-0c1c-4c78-f5aa-0e9e0e9d8523"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sale_date</th><th>Product A</th><th>Product B</th><th>Product C</th></tr></thead><tbody><tr><td>2024-01-01</td><td>100.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2024-01-02</td><td>0.0</td><td>150.0</td><td>0.0</td></tr><tr><td>2024-01-03</td><td>120.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2024-01-04</td><td>0.0</td><td>0.0</td><td>200.0</td></tr><tr><td>2024-01-05</td><td>0.0</td><td>180.0</td><td>0.0</td></tr><tr><td>2024-01-06</td><td>110.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2024-01-07</td><td>0.0</td><td>0.0</td><td>220.0</td></tr><tr><td>2024-01-08</td><td>0.0</td><td>160.0</td><td>0.0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["2024-01-01",100,0,0],["2024-01-02",0,150,0],["2024-01-03",120,0,0],["2024-01-04",0,0,200],["2024-01-05",0,180,0],["2024-01-06",110,0,0],["2024-01-07",0,0,220],["2024-01-08",0,160,0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"sale_date","type":"\"string\""},{"metadata":"{}","name":"Product A","type":"\"double\""},{"metadata":"{}","name":"Product B","type":"\"double\""},{"metadata":"{}","name":"Product C","type":"\"double\""}],"type":"table"}}}],"source":["# Pivot operations\n","from pyspark.sql.functions import first\n","\n","pivot_df = sales_df.groupBy(\"sale_date\").pivot(\"product\").agg(\n","    spark_sum(\"amount\").alias(\"total_amount\")\n",").na.fill(0)\n","\n","display(pivot_df)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a1d7871a-baec-4d0c-befe-5e1a6d86e9cb","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"hD3ag-r-Z5FZ"},"source":["## User-Defined Functions (UDFs)\n","\n","UDFs allow you to extend Spark SQL with custom functions. However, use them sparingly as they can impact performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1462666b-3ff8-4a79-9ebb-00709844a73e","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"SYDrzDyJZ5FZ","outputId":"fe27eb03-b2cf-4ea8-9f68-d2aa85a75ba8"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>department</th><th>salary</th><th>hire_date</th><th>salary_category</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>Engineering</td><td>75000</td><td>2020-01-15</td><td>Medium</td></tr><tr><td>2</td><td>Bob</td><td>Sales</td><td>65000</td><td>2019-03-20</td><td>Medium</td></tr><tr><td>3</td><td>Charlie</td><td>Engineering</td><td>80000</td><td>2018-06-10</td><td>High</td></tr><tr><td>4</td><td>Diana</td><td>Marketing</td><td>60000</td><td>2021-02-05</td><td>Low</td></tr><tr><td>5</td><td>Eve</td><td>Sales</td><td>70000</td><td>2020-11-12</td><td>Medium</td></tr><tr><td>6</td><td>Frank</td><td>Engineering</td><td>85000</td><td>2017-09-01</td><td>High</td></tr><tr><td>7</td><td>Grace</td><td>HR</td><td>55000</td><td>2022-01-10</td><td>Low</td></tr><tr><td>8</td><td>Henry</td><td>Engineering</td><td>90000</td><td>2016-04-15</td><td>High</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,"Alice","Engineering",75000,"2020-01-15","Medium"],[2,"Bob","Sales",65000,"2019-03-20","Medium"],[3,"Charlie","Engineering",80000,"2018-06-10","High"],[4,"Diana","Marketing",60000,"2021-02-05","Low"],[5,"Eve","Sales",70000,"2020-11-12","Medium"],[6,"Frank","Engineering",85000,"2017-09-01","High"],[7,"Grace","HR",55000,"2022-01-10","Low"],[8,"Henry","Engineering",90000,"2016-04-15","High"]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"employee_id","type":"\"integer\""},{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"salary","type":"\"integer\""},{"metadata":"{}","name":"hire_date","type":"\"string\""},{"metadata":"{}","name":"salary_category","type":"\"string\""}],"type":"table"}}}],"source":["# Register UDF\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","# Simple UDF\n","def categorize_salary(salary):\n","    if salary >= 80000:\n","        return \"High\"\n","    elif salary >= 65000:\n","        return \"Medium\"\n","    else:\n","        return \"Low\"\n","\n","salary_category_udf = udf(categorize_salary, StringType())\n","\n","# Use UDF\n","result_df = employees_df.withColumn(\n","    \"salary_category\",\n","    salary_category_udf(col(\"salary\"))\n",")\n","\n","display(result_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"99ee9019-a9e5-4a32-9370-cdb9aebe959f","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"fdsQqPVIZ5FZ","outputId":"38aef615-4ff3-41d7-b547-fbcad5ee9990"},"outputs":[{"output_type":"stream","name":"stdout","text":["UDF registered. You can now use it in SQL cells:\nSELECT name, salary, categorize_salary(salary) as category FROM employees\n"]}],"source":["# Register UDF for SQL use\n","spark.udf.register(\"categorize_salary\", categorize_salary, StringType())\n","\n","# Now use it in SQL\n","print(\"UDF registered. You can now use it in SQL cells:\")\n","print(\"SELECT name, salary, categorize_salary(salary) as category FROM employees\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"df40c37a-2895-4898-8925-84e866a08d88","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"9HLPHyIXZ5FZ","outputId":"4f52199a-4802-4721-a5bc-adc1da07797f"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>salary</th><th>salary_category</th></tr></thead><tbody><tr><td>Henry</td><td>90000</td><td>High</td></tr><tr><td>Frank</td><td>85000</td><td>High</td></tr><tr><td>Charlie</td><td>80000</td><td>High</td></tr><tr><td>Alice</td><td>75000</td><td>Medium</td></tr><tr><td>Eve</td><td>70000</td><td>Medium</td></tr><tr><td>Bob</td><td>65000</td><td>Medium</td></tr><tr><td>Diana</td><td>60000</td><td>Low</td></tr><tr><td>Grace</td><td>55000</td><td>Low</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Henry",90000,"High"],["Frank",85000,"High"],["Charlie",80000,"High"],["Alice",75000,"Medium"],["Eve",70000,"Medium"],["Bob",65000,"Medium"],["Diana",60000,"Low"],["Grace",55000,"Low"]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"name","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"integer"},{"metadata":{},"name":"salary_category","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":17},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"salary","type":"\"integer\""},{"metadata":"{}","name":"salary_category","type":"\"string\""}],"type":"table"}}}],"source":["%sql\n","-- Use the registered UDF\n","SELECT\n","    name,\n","    salary,\n","    categorize_salary(salary) as salary_category\n","FROM employees\n","ORDER BY salary DESC\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f1baa7dc-523f-445c-9371-0c1a1201b8c4","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"-AX-xJEuZ5FZ"},"source":["## Performance Optimization Techniques\n","\n","### 1. Caching and Persistence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"0e244201-c0c0-4ae8-a1fb-2b1b8ca37b83","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"Uand2cfWZ5FZ","outputId":"6010cbef-8d37-403d-f936-9d7c4de7f647"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n","File \u001b[0;32m<command-5296116087534853>, line 13\u001b[0m\n","\u001b[1;32m      2\u001b[0m complex_df \u001b[38;5;241m=\u001b[39m employees_df\u001b[38;5;241m.\u001b[39mjoin(\n","\u001b[1;32m      3\u001b[0m     sales_df,\n","\u001b[1;32m      4\u001b[0m     employees_df\u001b[38;5;241m.\u001b[39memployee_id \u001b[38;5;241m==\u001b[39m sales_df\u001b[38;5;241m.\u001b[39memployee_id,\n","\u001b[0;32m   (...)\u001b[0m\n","\u001b[1;32m      9\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;32m     10\u001b[0m )\n","\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Cache the DataFrame\u001b[39;00m\n","\u001b[0;32m---> 13\u001b[0m complex_df\u001b[38;5;241m.\u001b[39mcache()\n","\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# First action - will compute and cache\u001b[39;00m\n","\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCached DataFrame count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomplex_df\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2163\u001b[0m, in \u001b[0;36mDataFrame.cache\u001b[0;34m(self)\u001b[0m\n","\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;32m-> 2163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersist()\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2172\u001b[0m, in \u001b[0;36mDataFrame.persist\u001b[0;34m(self, storageLevel)\u001b[0m\n","\u001b[1;32m   2167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpersist\u001b[39m(\n","\u001b[1;32m   2168\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n","\u001b[1;32m   2169\u001b[0m     storageLevel: StorageLevel \u001b[38;5;241m=\u001b[39m (StorageLevel\u001b[38;5;241m.\u001b[39mMEMORY_AND_DISK_DESER),\n","\u001b[1;32m   2170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[1;32m   2171\u001b[0m     relation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mplan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n","\u001b[0;32m-> 2172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_analyze(\n","\u001b[1;32m   2173\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersist\u001b[39m\u001b[38;5;124m\"\u001b[39m, relation\u001b[38;5;241m=\u001b[39mrelation, storage_level\u001b[38;5;241m=\u001b[39mstorageLevel\n","\u001b[1;32m   2174\u001b[0m     )\n","\u001b[1;32m   2175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1549\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n","\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid state during retry exception handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n","\u001b[0;32m-> 1549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2056\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n","\u001b[1;32m   2054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n","\u001b[0;32m-> 2056\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n","\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n","\u001b[1;32m   2058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2137\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n","\u001b[1;32m   2133\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n","\u001b[1;32m   2135\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error_with_error_info(info, status\u001b[38;5;241m.\u001b[39mmessage, status_code)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n","\u001b[0;32m-> 2137\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n","\u001b[1;32m   2138\u001b[0m                 info,\n","\u001b[1;32m   2139\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n","\u001b[1;32m   2140\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n","\u001b[1;32m   2141\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n","\u001b[1;32m   2142\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[1;32m   2144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n","\u001b[1;32m   2145\u001b[0m         message\u001b[38;5;241m=\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmessage,\n","\u001b[1;32m   2146\u001b[0m         sql_state\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n","\u001b[1;32m   2147\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\n","\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n","\n","JVM stacktrace:\n","org.apache.spark.sql.AnalysisException\n","\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n","\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n","\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:296)\n","\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:79)\n","\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:71)\n","\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n","\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n","\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n","\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n","\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n","\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n","\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n","\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:71)\n","\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:56)\n","\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n","\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:55)\n","\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:116)\n","\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n","\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n","\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n","\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n","\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n","\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n","\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n","\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n","\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n","\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n","\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n","\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n","\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n","\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n","\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n","\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n","\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n","\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n","\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n","\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n","\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n","\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n","\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n","\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n","\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n","\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n","\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n","\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n","\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n","\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n","\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n","\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n","\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n","\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n","\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n","\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n","\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n","\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n","\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n","\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n","\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n","\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n","\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n","\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n","\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n","\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n","\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n","\tat java.lang.Thread.run(Thread.java:840)"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"datasetInfos":[],"jupyterProps":{"ename":"AnalysisException","evalue":"[NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:296)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:79)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:71)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:71)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:56)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:55)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:116)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"},"metadata":{"errorSummary":"[NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000"},"removedWidgets":[],"sqlProps":{"breakingChangeInfo":null,"errorClass":"NOT_SUPPORTED_WITH_SERVERLESS","pysparkCallSite":"","pysparkFragment":"","pysparkSummary":null,"sqlState":"0A000","stackTrace":"org.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:296)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:79)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:71)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:71)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:56)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:55)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:116)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)","startIndex":null,"stopIndex":null},"stackFrames":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<command-5296116087534853>, line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m complex_df \u001b[38;5;241m=\u001b[39m employees_df\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      3\u001b[0m     sales_df,\n\u001b[1;32m      4\u001b[0m     employees_df\u001b[38;5;241m.\u001b[39memployee_id \u001b[38;5;241m==\u001b[39m sales_df\u001b[38;5;241m.\u001b[39memployee_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Cache the DataFrame\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m complex_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# First action - will compute and cache\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCached DataFrame count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomplex_df\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2163\u001b[0m, in \u001b[0;36mDataFrame.cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersist()\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2172\u001b[0m, in \u001b[0;36mDataFrame.persist\u001b[0;34m(self, storageLevel)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpersist\u001b[39m(\n\u001b[1;32m   2168\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2169\u001b[0m     storageLevel: StorageLevel \u001b[38;5;241m=\u001b[39m (StorageLevel\u001b[38;5;241m.\u001b[39mMEMORY_AND_DISK_DESER),\n\u001b[1;32m   2170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2171\u001b[0m     relation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mplan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 2172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_analyze(\n\u001b[1;32m   2173\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersist\u001b[39m\u001b[38;5;124m\"\u001b[39m, relation\u001b[38;5;241m=\u001b[39mrelation, storage_level\u001b[38;5;241m=\u001b[39mstorageLevel\n\u001b[1;32m   2174\u001b[0m     )\n\u001b[1;32m   2175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1549\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid state during retry exception handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2056\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 2056\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2137\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   2133\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[1;32m   2135\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error_with_error_info(info, status\u001b[38;5;241m.\u001b[39mmessage, status_code)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[0;32m-> 2137\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   2138\u001b[0m                 info,\n\u001b[1;32m   2139\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2140\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   2141\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   2142\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[1;32m   2145\u001b[0m         message\u001b[38;5;241m=\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2146\u001b[0m         sql_state\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_WITH_SERVERLESS] PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.throwError(ServerlessGCEdgeCheck.scala:65)\n\tat com.databricks.serverless.ServerlessGCEdgeCheck$.checkBlockCacheCommand(ServerlessGCEdgeCheck.scala:43)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:296)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:79)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:71)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:71)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:56)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:55)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:116)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"],"type":"baseError"}}}],"source":["# Create a complex DataFrame that will be reused\n","complex_df = employees_df.join(\n","    sales_df,\n","    employees_df.employee_id == sales_df.employee_id,\n","    \"inner\"\n",").groupBy(\"department\", \"product\").agg(\n","    spark_sum(\"amount\").alias(\"total_amount\"),\n","    spark_avg(\"amount\").alias(\"avg_amount\"),\n","    count(\"*\").alias(\"transaction_count\")\n",")\n","\n","# Cache the DataFrame\n","complex_df.cache()\n","\n","# First action - will compute and cache\n","print(f\"Cached DataFrame count: {complex_df.count()}\")\n","\n","# Subsequent actions - will use cache\n","print(\"\\nUsing cached DataFrame:\")\n","display(complex_df.filter(col(\"total_amount\") > 150))\n","\n","# Unpersist when done\n","complex_df.unpersist()\n","print(\"\\nDataFrame unpersisted\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b37ccc49-f3e2-4568-8f04-5983e49f8e15","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"wkEVTbjQZ5Fa"},"source":["### 2. Broadcast Joins\n","\n","For small lookup tables, use broadcast joins to improve performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"237dc486-b865-478c-91ad-ad457c4acbd7","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"gwN5Rd8hZ5Fa","outputId":"143367fd-2834-419d-9fc3-f5cbd74890c0"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sale_date</th><th>product</th><th>amount</th><th>quantity</th><th>employee_id</th><th>department</th><th>category</th></tr></thead><tbody><tr><td>2024-01-01</td><td>Product A</td><td>100.0</td><td>10</td><td>1</td><td>null</td><td>null</td></tr><tr><td>2024-01-02</td><td>Product B</td><td>150.0</td><td>15</td><td>2</td><td>null</td><td>null</td></tr><tr><td>2024-01-03</td><td>Product A</td><td>120.0</td><td>12</td><td>1</td><td>null</td><td>null</td></tr><tr><td>2024-01-04</td><td>Product C</td><td>200.0</td><td>20</td><td>3</td><td>null</td><td>null</td></tr><tr><td>2024-01-05</td><td>Product B</td><td>180.0</td><td>18</td><td>2</td><td>null</td><td>null</td></tr><tr><td>2024-01-06</td><td>Product A</td><td>110.0</td><td>11</td><td>1</td><td>null</td><td>null</td></tr><tr><td>2024-01-07</td><td>Product C</td><td>220.0</td><td>22</td><td>3</td><td>null</td><td>null</td></tr><tr><td>2024-01-08</td><td>Product B</td><td>160.0</td><td>16</td><td>2</td><td>null</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["2024-01-01","Product A",100,10,1,null,null],["2024-01-02","Product B",150,15,2,null,null],["2024-01-03","Product A",120,12,1,null,null],["2024-01-04","Product C",200,20,3,null,null],["2024-01-05","Product B",180,18,2,null,null],["2024-01-06","Product A",110,11,1,null,null],["2024-01-07","Product C",220,22,3,null,null],["2024-01-08","Product B",160,16,2,null,null]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"sale_date","type":"\"string\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"amount","type":"\"double\""},{"metadata":"{}","name":"quantity","type":"\"integer\""},{"metadata":"{}","name":"employee_id","type":"\"integer\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"category","type":"\"string\""}],"type":"table"}}}],"source":["from pyspark.sql.functions import broadcast\n","\n","# Create a small lookup table\n","departments_df = spark.createDataFrame([\n","    (\"Engineering\", \"Tech\"),\n","    (\"Sales\", \"Business\"),\n","    (\"Marketing\", \"Business\"),\n","    (\"HR\", \"Support\")\n","], [\"department\", \"category\"])\n","\n","# Use broadcast join for small table\n","joined_with_broadcast = sales_df.join(\n","    broadcast(departments_df),\n","    sales_df.product == departments_df.department,\n","    \"left\"\n",")\n","\n","display(joined_with_broadcast)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e7f4129b-8c7c-4b48-ae87-3a87cf12a030","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"tGrgIp8CZ5Fa"},"source":["### 3. Partitioning and Coalescing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"9247585c-58cb-4a42-a699-e099ce887466","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"hJppzKdrZ5Fa","outputId":"48b1633e-4785-4520-cac5-8bb3a4020803"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mPySparkNotImplementedError\u001b[0m                Traceback (most recent call last)\n","File \u001b[0;32m<command-5296116087534857>, line 2\u001b[0m\n","\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check current partitions\u001b[39;00m\n","\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of partitions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memployees_df\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mgetNumPartitions()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Repartition\u001b[39;00m\n","\u001b[1;32m      5\u001b[0m repartitioned_df \u001b[38;5;241m=\u001b[39m employees_df\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m)\n","\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2432\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n","\u001b[1;32m   2430\u001b[0m \u001b[38;5;129m@property\u001b[39m\n","\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrdd\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Row]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;32m-> 2432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkNotImplementedError(\n","\u001b[1;32m   2433\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_IMPLEMENTED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n","\u001b[1;32m   2434\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrdd\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n","\u001b[1;32m   2435\u001b[0m     )\n","\n","\u001b[0;31mPySparkNotImplementedError\u001b[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"datasetInfos":[],"jupyterProps":{"ename":"PySparkNotImplementedError","evalue":"[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"},"metadata":{"errorSummary":"[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"},"removedWidgets":[],"sqlProps":{"breakingChangeInfo":null,"errorClass":"NOT_IMPLEMENTED","pysparkCallSite":"","pysparkFragment":"","pysparkSummary":null,"sqlState":null,"stackTrace":null,"startIndex":null,"stopIndex":null},"stackFrames":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPySparkNotImplementedError\u001b[0m                Traceback (most recent call last)","File \u001b[0;32m<command-5296116087534857>, line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check current partitions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of partitions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memployees_df\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mgetNumPartitions()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Repartition\u001b[39;00m\n\u001b[1;32m      5\u001b[0m repartitioned_df \u001b[38;5;241m=\u001b[39m employees_df\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m)\n","File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2432\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2430\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrdd\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Row]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkNotImplementedError(\n\u001b[1;32m   2433\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_IMPLEMENTED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2434\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrdd\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   2435\u001b[0m     )\n","\u001b[0;31mPySparkNotImplementedError\u001b[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"],"type":"baseError"}}}],"source":["# Check current partitions\n","print(f\"Number of partitions: {employees_df.rdd.getNumPartitions()}\")\n","\n","# Repartition\n","repartitioned_df = employees_df.repartition(4)\n","print(f\"After repartition: {repartitioned_df.rdd.getNumPartitions()} partitions\")\n","\n","# Coalesce (reduces partitions)\n","coalesced_df = repartitioned_df.coalesce(2)\n","print(f\"After coalesce: {coalesced_df.rdd.getNumPartitions()} partitions\")\n","\n","# Repartition by column (useful for joins)\n","repartitioned_by_dept = employees_df.repartition(\"department\")\n","print(f\"Repartitioned by department: {repartitioned_by_dept.rdd.getNumPartitions()} partitions\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fa3edf5f-e846-402b-9e2e-ce68fec20629","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"aZb6ASRTZ5Fa"},"source":["## Converting Between SQL and DataFrames\n","\n","One of Databricks' strengths is seamless integration between SQL and Python.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"15e7c72c-410e-47a8-9e80-a6b83f70734d","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"mZJWc0EvZ5Fa"},"outputs":[],"source":["%sql\n","-- Create a view from SQL query\n","CREATE OR REPLACE TEMP VIEW department_summary AS\n","SELECT\n","    e.department,\n","    COUNT(DISTINCT e.employee_id) as employee_count,\n","    AVG(e.salary) as avg_salary,\n","    SUM(s.amount) as total_sales\n","FROM employees e\n","LEFT JOIN sales s ON e.employee_id = s.employee_id\n","GROUP BY e.department\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2cc01efc-6670-43e8-a0ec-250d3edd5e94","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"tQOHZNaaZ5Fa","outputId":"fe215202-e710-4b35-fd86-f006f449cdb8"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employee_count</th><th>avg_salary</th><th>total_sales</th></tr></thead><tbody><tr><td>HR</td><td>1</td><td>55000.0</td><td>null</td></tr><tr><td>Sales</td><td>2</td><td>66250.0</td><td>490.0</td></tr><tr><td>Marketing</td><td>1</td><td>60000.0</td><td>null</td></tr><tr><td>Engineering</td><td>4</td><td>80000.0</td><td>750.0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["HR",1,55000,null],["Sales",2,66250,490],["Marketing",1,60000,null],["Engineering",4,80000,750]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"employee_count","type":"\"long\""},{"metadata":"{}","name":"avg_salary","type":"\"double\""},{"metadata":"{}","name":"total_sales","type":"\"double\""}],"type":"table"}}},{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employee_count</th><th>avg_salary</th><th>total_sales</th></tr></thead><tbody><tr><td>Sales</td><td>2</td><td>66250.0</td><td>490.0</td></tr><tr><td>Engineering</td><td>4</td><td>80000.0</td><td>750.0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Sales",2,66250,490],["Engineering",4,80000,750]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"employee_count","type":"\"long\""},{"metadata":"{}","name":"avg_salary","type":"\"double\""},{"metadata":"{}","name":"total_sales","type":"\"double\""}],"type":"table"}}}],"source":["# Use the SQL view in Python\n","dept_summary_df = spark.table(\"department_summary\")\n","display(dept_summary_df)\n","\n","# Now you can use DataFrame operations\n","filtered_df = dept_summary_df.filter(col(\"total_sales\") > 0)\n","display(filtered_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"06081658-aa85-48ee-8bdc-6c81d6591176","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"i1p5OBELZ5Fa","outputId":"0621b28e-051b-457b-f308-a98156229aea"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>department</th><th>sales_count</th><th>total_sales</th></tr></thead><tbody><tr><td>Bob</td><td>Sales</td><td>3</td><td>490.0</td></tr><tr><td>Charlie</td><td>Engineering</td><td>2</td><td>420.0</td></tr><tr><td>Alice</td><td>Engineering</td><td>3</td><td>330.0</td></tr><tr><td>Diana</td><td>Marketing</td><td>0</td><td>null</td></tr><tr><td>Eve</td><td>Sales</td><td>0</td><td>null</td></tr><tr><td>Frank</td><td>Engineering</td><td>0</td><td>null</td></tr><tr><td>Grace</td><td>HR</td><td>0</td><td>null</td></tr><tr><td>Henry</td><td>Engineering</td><td>0</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["Bob","Sales",3,490],["Charlie","Engineering",2,420],["Alice","Engineering",3,330],["Diana","Marketing",0,null],["Eve","Sales",0,null],["Frank","Engineering",0,null],["Grace","HR",0,null],["Henry","Engineering",0,null]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"department","type":"\"string\""},{"metadata":"{}","name":"sales_count","type":"\"long\""},{"metadata":"{}","name":"total_sales","type":"\"double\""}],"type":"table"}}}],"source":["# Execute SQL from Python and get result as DataFrame\n","sql_query = \"\"\"\n","SELECT\n","    e.name,\n","    e.department,\n","    COUNT(s.sale_date) as sales_count,\n","    SUM(s.amount) as total_sales\n","FROM employees e\n","LEFT JOIN sales s ON e.employee_id = s.employee_id\n","GROUP BY e.name, e.department\n","ORDER BY total_sales DESC\n","\"\"\"\n","\n","result_df = spark.sql(sql_query)\n","display(result_df)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"75ace8e2-27d6-47a6-aa3c-e680612c3bbe","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"aWfvguToZ5Fa"},"source":["## Best Practices\n","\n","1. **Use SQL for complex queries** - Often more readable for complex joins and aggregations\n","2. **Use DataFrames for programmatic logic** - Better for conditional logic and loops\n","3. **Cache frequently used DataFrames** - But remember to unpersist when done\n","4. **Use broadcast joins for small tables** - Improves join performance\n","5. **Partition wisely** - Too many partitions can hurt performance\n","6. **Avoid UDFs when possible** - Use built-in functions for better performance\n","7. **Use temporary views** - Share data between SQL and Python cells\n","8. **Leverage display()** - Better visualization than show()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"adda37bc-d9cd-4374-8037-7a00604ab9b2","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"FzLgK67IZ5Fb"},"source":["## Summary\n","\n","In this module, you learned:\n","\n"," **Temporary and Global Views** - Sharing data between SQL and Python\n","\n"," **Advanced SQL** - JOINs, aggregations, and complex queries\n","\n"," **Window Functions** - ROW_NUMBER, RANK, running totals, and moving averages\n","\n"," **Advanced DataFrame Operations** - Window functions, pivots, and complex aggregations\n","\n"," **UDFs** - Creating and using user-defined functions\n","\n"," **Performance Optimization** - Caching, broadcast joins, and partitioning\n","\n"," **SQL-DataFrame Integration** - Seamless switching between SQL and Python\n","\n","### Next Steps\n","\n","In the next module, we'll explore:\n","- Delta Lake: ACID transactions and time travel\n","- Unity Catalog: Data governance and cataloging\n","- Jobs: Scheduling and automation\n","- Advanced Databricks features\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"02a00ce2-b16e-4de7-80bd-1a6a07b3fd19","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"0YIteVefZ5Fb"},"source":["## Exercise\n","\n","Try these exercises to practice:\n","\n","1. Create a complex SQL query with multiple JOINs and window functions\n","2. Convert the SQL query result to a DataFrame and apply additional transformations\n","3. Create a UDF and use it in both Python and SQL\n","5. Use broadcast join for a small lookup table\n","7. Use window functions to calculate running totals and moving averages\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"computePreferences":null,"dashboards":[],"environmentMetadata":null,"inputWidgetPreferences":null,"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":5296116087534859,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"03_spark_sql_dataframes","widgets":{}},"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}