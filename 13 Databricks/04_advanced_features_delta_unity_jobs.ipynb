{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"eeaaa17f-376e-413e-a6c2-eb47485417c3","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"QLq1cfTFZ8fa"},"source":["# Module 04 - Advanced Features: Delta Lake, Unity Catalog, and Jobs\n","\n","## Overview\n","\n","This module covers advanced Databricks features including Delta Lake for ACID transactions, Unity Catalog for data governance, and Jobs for automation.\n","\n","## Learning Objectives\n","\n","By the end of this module, you will understand:\n","- Delta Lake: ACID transactions, time travel, and schema evolution\n","- Unity Catalog: Data governance and cataloging\n","- Jobs: Scheduling and automating notebook execution\n","- Workflows: Orchestrating multiple tasks\n","- Best practices for production workloads\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"de3857be-79f4-4c2a-997d-e1445f67987d","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"iYRs63tMZ8fb"},"source":["## Introduction to Delta Lake\n","\n","Delta Lake is an open-source storage layer that brings ACID transactions to data lakes. It's built on top of Parquet and provides:\n","\n","### Key Features\n","\n","1. **ACID Transactions**: Ensures data consistency\n","2. **Time Travel**: Query historical versions of data\n","3. **Schema Enforcement**: Prevents bad data from being written\n","4. **Schema Evolution**: Allows schema changes over time\n","5. **Upserts**: Update and insert operations (MERGE)\n","6. **Optimized Performance**: Better query performance than Parquet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"7829aad2-5cb4-4e68-8d92-e30f28ac455e","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"PeuU4bqxZ8fc","outputId":"fd98adb5-83be-41ed-ac04-0edb883caa44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initial DataFrame:\n"]},{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>product</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>1</td><td>Product A</td><td>100.0</td><td>10</td></tr><tr><td>2</td><td>Product B</td><td>150.0</td><td>15</td></tr><tr><td>3</td><td>Product C</td><td>200.0</td><td>20</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,"Product A",100,10],[2,"Product B",150,15],[3,"Product C",200,20]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"id","type":"\"integer\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"price","type":"\"double\""},{"metadata":"{}","name":"quantity","type":"\"integer\""}],"type":"table"}}}],"source":["# Create sample data for Delta Lake demonstrations\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n","from pyspark.sql.functions import col, current_timestamp\n","\n","# Initial dataset\n","data = [\n","    (1, \"Product A\", 100.0, 10),\n","    (2, \"Product B\", 150.0, 15),\n","    (3, \"Product C\", 200.0, 20),\n","]\n","\n","schema = StructType([\n","    StructField(\"id\", IntegerType(), True),\n","    StructField(\"product\", StringType(), True),\n","    StructField(\"price\", DoubleType(), True),\n","    StructField(\"quantity\", IntegerType(), True)\n","])\n","\n","df = spark.createDataFrame(data, schema)\n","print(\"Initial DataFrame:\")\n","df.display()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"93af7f61-a9d7-4612-9ddb-70da0f48a321","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"qZmE_cjrZ8fd"},"source":["## Creating Delta Tables\n","\n","Delta tables are created by writing DataFrames in Delta format.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"ef5affbd-153f-4229-9a67-6f210f964a92","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"zByqAHaTZ8fd","outputId":"02defc67-2b97-42c7-c263-8c520c3ce40e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Delta table created at: /Volumes/workspace/default/databricks_demo/products_delta\n\nReading Delta table:\n"]},{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>product</th><th>price</th><th>quantity</th><th>category</th></tr></thead><tbody><tr><td>1</td><td>Product A</td><td>100.0</td><td>10</td><td>null</td></tr><tr><td>2</td><td>Product B</td><td>150.0</td><td>15</td><td>null</td></tr><tr><td>3</td><td>Product C</td><td>200.0</td><td>20</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,"Product A",100,10,null],[2,"Product B",150,15,null],[3,"Product C",200,20,null]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"id","type":"\"integer\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"price","type":"\"double\""},{"metadata":"{}","name":"quantity","type":"\"integer\""},{"metadata":"{}","name":"category","type":"\"string\""}],"type":"table"}}}],"source":["# Create a Delta table\n","delta_path = \"/Volumes/workspace/default/databricks_demo/products_delta\"\n","\n","# Write as Delta\n","df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n","print(f\"Delta table created at: {delta_path}\")\n","\n","# Read Delta table\n","delta_df = spark.read.format(\"delta\").load(delta_path)\n","print(\"\\nReading Delta table:\")\n","delta_df.display()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8a8b0750-889a-4aa8-a6cb-35ec05a41678","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"HMoBrc0KZ8fd"},"source":["## Delta Table Operations\n","\n","### 1. Append Mode\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"0d29bef0-fcf7-436c-91a2-ad5a7d670300","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"dvw6jQKGZ8fe","outputId":"4b3a83d2-b2d4-49dc-ee07-ac7050783556"},"outputs":[{"output_type":"stream","name":"stdout","text":["After appending:\n"]},{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>product</th><th>price</th><th>quantity</th><th>category</th></tr></thead><tbody><tr><td>1</td><td>Product A</td><td>100.0</td><td>10</td><td>null</td></tr><tr><td>2</td><td>Product B</td><td>150.0</td><td>15</td><td>null</td></tr><tr><td>3</td><td>Product C</td><td>200.0</td><td>20</td><td>null</td></tr><tr><td>4</td><td>Product D</td><td>250.0</td><td>25</td><td>null</td></tr><tr><td>5</td><td>Product E</td><td>300.0</td><td>30</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,"Product A",100,10,null],[2,"Product B",150,15,null],[3,"Product C",200,20,null],[4,"Product D",250,25,null],[5,"Product E",300,30,null]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"id","type":"\"integer\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"price","type":"\"double\""},{"metadata":"{}","name":"quantity","type":"\"integer\""},{"metadata":"{}","name":"category","type":"\"string\""}],"type":"table"}}}],"source":["# Append new data to Delta table\n","new_data = [\n","    (4, \"Product D\", 250.0, 25),\n","    (5, \"Product E\", 300.0, 30),\n","]\n","\n","new_df = spark.createDataFrame(new_data, schema)\n","new_df.write.format(\"delta\").mode(\"append\").save(delta_path)\n","\n","# Read updated table\n","updated_df = spark.read.format(\"delta\").load(delta_path)\n","print(\"After appending:\")\n","updated_df.display()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bab9af13-2062-4bb1-912b-92e3cd28a75e","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"eWSuL1_FZ8fe"},"source":["### 2. Update Operations (MERGE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"9c1e6eaf-f125-4dc3-937d-673748f96e53","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"1nl_Z4_BZ8ff","outputId":"ed321d00-d350-4b37-e0e9-dbd12e7b59a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["MERGE operation completed\n\nUpdated Delta table:\n"]},{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>product</th><th>price</th><th>quantity</th><th>category</th></tr></thead><tbody><tr><td>1</td><td>Product A Updated</td><td>110.0</td><td>12</td><td>null</td></tr><tr><td>6</td><td>Product F</td><td>350.0</td><td>35</td><td>null</td></tr><tr><td>2</td><td>Product B</td><td>150.0</td><td>15</td><td>null</td></tr><tr><td>3</td><td>Product C</td><td>200.0</td><td>20</td><td>null</td></tr><tr><td>4</td><td>Product D</td><td>250.0</td><td>25</td><td>null</td></tr><tr><td>5</td><td>Product E</td><td>300.0</td><td>30</td><td>null</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[1,"Product A Updated",110,12,null],[6,"Product F",350,35,null],[2,"Product B",150,15,null],[3,"Product C",200,20,null],[4,"Product D",250,25,null],[5,"Product E",300,30,null]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"id","type":"\"integer\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"price","type":"\"double\""},{"metadata":"{}","name":"quantity","type":"\"integer\""},{"metadata":"{}","name":"category","type":"\"string\""}],"type":"table"}}}],"source":["from delta.tables import DeltaTable\n","from pyspark.sql.functions import lit\n","\n","# Create updates DataFrame and add 'category' column with nulls to match Delta table schema\n","updates = [\n","    (1, \"Product A Updated\", 110.0, 12),  # Update existing\n","    (6, \"Product F\", 350.0, 35),          # New record\n","]\n","\n","updates_df = spark.createDataFrame(updates, schema).withColumn(\"category\", lit(None).cast(\"string\"))\n","\n","# Perform MERGE operation\n","delta_table = DeltaTable.forPath(spark, delta_path)\n","\n","delta_table.alias(\"target\").merge(\n","    updates_df.alias(\"source\"),\n","    \"target.id = source.id\"\n",").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n","\n","print(\"MERGE operation completed\")\n","print(\"\\nUpdated Delta table:\")\n","spark.read.format(\"delta\").load(delta_path).display()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"097377de-ddf0-4ed8-b4ec-db6cc7512322","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"bD7kfJ5ZZ8ff"},"source":["### 3. Time Travel\n","\n","Delta Lake maintains a history of all changes, allowing you to query previous versions.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"be3b3bf2-9015-4a0f-97a6-13d4bbc9b594","showTitle":false,"tableResultSettingsMap":{},"title":""},"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"w5-MGIP8Z8ff","executionInfo":{"status":"error","timestamp":1767948739183,"user_tz":-330,"elapsed":48,"user":{"displayName":"Rana Nandy","userId":"16026419471031214450"}},"outputId":"b080ec7d-16dc-4554-ae44-aec53f27d5db"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'DeltaTable' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1134434525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get history of Delta table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdelta_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeltaTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Delta table history:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DeltaTable' is not defined"]}],"source":["# Get history of Delta table\n","delta_table = DeltaTable.forPath(spark, delta_path)\n","history = delta_table.history()\n","\n","print(\"Delta table history:\")\n","display(history)\n","\n","# Query a specific version\n","print(\"\\nQuerying version 0 (initial version):\")\n","version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n","display(version_0)\n","\n","# Query by timestamp (if you know the timestamp)\n","# timestamp_df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01 00:00:00\").load(delta_path)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"786171ec-cb90-4390-bc12-855520035c8e","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"RkkN2yE1Z8fg"},"source":["### 4. Schema Evolution\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"8aa3da42-4c43-48d3-ba54-715743f9efd3","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"l8jrtZdrZ8fg","outputId":"dae51898-30fa-40a3-a13c-fdb05af1b325"},"outputs":[{"output_type":"stream","name":"stdout","text":["Schema evolved - new column 'category' added\n+---+-----------------+-----+--------+-----------+\n| id|          product|price|quantity|   category|\n+---+-----------------+-----+--------+-----------+\n|  7|        Product G|400.0|      40|Electronics|\n|  8|        Product H|450.0|      45|Electronics|\n|  2|        Product B|150.0|      15|       NULL|\n|  3|        Product C|200.0|      20|       NULL|\n|  4|        Product D|250.0|      25|       NULL|\n|  5|        Product E|300.0|      30|       NULL|\n|  1|Product A Updated|110.0|      12|       NULL|\n|  6|        Product F|350.0|      35|       NULL|\n+---+-----------------+-----+--------+-----------+\n\n"]}],"source":["# Add a new column to the schema\n","from pyspark.sql.types import StructType, StructField, StringType\n","\n","# New data with additional column\n","new_schema = StructType([\n","    StructField(\"id\", IntegerType(), True),\n","    StructField(\"product\", StringType(), True),\n","    StructField(\"price\", DoubleType(), True),\n","    StructField(\"quantity\", IntegerType(), True),\n","    StructField(\"category\", StringType(), True)  # New column\n","])\n","\n","new_data_with_category = [\n","    (7, \"Product G\", 400.0, 40, \"Electronics\"),\n","    (8, \"Product H\", 450.0, 45, \"Electronics\"),\n","]\n","\n","new_df_with_category = spark.createDataFrame(new_data_with_category, new_schema)\n","\n","# Write with mergeSchema option to evolve schema\n","new_df_with_category.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(delta_path)\n","\n","print(\"Schema evolved - new column 'category' added\")\n","spark.read.format(\"delta\").load(delta_path).show()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"375eb09b-2edf-48ca-98ae-4b258cc10c4a","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"Vh_NQ3uaZ8fg"},"source":["## Delta Lake Best Practices\n","\n","1. **Use Delta for all production data** - Better performance and reliability\n","2. **Partition large tables** - Improves query performance\n","3. **Compact small files** - Use OPTIMIZE to merge small files\n","4. **Vacuum old versions** - Clean up old data files (be careful with time travel)\n","5. **Use Z-ordering** - For better query performance on specific columns\n","6. **Enable schema enforcement** - Prevent bad data\n","7. **Use mergeSchema carefully** - Understand the implications\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c129f397-c9d8-462e-99f2-4fd986e2bbf4","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"oU2-aYDxZ8fg","outputId":"4e0722f4-9842-4ee4-fa54-f85ba664dab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Delta table optimized\n"]}],"source":["# Optimize Delta table (compact small files)\n","delta_table = DeltaTable.forPath(spark, delta_path)\n","delta_table.optimize().executeCompaction()\n","\n","print(\"Delta table optimized\")\n","\n","# Z-order by specific column for better query performance\n","# delta_table.optimize().executeZOrderBy(\"product\")\n","\n","# Vacuum old files (removes files older than retention period)\n","# delta_table.vacuum(retentionHours=168)  # 7 days\n","# print(\"Vacuum completed\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"15fb76d0-fc2a-422d-b9df-56dbc87463b7","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"HCjxBBM5Z8fg"},"source":["## Unity Catalog (Overview)\n","\n","Unity Catalog is Databricks' unified governance solution for data and AI. It provides:\n","\n","### Key Features\n","\n","1. **Centralized Metadata**: Single source of truth for all data assets\n","2. **Fine-grained Access Control**: Column and row-level security\n","3. **Data Lineage**: Track data flow and dependencies\n","4. **Audit Logging**: Monitor data access and changes\n","5. **Cross-cloud Support**: Works across AWS, Azure, and GCP\n","\n","### Note for Free Tier\n","\n","Unity Catalog may have limited features in the free tier. Check your Databricks edition for availability.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c6cdcc6d-4a12-4ff5-a953-45b2faeec749","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"-W-_wqL_Z8fh","outputId":"f2976cae-d237-403c-a307-3f99aa02ebec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unity Catalog is available in Databricks workspaces with appropriate licenses.\nIt provides centralized governance for all your data assets.\n\nKey concepts:\n- Catalog: Top-level container (e.g., 'main')\n- Schema/Database: Second-level container\n- Table: Actual data table\n"]}],"source":["# Unity Catalog concepts (if available in your workspace)\n","# Unity Catalog uses a three-level namespace: catalog.schema.table\n","\n","# Example: Query from Unity Catalog\n","# df = spark.table(\"main.default.products\")\n","\n","# Create table in Unity Catalog (if you have permissions)\n","# df.write.saveAsTable(\"main.default.products\")\n","\n","print(\"Unity Catalog is available in Databricks workspaces with appropriate licenses.\")\n","print(\"It provides centralized governance for all your data assets.\")\n","print(\"\\nKey concepts:\")\n","print(\"- Catalog: Top-level container (e.g., 'main')\")\n","print(\"- Schema/Database: Second-level container\")\n","print(\"- Table: Actual data table\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"22b1674f-7e7b-4a77-8eba-0ad4dfb2380f","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"LqncIZ83Z8fh"},"source":["## Working with Managed Tables\n","\n","Managed tables are tables where Databricks manages both the data and metadata.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f691da9d-5bf4-4d5b-9de5-a0aaba125d94","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"l1Lm-9-oZ8fh","outputId":"799dc603-e2fe-4f27-8c76-88cda6f3d893"},"outputs":[{"output_type":"stream","name":"stdout","text":["Managed table 'products_managed' created\nYou can query it with: SELECT * FROM products_managed\n+---+---------+-----+--------+\n| id|  product|price|quantity|\n+---+---------+-----+--------+\n|  1|Product A|100.0|      10|\n|  2|Product B|150.0|      15|\n|  3|Product C|200.0|      20|\n+---+---------+-----+--------+\n\n"]}],"source":["# Create a managed table\n","df.write.mode(\"overwrite\").saveAsTable(\"products_managed\")\n","\n","print(\"Managed table 'products_managed' created\")\n","print(\"You can query it with: SELECT * FROM products_managed\")\n","\n","# Query the managed table\n","spark.table(\"products_managed\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"implicitDf":true,"rowLimit":10000},"inputWidgets":{},"nuid":"9b140f59-f9cb-4e5f-a636-0e9280d217c5","showTitle":false,"tableResultSettingsMap":{},"title":""},"vscode":{"languageId":"sql"},"id":"d-QoBtisZ8fh","outputId":"64651304-d634-4220-b708-39a081f29bd2"},"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>product</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>3</td><td>Product C</td><td>200.0</td><td>20</td></tr><tr><td>2</td><td>Product B</td><td>150.0</td><td>15</td></tr><tr><td>1</td><td>Product A</td><td>100.0</td><td>10</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[[3,"Product C",200,20],[2,"Product B",150,15],[1,"Product A",100,10]],"datasetInfos":[{"name":"_sqldf","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"integer"},{"metadata":{},"name":"product","nullable":true,"type":"string"},{"metadata":{},"name":"price","nullable":true,"type":"double"},{"metadata":{},"name":"quantity","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null,"typeStr":"pyspark.sql.connect.dataframe.DataFrame"}],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"createTempViewForImplicitDf":true,"dataframeName":"_sqldf","executionCount":11},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"id","type":"\"integer\""},{"metadata":"{}","name":"product","type":"\"string\""},{"metadata":"{}","name":"price","type":"\"double\""},{"metadata":"{}","name":"quantity","type":"\"integer\""}],"type":"table"}}}],"source":["%sql\n","-- Query managed table using SQL\n","SELECT * FROM products_managed\n","ORDER BY price DESC\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6ebf5778-d0ee-4695-9ada-9acc1c9e8a24","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"G5PxFRaKZ8fh"},"source":["## Databricks Jobs\n","\n","Jobs allow you to run notebooks or scripts on a schedule or trigger. They're essential for production workloads.\n","\n","### Job Types\n","\n","1. **Notebook Jobs**: Run Databricks notebooks\n","2. **Python Scripts**: Run Python files\n","3. **JAR Jobs**: Run JAR files\n","4. **Spark Submit**: Run Spark applications\n","\n","### Job Features\n","\n","- **Scheduling**: Cron-based scheduling\n","- **Retries**: Automatic retry on failure\n","- **Notifications**: Email/Slack alerts\n","- **Job Clusters**: Automatic cluster management\n","- **Dependencies**: Chain multiple jobs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3f6cc1b5-892c-46be-8860-14fc29c3fb9d","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"TtbN9b25Z8fh","outputId":"4099def0-78a5-4530-a1be-57bfdc132e6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating Jobs:\n\n1. Via UI:\n   - Go to Workflows > Jobs\n   - Click 'Create Job'\n   - Add tasks (notebooks, scripts, etc.)\n   - Configure schedule and cluster\n   - Set up notifications\n\n2. Via Databricks CLI:\n   databricks jobs create --json-file job_config.json\n\n3. Via REST API:\n   POST /api/2.1/jobs/create\n\nExample job configuration:\n\nJob config structure:\n  name: Daily ETL Job\n  tasks: [{'task_key': 'extract_data', 'notebook_task': {'notebook_path': '/Users/your_email@domain.com/extract_data'}, 'existing_cluster_id': 'your-cluster-id'}, {'task_key': 'transform_data', 'notebook_task': {'notebook_path': '/Users/your_email@domain.com/transform_data'}, 'depends_on': [{'task_key': 'extract_data'}], 'existing_cluster_id': 'your-cluster-id'}]\n  schedule: {'quartz_cron_expression': '0 0 2 * * ?', 'timezone_id': 'America/New_York'}\n  email_notifications: {'on_success': ['your-email@domain.com'], 'on_failure': ['your-email@domain.com']}\n"]}],"source":["# Jobs are typically created via the Databricks UI or REST API\n","# Here's how to create a job programmatically using Databricks API\n","\n","print(\"Creating Jobs:\")\n","print(\"\\n1. Via UI:\")\n","print(\"   - Go to Workflows > Jobs\")\n","print(\"   - Click 'Create Job'\")\n","print(\"   - Add tasks (notebooks, scripts, etc.)\")\n","print(\"   - Configure schedule and cluster\")\n","print(\"   - Set up notifications\")\n","\n","print(\"\\n2. Via Databricks CLI:\")\n","print(\"   databricks jobs create --json-file job_config.json\")\n","\n","print(\"\\n3. Via REST API:\")\n","print(\"   POST /api/2.1/jobs/create\")\n","\n","print(\"\\nExample job configuration:\")\n","job_config_example = {\n","    \"name\": \"Daily ETL Job\",\n","    \"tasks\": [\n","        {\n","            \"task_key\": \"extract_data\",\n","            \"notebook_task\": {\n","                \"notebook_path\": \"/Users/your_email@domain.com/extract_data\"\n","            },\n","            \"existing_cluster_id\": \"your-cluster-id\"\n","        },\n","        {\n","            \"task_key\": \"transform_data\",\n","            \"notebook_task\": {\n","                \"notebook_path\": \"/Users/your_email@domain.com/transform_data\"\n","            },\n","            \"depends_on\": [{\"task_key\": \"extract_data\"}],\n","            \"existing_cluster_id\": \"your-cluster-id\"\n","        }\n","    ],\n","    \"schedule\": {\n","        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n","        \"timezone_id\": \"America/New_York\"\n","    },\n","    \"email_notifications\": {\n","        \"on_success\": [\"your-email@domain.com\"],\n","        \"on_failure\": [\"your-email@domain.com\"]\n","    }\n","}\n","\n","print(\"\\nJob config structure:\")\n","for key, value in job_config_example.items():\n","    print(f\"  {key}: {value}\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ead39096-2f9c-4977-8b39-62d4cdb92d9f","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"YVVqnDG9Z8fi"},"source":["## Parameterizing Notebooks for Jobs\n","\n","Notebooks can accept parameters when run as jobs, making them reusable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a7213830-1597-4ad2-8f62-f73844fbb6d0","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"PVCp-kKfZ8fi","outputId":"a530b6c9-97ae-43bb-a6fa-592a6bfa8e97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Path: /tmp/input\nOutput Path: /tmp/output\nMode: overwrite\n"]}],"source":["# Using widgets for parameters\n","dbutils.widgets.text(\"input_path\", \"/tmp/input\", \"Input Path\")\n","dbutils.widgets.text(\"output_path\", \"/tmp/output\", \"Output Path\")\n","dbutils.widgets.dropdown(\"mode\", \"overwrite\", [\"overwrite\", \"append\"], \"Write Mode\")\n","\n","# Get parameter values\n","input_path = dbutils.widgets.get(\"input_path\")\n","output_path = dbutils.widgets.get(\"output_path\")\n","mode = dbutils.widgets.get(\"mode\")\n","\n","print(f\"Input Path: {input_path}\")\n","print(f\"Output Path: {output_path}\")\n","print(f\"Mode: {mode}\")\n","\n","# Use parameters in your code\n","# df = spark.read.parquet(input_path)\n","# df.write.mode(mode).parquet(output_path)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"161ffef3-8f29-4543-84ab-da688d8f39dd","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"Y9lWBcsZZ8fi"},"source":["## Running Notebooks from Other Notebooks\n","\n","You can orchestrate workflows by running notebooks from other notebooks.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6d156ba5-c679-447f-8f74-f6bf9c34bbec","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"UVBtt6eBZ8fi"},"outputs":[],"source":["# Run another notebook\n","# result = dbutils.notebook.run(\n","#     \"/path/to/other/notebook\",\n","#     timeout_seconds=300,\n","#     arguments={\n","#         \"param1\": \"value1\",\n","#         \"param2\": \"value2\"\n","#     }\n","# )\n","\n","# print(f\"Notebook execution result: {result}\")\n","\n","print(\"To run a notebook from another notebook:\")\n","print(\"result = dbutils.notebook.run('/path/to/notebook', timeout_seconds=300)\")\n","print(\"\\nThis is useful for:\")\n","print(\"- Orchestrating multi-step workflows\")\n","print(\"- Reusing common logic\")\n","print(\"- Building modular data pipelines\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5596ad5b-5353-4d15-92c2-e9742f1d302f","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"JxFiKjxnZ8fi"},"source":["## Summary\n","\n","In this module, you learned:\n","\n","✅ **Delta Lake** - ACID transactions, time travel, schema evolution, and MERGE operations\n","\n","✅ **Unity Catalog** - Data governance and centralized metadata management\n","\n","✅ **Managed Tables** - Creating and managing tables in Databricks\n","\n","✅ **Jobs** - Scheduling and automating notebook execution\n","\n","✅ **Parameterization** - Making notebooks reusable with widgets\n","\n","✅ **Notebook Orchestration** - Running notebooks from other notebooks\n","\n","### Next Steps\n","\n","In the final module, we'll explore:\n","- Production best practices\n","- Performance optimization\n","- Monitoring and debugging\n","- Real-world scenarios and case studies\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fecf5e94-c12c-4919-944a-ddbbed925076","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"s2ED2JbJZ8fi"},"source":["## Exercise\n","\n","Try these exercises to practice:\n","\n","1. Create a Delta table and perform INSERT, UPDATE, and DELETE operations\n","2. Use time travel to query a previous version of your Delta table\n","3. Evolve the schema of a Delta table by adding a new column\n","4. Create a managed table and query it using SQL\n","7. Use MERGE to upsert data into a Delta table\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"computePreferences":null,"dashboards":[],"environmentMetadata":null,"inputWidgetPreferences":null,"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":5296116087534884,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"04_advanced_features_delta_unity_jobs","widgets":{"input_path":{"currentValue":"/tmp/input","nuid":"b12e5bd3-bd8e-44d9-a613-11c7b9a3af66","typedWidgetInfo":{"autoCreated":false,"defaultValue":"/tmp/input","label":"Input Path","name":"input_path","options":{"widgetDisplayType":"Text","validationRegex":null},"parameterDataType":"String"},"widgetInfo":{"widgetType":"text","defaultValue":"/tmp/input","label":"Input Path","name":"input_path","options":{"widgetType":"text","autoCreated":false,"validationRegex":null}}},"mode":{"currentValue":"overwrite","nuid":"46732264-1100-480f-9775-b3c4e8198256","typedWidgetInfo":{"autoCreated":false,"defaultValue":"overwrite","label":"Write Mode","name":"mode","options":{"widgetDisplayType":"Dropdown","choices":["overwrite","append"],"fixedDomain":true,"multiselect":false},"parameterDataType":"String"},"widgetInfo":{"widgetType":"dropdown","defaultValue":"overwrite","label":"Write Mode","name":"mode","options":{"widgetType":"dropdown","autoCreated":false,"choices":["overwrite","append"]}}},"output_path":{"currentValue":"/tmp/output","nuid":"eab52c79-9225-4166-bc3d-e5a03c355162","typedWidgetInfo":{"autoCreated":false,"defaultValue":"/tmp/output","label":"Output Path","name":"output_path","options":{"widgetDisplayType":"Text","validationRegex":null},"parameterDataType":"String"},"widgetInfo":{"widgetType":"text","defaultValue":"/tmp/output","label":"Output Path","name":"output_path","options":{"widgetType":"text","autoCreated":false,"validationRegex":null}}}}},"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}