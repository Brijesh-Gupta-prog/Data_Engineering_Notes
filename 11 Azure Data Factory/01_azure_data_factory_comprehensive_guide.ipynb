{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Data Factory - Comprehensive Guide\n",
        "\n",
        "## Overview\n",
        "\n",
        "This comprehensive guide covers Azure Data Factory (ADF) features, components, and best practices. Use this as a reference before working in ADF Studio to understand the concepts and architecture.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this guide, you will understand:\n",
        "- Azure Data Factory architecture and components\n",
        "- Linked Services and connection management\n",
        "- Datasets and data structure definitions\n",
        "- Pipelines and workflow orchestration\n",
        "- Various activity types and their use cases\n",
        "- Data Flows for transformations\n",
        "- Integration Runtimes and compute options\n",
        "- Triggers and scheduling\n",
        "- Parameters, variables, and expressions\n",
        "- Error handling and monitoring\n",
        "- Best practices and common patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Azure Data Factory?\n",
        "\n",
        "**Azure Data Factory (ADF)** is a cloud-based data integration service that enables you to create data-driven workflows for orchestrating and automating data movement and data transformation.\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "- **Serverless**: No infrastructure to manage\n",
        "- **Visual Interface**: Drag-and-drop pipeline designer\n",
        "- **90+ Connectors**: Built-in connectors for various data sources\n",
        "- **Code-Free ETL**: Build pipelines without writing code\n",
        "- **Hybrid Integration**: Connect to on-premises and cloud data sources\n",
        "- **Scalable**: Automatically scales based on workload\n",
        "- **Cost-Effective**: Pay only for what you use\n",
        "\n",
        "### ADF vs Traditional ETL Tools\n",
        "\n",
        "| Feature | Traditional ETL | Azure Data Factory |\n",
        "|---------|----------------|-------------------|\n",
        "| **Infrastructure** | Requires servers | Serverless |\n",
        "| **Scalability** | Manual scaling | Auto-scaling |\n",
        "| **Cost Model** | Fixed costs | Pay-per-use |\n",
        "| **Maintenance** | High maintenance | Low maintenance |\n",
        "| **Cloud Integration** | Limited | Native Azure integration |\n",
        "| **Visual Design** | Limited | Full visual designer |\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "✅ **Data Migration**: Move data from on-premises to cloud\n",
        "\n",
        "✅ **ETL/ELT Pipelines**: Extract, transform, and load data\n",
        "\n",
        "✅ **Data Integration**: Combine data from multiple sources\n",
        "\n",
        "✅ **Scheduled Data Loads**: Automate daily/weekly/monthly refreshes\n",
        "\n",
        "✅ **Data Orchestration**: Coordinate complex data workflows\n",
        "\n",
        "✅ **Real-time Data Processing**: Process streaming data\n",
        "\n",
        "✅ **Data Warehousing**: Load data into data warehouses\n",
        "\n",
        "✅ **Big Data Processing**: Process large volumes of data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ADF Architecture and Components\n",
        "\n",
        "Azure Data Factory consists of several key components that work together:\n",
        "\n",
        "```\n",
        "Azure Data Factory Instance\n",
        "│\n",
        "├── Linked Services (Connections)\n",
        "│   ├── Data Store Linked Services\n",
        "│   └── Compute Linked Services\n",
        "│\n",
        "├── Datasets (Data Definitions)\n",
        "│   ├── Source Datasets\n",
        "│   └── Sink Datasets\n",
        "│\n",
        "├── Pipelines (Workflows)\n",
        "│   ├── Activities\n",
        "│   │   ├── Data Movement Activities\n",
        "│   │   ├── Data Transformation Activities\n",
        "│   │   └── Control Flow Activities\n",
        "│   ├── Parameters\n",
        "│   └── Variables\n",
        "│\n",
        "├── Data Flows (Transformations)\n",
        "│   ├── Source Transformations\n",
        "│   ├── Transform Steps\n",
        "│   └── Sink Transformations\n",
        "│\n",
        "├── Integration Runtimes (Compute)\n",
        "│   ├── Azure Integration Runtime\n",
        "│   ├── Self-Hosted Integration Runtime\n",
        "│   └── Azure-SSIS Integration Runtime\n",
        "│\n",
        "└── Triggers (Scheduling)\n",
        "    ├── Schedule Triggers\n",
        "    ├── Tumbling Window Triggers\n",
        "    └── Event-Based Triggers\n",
        "```\n",
        "\n",
        "### Component Relationships\n",
        "\n",
        "```\n",
        "Linked Service → Dataset → Pipeline Activity\n",
        "     ↓              ↓            ↓\n",
        "  Connection    Data Structure  Task Execution\n",
        "```\n",
        "\n",
        "### ADF Studio Interface\n",
        "\n",
        "When you open ADF Studio, you'll see:\n",
        "\n",
        "- **Author Tab**: Design and create pipelines, datasets, linked services\n",
        "- **Monitor Tab**: View pipeline runs, activity executions, debug sessions\n",
        "- **Manage Tab**: Manage linked services, integration runtimes, triggers\n",
        "- **Learning Center Tab**: Browse templates and samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linked Services\n",
        "\n",
        "**Linked Services** are connection definitions that contain the connection information needed for Data Factory to connect to external resources.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Store connection information (connection strings, credentials, endpoints)\n",
        "- Enable reusability across multiple pipelines\n",
        "- Secure credential management (Azure Key Vault integration)\n",
        "- Abstract connection details from pipelines\n",
        "\n",
        "### Linked Service Structure\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"LinkedServiceName\",\n",
        "  \"type\": \"LinkedServiceType\",\n",
        "  \"typeProperties\": {\n",
        "    // Connection-specific properties\n",
        "  },\n",
        "  \"connectVia\": {\n",
        "    // Integration Runtime reference (optional)\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Types of Linked Services\n",
        "\n",
        "#### 1. Data Store Linked Services\n",
        "\n",
        "Connect to data storage systems:\n",
        "\n",
        "**Azure Storage:**\n",
        "- `AzureBlobStorage` - Azure Blob Storage\n",
        "- `AzureDataLakeStorageGen2` - ADLS Gen2\n",
        "- `AzureFileStorage` - Azure Files\n",
        "- `AzureTableStorage` - Azure Table Storage\n",
        "\n",
        "**Databases:**\n",
        "- `AzureSqlDatabase` - Azure SQL Database\n",
        "- `AzureSqlMI` - Azure SQL Managed Instance\n",
        "- `SqlServer` - SQL Server (on-premises or Azure VM)\n",
        "- `AzureSynapseAnalytics` - Azure Synapse Analytics\n",
        "- `Oracle`, `MySQL`, `PostgreSQL` - Various databases\n",
        "\n",
        "**NoSQL:**\n",
        "- `CosmosDb` - Azure Cosmos DB\n",
        "- `MongoDb` - MongoDB\n",
        "\n",
        "**File Systems:**\n",
        "- `FileServer` - On-premises file system\n",
        "- `FtpServer` - FTP server\n",
        "- `Sftp` - SFTP server\n",
        "\n",
        "**Cloud Storage:**\n",
        "- `AmazonS3` - Amazon S3\n",
        "- `GoogleCloudStorage` - Google Cloud Storage\n",
        "\n",
        "**Other:**\n",
        "- `HttpServer` - HTTP/REST APIs\n",
        "- `OData` - OData services\n",
        "- `Salesforce`, `Dynamics365` - CRM systems\n",
        "\n",
        "#### 2. Compute Linked Services\n",
        "\n",
        "Connect to compute services for data transformation:\n",
        "\n",
        "- `AzureDatabricks` - Azure Databricks\n",
        "- `AzureHDInsight` - Azure HDInsight\n",
        "- `AzureBatch` - Azure Batch\n",
        "- `AzureMachineLearning` - Azure ML\n",
        "\n",
        "### Linked Service Examples\n",
        "\n",
        "#### Azure Blob Storage Linked Service\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"LS_AzureBlobStorage\",\n",
        "  \"type\": \"AzureBlobStorage\",\n",
        "  \"typeProperties\": {\n",
        "    \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=***;EndpointSuffix=core.windows.net\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Azure SQL Database Linked Service (with Key Vault)\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"LS_AzureSQLDatabase\",\n",
        "  \"type\": \"AzureSqlDatabase\",\n",
        "  \"typeProperties\": {\n",
        "    \"connectionString\": \"Server=tcp:myserver.database.windows.net,1433;Database=mydatabase;User ID=myuser;Password=***;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\",\n",
        "    \"password\": {\n",
        "      \"type\": \"AzureKeyVaultSecret\",\n",
        "      \"store\": {\n",
        "        \"referenceName\": \"LS_AzureKeyVault\",\n",
        "        \"type\": \"LinkedServiceReference\"\n",
        "      },\n",
        "      \"secretName\": \"sqlPassword\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Self-Hosted Integration Runtime Linked Service\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"LS_SqlServerOnPrem\",\n",
        "  \"type\": \"SqlServer\",\n",
        "  \"typeProperties\": {\n",
        "    \"connectionString\": \"Integrated Security=False;Data Source=myserver;Initial Catalog=mydatabase;User ID=myuser;Password=***\",\n",
        "    \"userName\": \"myuser\",\n",
        "    \"password\": {\n",
        "      \"type\": \"SecureString\",\n",
        "      \"value\": \"***\"\n",
        "    }\n",
        "  },\n",
        "  \"connectVia\": {\n",
        "    \"referenceName\": \"IR_SelfHosted\",\n",
        "    \"type\": \"IntegrationRuntimeReference\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices for Linked Services\n",
        "\n",
        "✅ **Use Azure Key Vault**: Store sensitive credentials in Key Vault\n",
        "\n",
        "✅ **Naming Convention**: Use prefixes like `LS_` (e.g., `LS_AzureBlobStorage_Prod`)\n",
        "\n",
        "✅ **Parameterize**: Use parameters for different environments (dev, test, prod)\n",
        "\n",
        "✅ **Reuse**: Create linked services that can be reused across pipelines\n",
        "\n",
        "✅ **Documentation**: Add descriptions to linked services\n",
        "\n",
        "✅ **Test Connections**: Always test connections after creating linked services\n",
        "\n",
        "✅ **Use Integration Runtimes**: Specify Integration Runtime for on-premises connections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n",
        "\n",
        "**Datasets** represent data structures within data stores. They define the structure, location, and format of data that you want to use as input or output in activities.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Define data structure (schema, columns, data types)\n",
        "- Specify data location (path, table name, query)\n",
        "- Define data format (CSV, JSON, Parquet, etc.)\n",
        "- Enable reusability across activities\n",
        "- Support parameterization for dynamic paths\n",
        "\n",
        "### Dataset Structure\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DatasetName\",\n",
        "  \"type\": \"DatasetType\",\n",
        "  \"linkedServiceName\": {\n",
        "    \"referenceName\": \"LinkedServiceName\",\n",
        "    \"type\": \"LinkedServiceReference\"\n",
        "  },\n",
        "  \"schema\": [\n",
        "    // Schema definition (optional)\n",
        "  ],\n",
        "  \"typeProperties\": {\n",
        "    // Type-specific properties\n",
        "  },\n",
        "  \"parameters\": {\n",
        "    // Parameters for dynamic paths\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Common Dataset Types\n",
        "\n",
        "#### File-Based Datasets\n",
        "\n",
        "**DelimitedText (CSV, TSV):**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DS_SalesCSV\",\n",
        "  \"type\": \"DelimitedText\",\n",
        "  \"linkedServiceName\": {\n",
        "    \"referenceName\": \"LS_AzureBlobStorage\",\n",
        "    \"type\": \"LinkedServiceReference\"\n",
        "  },\n",
        "  \"schema\": [\n",
        "    { \"name\": \"CustomerID\", \"type\": \"Int32\" },\n",
        "    { \"name\": \"SalesAmount\", \"type\": \"Decimal\" },\n",
        "    { \"name\": \"SaleDate\", \"type\": \"DateTime\" }\n",
        "  ],\n",
        "  \"typeProperties\": {\n",
        "    \"location\": {\n",
        "      \"type\": \"AzureBlobStorageLocation\",\n",
        "      \"container\": \"raw-data\",\n",
        "      \"folderPath\": \"sales/2024\"\n",
        "    },\n",
        "    \"columnDelimiter\": \",\",\n",
        "    \"firstRowAsHeader\": true,\n",
        "    \"compressionCodec\": \"gzip\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Json:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DS_ProductsJSON\",\n",
        "  \"type\": \"Json\",\n",
        "  \"typeProperties\": {\n",
        "    \"location\": {\n",
        "      \"type\": \"AzureBlobStorageLocation\",\n",
        "      \"container\": \"data\",\n",
        "      \"folderPath\": \"products\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Parquet:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DS_SalesParquet\",\n",
        "  \"type\": \"Parquet\",\n",
        "  \"typeProperties\": {\n",
        "    \"location\": {\n",
        "      \"type\": \"AzureBlobStorageLocation\",\n",
        "      \"container\": \"processed-data\",\n",
        "      \"folderPath\": \"sales\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Database Datasets\n",
        "\n",
        "**AzureSqlTable:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DS_SalesTable\",\n",
        "  \"type\": \"AzureSqlTable\",\n",
        "  \"linkedServiceName\": {\n",
        "    \"referenceName\": \"LS_AzureSQLDatabase\",\n",
        "    \"type\": \"LinkedServiceReference\"\n",
        "  },\n",
        "  \"schema\": [\n",
        "    { \"name\": \"CustomerID\", \"type\": \"Int32\" },\n",
        "    { \"name\": \"SalesAmount\", \"type\": \"Decimal\" }\n",
        "  ],\n",
        "  \"typeProperties\": {\n",
        "    \"schema\": \"dbo\",\n",
        "    \"table\": \"Sales\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Parameterized Datasets\n",
        "\n",
        "Use parameters for dynamic paths and table names:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"DS_SalesCSV_Param\",\n",
        "  \"type\": \"DelimitedText\",\n",
        "  \"parameters\": {\n",
        "    \"folderPath\": {\n",
        "      \"type\": \"String\"\n",
        "    },\n",
        "    \"fileName\": {\n",
        "      \"type\": \"String\"\n",
        "    }\n",
        "  },\n",
        "  \"typeProperties\": {\n",
        "    \"location\": {\n",
        "      \"type\": \"AzureBlobStorageLocation\",\n",
        "      \"container\": \"raw-data\",\n",
        "      \"folderPath\": {\n",
        "        \"value\": \"@dataset().folderPath\",\n",
        "        \"type\": \"Expression\"\n",
        "      },\n",
        "      \"fileName\": {\n",
        "        \"value\": \"@dataset().fileName\",\n",
        "        \"type\": \"Expression\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Dataset Schema\n",
        "\n",
        "**Explicit Schema:**\n",
        "- Define columns and data types explicitly\n",
        "- Use when schema is known and stable\n",
        "- Better for validation and error detection\n",
        "\n",
        "**Implicit Schema:**\n",
        "- Let ADF infer schema from data\n",
        "- Use when schema is unknown or changes frequently\n",
        "- Less control but more flexible\n",
        "\n",
        "### Best Practices for Datasets\n",
        "\n",
        "✅ **Parameterize Paths**: Use parameters for dynamic paths (dates, partitions)\n",
        "\n",
        "✅ **Define Schema**: Explicitly define schema when known for better validation\n",
        "\n",
        "✅ **Use Descriptive Names**: Clear, meaningful dataset names (e.g., `DS_SalesCSV_Source`)\n",
        "\n",
        "✅ **Reuse**: Create reusable dataset definitions\n",
        "\n",
        "✅ **Compression**: Use compression for large files (gzip, snappy)\n",
        "\n",
        "✅ **Partitioning**: Use partitioned datasets for large data volumes\n",
        "\n",
        "✅ **Naming Convention**: Use prefixes like `DS_` for datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipelines\n",
        "\n",
        "**Pipelines** are logical groupings of activities that together perform a task. A pipeline defines a workflow that orchestrates data movement and transformation.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Orchestrate multiple activities in a workflow\n",
        "- Define activity dependencies and execution order\n",
        "- Accept parameters for flexibility\n",
        "- Support variables for internal logic\n",
        "- Enable error handling and retry policies\n",
        "- Can be triggered on schedule or events\n",
        "\n",
        "### Pipeline Structure\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"PipelineName\",\n",
        "  \"properties\": {\n",
        "    \"activities\": [\n",
        "      // Array of activities\n",
        "    ],\n",
        "    \"parameters\": {\n",
        "      // Pipeline parameters\n",
        "    },\n",
        "    \"variables\": {\n",
        "      // Pipeline variables\n",
        "    },\n",
        "    \"annotations\": []\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Pipeline Components\n",
        "\n",
        "#### 1. Activities\n",
        "Individual tasks that perform operations (copy, transform, etc.)\n",
        "\n",
        "#### 2. Parameters\n",
        "Input parameters passed to the pipeline (from triggers or manual execution)\n",
        "\n",
        "#### 3. Variables\n",
        "Internal variables used within the pipeline for logic and calculations\n",
        "\n",
        "#### 4. Dependencies\n",
        "Define the order of activity execution (activity B depends on activity A)\n",
        "\n",
        "#### 5. Error Handling\n",
        "Configure how to handle failures (retry, fail, continue)\n",
        "\n",
        "### Pipeline Example\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"PL_LoadSalesData\",\n",
        "  \"properties\": {\n",
        "    \"activities\": [\n",
        "      {\n",
        "        \"name\": \"CopyFromBlobToSQL\",\n",
        "        \"type\": \"Copy\",\n",
        "        \"dependsOn\": [],\n",
        "        \"inputs\": [{\"referenceName\": \"DS_SalesCSV\"}],\n",
        "        \"outputs\": [{\"referenceName\": \"DS_SalesTable\"}]\n",
        "      },\n",
        "      {\n",
        "        \"name\": \"TransformData\",\n",
        "        \"type\": \"ExecuteDataFlow\",\n",
        "        \"dependsOn\": [{\"activity\": \"CopyFromBlobToSQL\"}],\n",
        "        \"typeProperties\": {\n",
        "          \"dataflow\": {\n",
        "            \"referenceName\": \"DF_TransformSales\",\n",
        "            \"type\": \"DataFlowReference\"\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    ],\n",
        "    \"parameters\": {\n",
        "      \"sourcePath\": {\"type\": \"String\"},\n",
        "      \"targetTable\": {\"type\": \"String\"}\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Pipeline Parameters\n",
        "\n",
        "Parameters make pipelines flexible and reusable:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"parameters\": {\n",
        "    \"sourceContainer\": {\n",
        "      \"type\": \"String\",\n",
        "      \"defaultValue\": \"raw-data\"\n",
        "    },\n",
        "    \"targetSchema\": {\n",
        "      \"type\": \"String\",\n",
        "      \"defaultValue\": \"dbo\"\n",
        "    },\n",
        "    \"loadDate\": {\n",
        "      \"type\": \"String\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Accessing Parameters:**\n",
        "- In expressions: `@pipeline().parameters.sourceContainer`\n",
        "- In activities: Reference parameters in activity properties\n",
        "\n",
        "### Pipeline Variables\n",
        "\n",
        "Variables store intermediate values:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"variables\": {\n",
        "    \"rowCount\": {\n",
        "      \"type\": \"Int32\",\n",
        "      \"defaultValue\": 0\n",
        "    },\n",
        "    \"processDate\": {\n",
        "      \"type\": \"String\",\n",
        "      \"defaultValue\": \"@formatDateTime(utcnow(), 'yyyy-MM-dd')\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Setting Variables:**\n",
        "- Use `Set Variable` activity\n",
        "- Use expressions: `@setVariable('rowCount', 100)`\n",
        "\n",
        "**Accessing Variables:**\n",
        "- In expressions: `@variables('rowCount')`\n",
        "\n",
        "### Activity Dependencies\n",
        "\n",
        "Control execution order:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"ActivityB\",\n",
        "  \"dependsOn\": [\n",
        "    {\n",
        "      \"activity\": \"ActivityA\",\n",
        "      \"dependencyConditions\": [\"Succeeded\"]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Dependency Conditions:**\n",
        "- `Succeeded` - Activity must succeed\n",
        "- `Failed` - Activity must fail\n",
        "- `Completed` - Activity must complete (succeed or fail)\n",
        "- `Skipped` - Activity must be skipped\n",
        "\n",
        "### Best Practices for Pipelines\n",
        "\n",
        "✅ **Single Responsibility**: Each pipeline should have one clear purpose\n",
        "\n",
        "✅ **Parameterize**: Use parameters for flexibility and reusability\n",
        "\n",
        "✅ **Error Handling**: Implement proper error handling and retry policies\n",
        "\n",
        "✅ **Logging**: Add logging activities for debugging\n",
        "\n",
        "✅ **Naming**: Use descriptive names (e.g., `PL_LoadSalesData_Daily`)\n",
        "\n",
        "✅ **Documentation**: Add descriptions and annotations\n",
        "\n",
        "✅ **Modularity**: Break complex pipelines into smaller, reusable pipelines\n",
        "\n",
        "✅ **Version Control**: Use Git integration for version control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activities\n",
        "\n",
        "**Activities** are individual tasks within a pipeline. Each activity performs a specific operation on data or controls pipeline flow.\n",
        "\n",
        "### Activity Categories\n",
        "\n",
        "#### 1. Data Movement Activities\n",
        "\n",
        "##### Copy Activity\n",
        "The most common activity for copying data from source to sink.\n",
        "\n",
        "**Key Features:**\n",
        "- Supports 90+ data sources\n",
        "- Handles schema mapping automatically\n",
        "- Supports transformations during copy\n",
        "- Parallel copy for performance\n",
        "- Data type conversion\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"CopySalesData\",\n",
        "  \"type\": \"Copy\",\n",
        "  \"inputs\": [{\"referenceName\": \"DS_Source\"}],\n",
        "  \"outputs\": [{\"referenceName\": \"DS_Sink\"}],\n",
        "  \"typeProperties\": {\n",
        "    \"source\": {\n",
        "      \"type\": \"DelimitedTextSource\",\n",
        "      \"skipLineCount\": 1\n",
        "    },\n",
        "    \"sink\": {\n",
        "      \"type\": \"DelimitedTextSink\",\n",
        "      \"writeBehavior\": \"append\"\n",
        "    },\n",
        "    \"enableStaging\": true,\n",
        "    \"stagingSettings\": {\n",
        "      \"linkedServiceName\": {\n",
        "        \"referenceName\": \"LS_BlobStorage\",\n",
        "        \"type\": \"LinkedServiceReference\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### 2. Data Transformation Activities\n",
        "\n",
        "##### Data Flow Activity\n",
        "Transform data using visual data flows (Spark-based).\n",
        "\n",
        "**Key Features:**\n",
        "- Code-free transformations\n",
        "- Spark-based execution\n",
        "- Supports complex transformations\n",
        "- Visual designer interface\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"TransformData\",\n",
        "  \"type\": \"ExecuteDataFlow\",\n",
        "  \"typeProperties\": {\n",
        "    \"dataflow\": {\n",
        "      \"referenceName\": \"DF_TransformSales\",\n",
        "      \"type\": \"DataFlowReference\"\n",
        "    },\n",
        "    \"compute\": {\n",
        "      \"computeType\": \"General\",\n",
        "      \"coreCount\": 8\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "##### Stored Procedure Activity\n",
        "Execute SQL stored procedures.\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"ExecuteSP\",\n",
        "  \"type\": \"SqlServerStoredProcedure\",\n",
        "  \"typeProperties\": {\n",
        "    \"storedProcedureName\": \"sp_ProcessSales\",\n",
        "    \"storedProcedureParameters\": {\n",
        "      \"LoadDate\": {\n",
        "        \"value\": \"@pipeline().parameters.loadDate\",\n",
        "        \"type\": \"String\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "##### Lookup Activity\n",
        "Look up values from datasets (single value or row).\n",
        "\n",
        "**Use Cases:**\n",
        "- Get configuration values\n",
        "- Reference data lookups\n",
        "- Conditional logic based on lookup results\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"LookupConfig\",\n",
        "  \"type\": \"Lookup\",\n",
        "  \"typeProperties\": {\n",
        "    \"source\": {\n",
        "      \"type\": \"AzureSqlSource\",\n",
        "      \"sqlReaderQuery\": \"SELECT ConfigValue FROM Config WHERE ConfigKey = 'MaxRows'\"\n",
        "    },\n",
        "    \"dataset\": {\n",
        "      \"referenceName\": \"DS_ConfigTable\",\n",
        "      \"type\": \"DatasetReference\"\n",
        "    },\n",
        "    \"firstRowOnly\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### 3. Control Flow Activities\n",
        "\n",
        "##### If Condition Activity\n",
        "Conditional branching (IF-THEN-ELSE logic).\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"CheckDataExists\",\n",
        "  \"type\": \"IfCondition\",\n",
        "  \"typeProperties\": {\n",
        "    \"expression\": {\n",
        "      \"value\": \"@greater(activity('LookupRowCount').output.firstRow.count, 0)\",\n",
        "      \"type\": \"Expression\"\n",
        "    },\n",
        "    \"ifTrueActivities\": [\n",
        "      {\n",
        "        \"name\": \"ProcessData\",\n",
        "        \"type\": \"Copy\"\n",
        "      }\n",
        "    ],\n",
        "    \"ifFalseActivities\": [\n",
        "      {\n",
        "        \"name\": \"LogNoData\",\n",
        "        \"type\": \"WebActivity\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "##### ForEach Activity\n",
        "Loop through items (arrays).\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"ProcessFiles\",\n",
        "  \"type\": \"ForEach\",\n",
        "  \"typeProperties\": {\n",
        "    \"items\": {\n",
        "      \"value\": \"@pipeline().parameters.fileList\",\n",
        "      \"type\": \"Expression\"\n",
        "    },\n",
        "    \"isSequential\": false,\n",
        "    \"batchCount\": 5,\n",
        "    \"activities\": [\n",
        "      {\n",
        "        \"name\": \"CopyFile\",\n",
        "        \"type\": \"Copy\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "##### Wait Activity\n",
        "Pause pipeline execution.\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"WaitForProcessing\",\n",
        "  \"type\": \"Wait\",\n",
        "  \"typeProperties\": {\n",
        "    \"waitTimeInSeconds\": 300\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "##### Until Activity\n",
        "Loop until condition is met (retry logic).\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"WaitForFile\",\n",
        "  \"type\": \"Until\",\n",
        "  \"typeProperties\": {\n",
        "    \"expression\": {\n",
        "      \"value\": \"@equals(activity('CheckFile').output.exists, true)\",\n",
        "      \"type\": \"Expression\"\n",
        "    },\n",
        "    \"timeout\": \"00:10:00\",\n",
        "    \"activities\": [\n",
        "      {\n",
        "        \"name\": \"CheckFile\",\n",
        "        \"type\": \"GetMetadata\"\n",
        "      },\n",
        "      {\n",
        "        \"name\": \"Wait\",\n",
        "        \"type\": \"Wait\",\n",
        "        \"typeProperties\": {\n",
        "          \"waitTimeInSeconds\": 30\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### 4. Other Activities\n",
        "\n",
        "##### Web Activity\n",
        "Call REST APIs.\n",
        "\n",
        "##### Get Metadata Activity\n",
        "Get metadata about data (file existence, schema, etc.).\n",
        "\n",
        "##### Set Variable Activity\n",
        "Set pipeline variable values.\n",
        "\n",
        "##### Filter Activity\n",
        "Filter arrays based on conditions.\n",
        "\n",
        "##### Validation Activity\n",
        "Validate data before processing.\n",
        "\n",
        "### Activity Retry and Timeout\n",
        "\n",
        "Configure retry and timeout policies:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"CopyData\",\n",
        "  \"type\": \"Copy\",\n",
        "  \"policy\": {\n",
        "    \"timeout\": \"01:00:00\",\n",
        "    \"retry\": 3,\n",
        "    \"retryIntervalInSeconds\": 30\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices for Activities\n",
        "\n",
        "✅ **Error Handling**: Configure retry policies for transient failures\n",
        "\n",
        "✅ **Timeout**: Set appropriate timeouts\n",
        "\n",
        "✅ **Dependencies**: Clearly define activity dependencies\n",
        "\n",
        "✅ **Naming**: Use descriptive activity names\n",
        "\n",
        "✅ **Logging**: Add logging for debugging\n",
        "\n",
        "✅ **Performance**: Use parallel copy and staging for large data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Flows\n",
        "\n",
        "**Data Flows** are visual data transformation pipelines that run on Spark clusters. They provide a code-free way to transform data at scale.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Transform data without writing code\n",
        "- Handle complex transformations visually\n",
        "- Scale automatically with Spark\n",
        "- Support data quality and profiling\n",
        "- Reusable across multiple pipelines\n",
        "\n",
        "### Data Flow Architecture\n",
        "\n",
        "```\n",
        "Data Flow\n",
        "├── Source (Read data)\n",
        "├── Transformations (Transform data)\n",
        "│   ├── Select\n",
        "│   ├── Filter\n",
        "│   ├── Aggregate\n",
        "│   ├── Join\n",
        "│   ├── Derived Column\n",
        "│   ├── Sort\n",
        "│   └── ... (many more)\n",
        "└── Sink (Write data)\n",
        "```\n",
        "\n",
        "### Key Transformations\n",
        "\n",
        "#### 1. Source Transformation\n",
        "- Read data from datasets\n",
        "- Define schema\n",
        "- Configure data sampling\n",
        "- Set up partitioning\n",
        "\n",
        "#### 2. Select Transformation\n",
        "- Select, rename, or drop columns\n",
        "- Reorder columns\n",
        "- Change data types\n",
        "\n",
        "#### 3. Filter Transformation\n",
        "- Filter rows based on conditions\n",
        "- Use expressions for complex filters\n",
        "\n",
        "#### 4. Derived Column Transformation\n",
        "- Create new columns\n",
        "- Modify existing columns\n",
        "- Use expressions and functions\n",
        "\n",
        "**Example Expressions:**\n",
        "- `toUpper(columnName)` - Convert to uppercase\n",
        "- `concat(firstName, ' ', lastName)` - Concatenate strings\n",
        "- `year(currentDate())` - Get year from date\n",
        "- `iif(amount > 1000, 'High', 'Low')` - Conditional logic\n",
        "\n",
        "#### 5. Aggregate Transformation\n",
        "- Group by columns\n",
        "- Calculate aggregations (sum, avg, count, etc.)\n",
        "- Window functions\n",
        "\n",
        "#### 6. Join Transformation\n",
        "- Inner join, left join, right join, full outer join\n",
        "- Join on multiple columns\n",
        "- Handle nulls\n",
        "\n",
        "#### 7. Sort Transformation\n",
        "- Sort by one or more columns\n",
        "- Ascending or descending\n",
        "- Null handling\n",
        "\n",
        "#### 8. Lookup Transformation\n",
        "- Look up values from another data flow\n",
        "- Reference data enrichment\n",
        "\n",
        "#### 9. Pivot/Unpivot Transformations\n",
        "- Pivot: Convert rows to columns\n",
        "- Unpivot: Convert columns to rows\n",
        "\n",
        "#### 10. Window Transformation\n",
        "- Window functions (ROW_NUMBER, RANK, etc.)\n",
        "- Partitioning and ordering\n",
        "\n",
        "#### 11. Sink Transformation\n",
        "- Write data to destination\n",
        "- Configure output settings\n",
        "- Handle partitioning\n",
        "\n",
        "### Data Flow Example\n",
        "\n",
        "**Scenario**: Transform sales data\n",
        "\n",
        "```\n",
        "Source (Sales CSV)\n",
        "    ↓\n",
        "Select (Choose columns)\n",
        "    ↓\n",
        "Filter (SalesAmount > 0)\n",
        "    ↓\n",
        "Derived Column (Calculate Total = Quantity * Price)\n",
        "    ↓\n",
        "Aggregate (Group by CustomerID, Sum Total)\n",
        "    ↓\n",
        "Join (with Customer table)\n",
        "    ↓\n",
        "Select (Final columns)\n",
        "    ↓\n",
        "Sink (Write to SQL Database)\n",
        "```\n",
        "\n",
        "### Data Flow Debug Mode\n",
        "\n",
        "- Test data flows interactively\n",
        "- See sample data at each transformation\n",
        "- Validate transformations before publishing\n",
        "- Use sample data or full data\n",
        "\n",
        "### Data Flow Performance\n",
        "\n",
        "**Optimization Techniques:**\n",
        "- **Partitioning**: Configure partitioning strategy\n",
        "- **Caching**: Cache intermediate results\n",
        "- **Sampling**: Use sampling for development\n",
        "- **Cluster Size**: Configure appropriate cluster size\n",
        "- **Broadcast Joins**: For small lookup tables\n",
        "\n",
        "### Best Practices for Data Flows\n",
        "\n",
        "✅ **Start Simple**: Begin with basic transformations\n",
        "\n",
        "✅ **Use Debug Mode**: Test transformations before publishing\n",
        "\n",
        "✅ **Optimize Joins**: Use broadcast joins for small tables\n",
        "\n",
        "✅ **Partitioning**: Configure appropriate partitioning\n",
        "\n",
        "✅ **Documentation**: Add descriptions to transformations\n",
        "\n",
        "✅ **Reusability**: Create reusable data flows\n",
        "\n",
        "✅ **Error Handling**: Handle nulls and data quality issues\n",
        "\n",
        "✅ **Performance**: Monitor and optimize performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration Runtimes\n",
        "\n",
        "**Integration Runtime (IR)** is the compute infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Execute data movement activities\n",
        "- Dispatch activities to compute services\n",
        "- Connect to data sources in different networks\n",
        "- Provide transformation capabilities\n",
        "\n",
        "### Types of Integration Runtimes\n",
        "\n",
        "#### 1. Azure Integration Runtime\n",
        "\n",
        "**Purpose**: Cloud-based, fully managed IR for cloud-to-cloud data movement\n",
        "\n",
        "**Characteristics:**\n",
        "- Serverless and fully managed\n",
        "- Automatically scales\n",
        "- No infrastructure to manage\n",
        "- Free (no additional cost)\n",
        "- Limited to cloud data sources\n",
        "\n",
        "**Use Cases:**\n",
        "- Copy data between cloud services\n",
        "- Execute cloud-based transformations\n",
        "- Connect to Azure services\n",
        "\n",
        "**Limitations:**\n",
        "- Cannot connect to on-premises data sources\n",
        "- Cannot connect to private networks\n",
        "\n",
        "#### 2. Self-Hosted Integration Runtime\n",
        "\n",
        "**Purpose**: IR installed on on-premises machines or VMs for hybrid connectivity\n",
        "\n",
        "**Characteristics:**\n",
        "- Installed on your infrastructure\n",
        "- Connects to on-premises data sources\n",
        "- Can connect to cloud services\n",
        "- Requires maintenance and updates\n",
        "- Supports high-availability setup\n",
        "\n",
        "**Use Cases:**\n",
        "- Connect to on-premises SQL Server\n",
        "- Connect to on-premises file systems\n",
        "- Connect to private networks\n",
        "- Hybrid data movement scenarios\n",
        "\n",
        "**Installation:**\n",
        "- Download and install on Windows machine or VM\n",
        "- Register with Data Factory using authentication key\n",
        "- Can install multiple nodes for high availability\n",
        "\n",
        "**High Availability:**\n",
        "- Install on multiple machines\n",
        "- Automatic failover\n",
        "- Load balancing\n",
        "\n",
        "#### 3. Azure-SSIS Integration Runtime\n",
        "\n",
        "**Purpose**: Lift and shift SQL Server Integration Services (SSIS) packages\n",
        "\n",
        "**Characteristics:**\n",
        "- Runs SSIS packages in Azure\n",
        "- Managed Azure SQL Database or Managed Instance\n",
        "- Supports SSIS catalog\n",
        "- Can join Azure Virtual Network\n",
        "\n",
        "**Use Cases:**\n",
        "- Migrate existing SSIS packages\n",
        "- Run SSIS packages in cloud\n",
        "- Leverage existing SSIS investments\n",
        "\n",
        "### Integration Runtime Selection\n",
        "\n",
        "**When to use Azure IR:**\n",
        "- Cloud-to-cloud data movement\n",
        "- Azure services only\n",
        "- No on-premises connectivity needed\n",
        "\n",
        "**When to use Self-Hosted IR:**\n",
        "- On-premises data sources\n",
        "- Private network connectivity\n",
        "- Hybrid scenarios\n",
        "- Network security requirements\n",
        "\n",
        "**When to use Azure-SSIS IR:**\n",
        "- Existing SSIS packages\n",
        "- SSIS-specific features needed\n",
        "- Complex SSIS transformations\n",
        "\n",
        "### Integration Runtime Configuration\n",
        "\n",
        "**Self-Hosted IR Setup:**\n",
        "1. Create Self-Hosted IR in ADF\n",
        "2. Download and install IR software\n",
        "3. Register IR with authentication key\n",
        "4. Configure network settings\n",
        "5. Test connectivity\n",
        "\n",
        "**Performance Tuning:**\n",
        "- Scale up machine resources\n",
        "- Use multiple nodes for parallel processing\n",
        "- Optimize network connectivity\n",
        "- Monitor IR health and performance\n",
        "\n",
        "### Best Practices for Integration Runtimes\n",
        "\n",
        "✅ **Right IR Type**: Choose appropriate IR for your scenario\n",
        "\n",
        "✅ **High Availability**: Use multiple nodes for Self-Hosted IR\n",
        "\n",
        "✅ **Monitoring**: Monitor IR health and performance\n",
        "\n",
        "✅ **Security**: Secure IR machines and network connections\n",
        "\n",
        "✅ **Updates**: Keep Self-Hosted IR updated\n",
        "\n",
        "✅ **Network**: Optimize network connectivity for performance\n",
        "\n",
        "✅ **Documentation**: Document IR configurations and purposes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Triggers\n",
        "\n",
        "**Triggers** determine when a pipeline execution should be started. They can be scheduled, event-based, or manual.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Automate pipeline execution\n",
        "- Schedule regular data loads\n",
        "- Respond to events (file arrival, etc.)\n",
        "- Coordinate multiple pipelines\n",
        "- Pass parameters to pipelines\n",
        "\n",
        "### Trigger Types\n",
        "\n",
        "#### 1. Schedule Trigger\n",
        "\n",
        "**Purpose**: Run pipeline on a recurring schedule\n",
        "\n",
        "**Characteristics:**\n",
        "- Based on calendar schedule\n",
        "- Supports time zones\n",
        "- Can pass parameters\n",
        "- Supports recurrence patterns\n",
        "\n",
        "**Schedule Patterns:**\n",
        "- Daily, weekly, monthly\n",
        "- Specific days of week\n",
        "- Specific dates\n",
        "- Custom intervals\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"TR_DailyAt2AM\",\n",
        "  \"type\": \"ScheduleTrigger\",\n",
        "  \"typeProperties\": {\n",
        "    \"recurrence\": {\n",
        "      \"frequency\": \"Day\",\n",
        "      \"interval\": 1,\n",
        "      \"startTime\": \"2024-01-01T02:00:00Z\",\n",
        "      \"timeZone\": \"UTC\",\n",
        "      \"schedule\": {\n",
        "        \"hours\": [2],\n",
        "        \"minutes\": [0]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"pipelines\": [\n",
        "    {\n",
        "      \"pipelineReference\": {\n",
        "        \"referenceName\": \"PL_LoadSalesData\"\n",
        "      },\n",
        "      \"parameters\": {\n",
        "        \"loadDate\": \"@formatDateTime(trigger().scheduledTime, 'yyyy-MM-dd')\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### 2. Tumbling Window Trigger\n",
        "\n",
        "**Purpose**: Run pipeline at regular intervals with fixed-size, non-overlapping time windows\n",
        "\n",
        "**Characteristics:**\n",
        "- Fixed-size time windows\n",
        "- Non-overlapping intervals\n",
        "- Supports retry on failure\n",
        "- Can pass window start/end times\n",
        "\n",
        "**Use Cases:**\n",
        "- Hourly data processing\n",
        "- Every 15 minutes processing\n",
        "- Fixed-interval batch processing\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"TR_Hourly\",\n",
        "  \"type\": \"TumblingWindowTrigger\",\n",
        "  \"typeProperties\": {\n",
        "    \"frequency\": \"Hour\",\n",
        "    \"interval\": 1,\n",
        "    \"startTime\": \"2024-01-01T00:00:00Z\",\n",
        "    \"maxConcurrency\": 1,\n",
        "    \"retryPolicy\": {\n",
        "      \"count\": 3,\n",
        "      \"intervalInSeconds\": 30\n",
        "    }\n",
        "  },\n",
        "  \"pipelines\": [\n",
        "    {\n",
        "      \"pipelineReference\": {\n",
        "        \"referenceName\": \"PL_ProcessHourlyData\"\n",
        "      },\n",
        "      \"parameters\": {\n",
        "        \"windowStart\": \"@trigger().outputs.windowStartTime\",\n",
        "        \"windowEnd\": \"@trigger().outputs.windowEndTime\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### 3. Event-Based Trigger\n",
        "\n",
        "**Purpose**: Trigger pipeline when events occur (file arrival, blob creation, etc.)\n",
        "\n",
        "**Characteristics:**\n",
        "- Responds to storage events\n",
        "- Near real-time processing\n",
        "- Event-driven architecture\n",
        "- Supports filtering\n",
        "\n",
        "**Supported Events:**\n",
        "- Blob created\n",
        "- Blob deleted\n",
        "- File created\n",
        "- File deleted\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"name\": \"TR_FileArrival\",\n",
        "  \"type\": \"BlobEventsTrigger\",\n",
        "  \"typeProperties\": {\n",
        "    \"blobPathBeginsWith\": \"/raw-data/sales/\",\n",
        "    \"blobPathEndsWith\": \".csv\",\n",
        "    \"scope\": \"/subscriptions/.../resourceGroups/.../providers/Microsoft.Storage/storageAccounts/mystorage\",\n",
        "    \"events\": [\"Microsoft.Storage.BlobCreated\"]\n",
        "  },\n",
        "  \"pipelines\": [\n",
        "    {\n",
        "      \"pipelineReference\": {\n",
        "        \"referenceName\": \"PL_ProcessFile\"\n",
        "      },\n",
        "      \"parameters\": {\n",
        "        \"fileName\": \"@triggerBody().fileName\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### 4. Custom Event Trigger\n",
        "\n",
        "**Purpose**: Trigger pipeline based on custom events from Azure Event Grid\n",
        "\n",
        "**Use Cases:**\n",
        "- Custom application events\n",
        "- Integration with other Azure services\n",
        "- Complex event scenarios\n",
        "\n",
        "#### 5. Manual Trigger\n",
        "\n",
        "**Purpose**: Trigger pipeline manually (on-demand)\n",
        "\n",
        "**Characteristics:**\n",
        "- No schedule or event\n",
        "- User-initiated\n",
        "- Useful for testing\n",
        "- Can pass parameters\n",
        "\n",
        "### Trigger Parameters\n",
        "\n",
        "Pass parameters from triggers to pipelines:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"pipelines\": [\n",
        "    {\n",
        "      \"pipelineReference\": {\n",
        "        \"referenceName\": \"PL_LoadData\"\n",
        "      },\n",
        "      \"parameters\": {\n",
        "        \"sourcePath\": \"@trigger().scheduledTime\",\n",
        "        \"targetTable\": \"Sales\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Trigger System Variables\n",
        "\n",
        "**Schedule Trigger:**\n",
        "- `@trigger().scheduledTime` - Scheduled execution time\n",
        "- `@trigger().startTime` - Actual start time\n",
        "\n",
        "**Tumbling Window Trigger:**\n",
        "- `@trigger().outputs.windowStartTime` - Window start\n",
        "- `@trigger().outputs.windowEndTime` - Window end\n",
        "\n",
        "**Event Trigger:**\n",
        "- `@triggerBody().fileName` - File name that triggered\n",
        "- `@triggerBody().folderPath` - Folder path\n",
        "\n",
        "### Best Practices for Triggers\n",
        "\n",
        "✅ **Naming Convention**: Use descriptive names (e.g., `TR_DailySalesLoad`)\n",
        "\n",
        "✅ **Time Zones**: Be aware of time zone settings\n",
        "\n",
        "✅ **Parameters**: Pass dynamic values via trigger parameters\n",
        "\n",
        "✅ **Concurrency**: Configure max concurrency appropriately\n",
        "\n",
        "✅ **Retry Policy**: Configure retry policies for transient failures\n",
        "\n",
        "✅ **Monitoring**: Monitor trigger executions\n",
        "\n",
        "✅ **Error Handling**: Handle trigger failures gracefully\n",
        "\n",
        "✅ **Documentation**: Document trigger schedules and purposes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters and Variables\n",
        "\n",
        "**Parameters** and **Variables** make pipelines flexible, reusable, and dynamic.\n",
        "\n",
        "### Parameters\n",
        "\n",
        "**Parameters** are inputs passed to pipelines, datasets, or linked services from outside (triggers, manual execution, or parent pipelines).\n",
        "\n",
        "#### Pipeline Parameters\n",
        "\n",
        "**Define Parameters:**\n",
        "```json\n",
        "{\n",
        "  \"parameters\": {\n",
        "    \"sourceContainer\": {\n",
        "      \"type\": \"String\",\n",
        "      \"defaultValue\": \"raw-data\"\n",
        "    },\n",
        "    \"targetTable\": {\n",
        "      \"type\": \"String\"\n",
        "    },\n",
        "    \"loadDate\": {\n",
        "      \"type\": \"String\",\n",
        "      \"defaultValue\": \"@formatDateTime(utcnow(), 'yyyy-MM-dd')\"\n",
        "    },\n",
        "    \"rowCount\": {\n",
        "      \"type\": \"Int32\",\n",
        "      \"defaultValue\": 1000\n",
        "    },\n",
        "    \"isProduction\": {\n",
        "      \"type\": \"Bool\",\n",
        "      \"defaultValue\": false\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Access Parameters:**\n",
        "- In expressions: `@pipeline().parameters.sourceContainer`\n",
        "- In activities: Reference in activity properties\n",
        "- In datasets: Pass as dataset parameters\n",
        "\n",
        "**Pass Parameters from Trigger:**\n",
        "```json\n",
        "{\n",
        "  \"pipelines\": [\n",
        "    {\n",
        "      \"pipelineReference\": {\n",
        "        \"referenceName\": \"PL_LoadData\"\n",
        "      },\n",
        "      \"parameters\": {\n",
        "        \"loadDate\": \"@formatDateTime(trigger().scheduledTime, 'yyyy-MM-dd')\",\n",
        "        \"sourceContainer\": \"raw-data\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### Dataset Parameters\n",
        "\n",
        "**Define Dataset Parameters:**\n",
        "```json\n",
        "{\n",
        "  \"parameters\": {\n",
        "    \"folderPath\": {\n",
        "      \"type\": \"String\"\n",
        "    },\n",
        "    \"fileName\": {\n",
        "      \"type\": \"String\"\n",
        "    }\n",
        "  },\n",
        "  \"typeProperties\": {\n",
        "    \"location\": {\n",
        "      \"folderPath\": {\n",
        "        \"value\": \"@dataset().folderPath\",\n",
        "        \"type\": \"Expression\"\n",
        "      },\n",
        "      \"fileName\": {\n",
        "        \"value\": \"@dataset().fileName\",\n",
        "        \"type\": \"Expression\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Pass Parameters to Dataset:**\n",
        "```json\n",
        "{\n",
        "  \"inputs\": [\n",
        "    {\n",
        "      \"referenceName\": \"DS_SalesCSV\",\n",
        "      \"type\": \"DatasetReference\",\n",
        "      \"parameters\": {\n",
        "        \"folderPath\": \"sales/2024/01\",\n",
        "        \"fileName\": \"sales_20240101.csv\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Variables\n",
        "\n",
        "**Variables** are internal values used within pipelines for logic and calculations.\n",
        "\n",
        "**Define Variables:**\n",
        "```json\n",
        "{\n",
        "  \"variables\": {\n",
        "    \"rowCount\": {\n",
        "      \"type\": \"Int32\",\n",
        "      \"defaultValue\": 0\n",
        "    },\n",
        "    \"processDate\": {\n",
        "      \"type\": \"String\",\n",
        "      \"defaultValue\": \"@formatDateTime(utcnow(), 'yyyy-MM-dd')\"\n",
        "    },\n",
        "    \"fileList\": {\n",
        "      \"type\": \"Array\",\n",
        "      \"defaultValue\": []\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Set Variables:**\n",
        "Use `Set Variable` activity:\n",
        "```json\n",
        "{\n",
        "  \"name\": \"SetRowCount\",\n",
        "  \"type\": \"SetVariable\",\n",
        "  \"typeProperties\": {\n",
        "    \"variableName\": \"rowCount\",\n",
        "    \"value\": {\n",
        "      \"value\": \"@activity('GetMetadata').output.itemCount\",\n",
        "      \"type\": \"Expression\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Access Variables:**\n",
        "- In expressions: `@variables('rowCount')`\n",
        "- In activities: Reference in activity properties\n",
        "\n",
        "### Expressions\n",
        "\n",
        "**Expressions** are used to create dynamic values using functions and operators.\n",
        "\n",
        "#### Common Expression Functions\n",
        "\n",
        "**String Functions:**\n",
        "- `concat(str1, str2, ...)` - Concatenate strings\n",
        "- `substring(str, start, length)` - Extract substring\n",
        "- `toUpper(str)` - Convert to uppercase\n",
        "- `toLower(str)` - Convert to lowercase\n",
        "- `replace(str, old, new)` - Replace text\n",
        "\n",
        "**Date/Time Functions:**\n",
        "- `utcnow()` - Current UTC time\n",
        "- `formatDateTime(timestamp, format)` - Format date\n",
        "- `addDays(timestamp, days)` - Add days\n",
        "- `addHours(timestamp, hours)` - Add hours\n",
        "- `year(timestamp)` - Get year\n",
        "- `month(timestamp)` - Get month\n",
        "- `day(timestamp)` - Get day\n",
        "\n",
        "**Numeric Functions:**\n",
        "- `add(value1, value2)` - Addition\n",
        "- `sub(value1, value2)` - Subtraction\n",
        "- `mul(value1, value2)` - Multiplication\n",
        "- `div(value1, value2)` - Division\n",
        "- `mod(value1, value2)` - Modulo\n",
        "\n",
        "**Logical Functions:**\n",
        "- `equals(value1, value2)` - Equality check\n",
        "- `greater(value1, value2)` - Greater than\n",
        "- `less(value1, value2)` - Less than\n",
        "- `and(condition1, condition2)` - Logical AND\n",
        "- `or(condition1, condition2)` - Logical OR\n",
        "- `not(condition)` - Logical NOT\n",
        "- `iif(condition, trueValue, falseValue)` - Conditional\n",
        "\n",
        "**Array Functions:**\n",
        "- `length(array)` - Array length\n",
        "- `first(array)` - First element\n",
        "- `last(array)` - Last element\n",
        "- `contains(array, value)` - Check if contains\n",
        "\n",
        "**Activity Functions:**\n",
        "- `activity('ActivityName').output` - Activity output\n",
        "- `activity('ActivityName').error` - Activity error\n",
        "- `activity('ActivityName').status` - Activity status\n",
        "\n",
        "### Expression Examples\n",
        "\n",
        "```json\n",
        "// Dynamic file path with date\n",
        "\"folderPath\": {\n",
        "  \"value\": \"@concat('sales/', formatDateTime(utcnow(), 'yyyy/MM/dd'))\",\n",
        "  \"type\": \"Expression\"\n",
        "}\n",
        "\n",
        "// Conditional value\n",
        "\"container\": {\n",
        "  \"value\": \"@iif(equals(pipeline().parameters.environment, 'prod'), 'prod-data', 'dev-data')\",\n",
        "  \"type\": \"Expression\"\n",
        "}\n",
        "\n",
        "// Calculate date range\n",
        "\"startDate\": {\n",
        "  \"value\": \"@formatDateTime(addDays(utcnow(), -7), 'yyyy-MM-dd')\",\n",
        "  \"type\": \"Expression\"\n",
        "}\n",
        "\n",
        "// Activity output\n",
        "\"rowCount\": {\n",
        "  \"value\": \"@activity('GetMetadata').output.itemCount\",\n",
        "  \"type\": \"Expression\"\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices for Parameters and Variables\n",
        "\n",
        "✅ **Parameterize Everything**: Make paths, table names, and configurations parameterized\n",
        "\n",
        "✅ **Default Values**: Provide default values for parameters when possible\n",
        "\n",
        "✅ **Naming**: Use clear, descriptive names\n",
        "\n",
        "✅ **Type Safety**: Use appropriate data types\n",
        "\n",
        "✅ **Documentation**: Document parameter purposes and expected values\n",
        "\n",
        "✅ **Validation**: Validate parameter values when possible\n",
        "\n",
        "✅ **Reusability**: Design for reuse across environments\n",
        "\n",
        "✅ **Expressions**: Use expressions for dynamic values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling and Monitoring\n",
        "\n",
        "Proper error handling and monitoring are essential for reliable data pipelines.\n",
        "\n",
        "### Error Handling Strategies\n",
        "\n",
        "#### 1. Activity Retry Policy\n",
        "\n",
        "Configure retry for transient failures:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"CopyData\",\n",
        "  \"type\": \"Copy\",\n",
        "  \"policy\": {\n",
        "    \"timeout\": \"01:00:00\",\n",
        "    \"retry\": 3,\n",
        "    \"retryIntervalInSeconds\": 30,\n",
        "    \"retryCount\": 3\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Retry Policy Properties:**\n",
        "- `retry` - Number of retry attempts\n",
        "- `retryIntervalInSeconds` - Wait time between retries\n",
        "- `timeout` - Maximum execution time\n",
        "\n",
        "#### 2. Activity Dependencies\n",
        "\n",
        "Control flow based on success/failure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"ActivityB\",\n",
        "  \"dependsOn\": [\n",
        "    {\n",
        "      \"activity\": \"ActivityA\",\n",
        "      \"dependencyConditions\": [\"Succeeded\"]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Dependency Conditions:**\n",
        "- `Succeeded` - Continue only if succeeded\n",
        "- `Failed` - Continue only if failed\n",
        "- `Completed` - Continue regardless of status\n",
        "- `Skipped` - Continue only if skipped\n",
        "\n",
        "#### 3. If Condition Activity\n",
        "\n",
        "Implement conditional error handling:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"HandleError\",\n",
        "  \"type\": \"IfCondition\",\n",
        "  \"typeProperties\": {\n",
        "    \"expression\": {\n",
        "      \"value\": \"@equals(activity('CopyData').status, 'Failed')\",\n",
        "      \"type\": \"Expression\"\n",
        "    },\n",
        "    \"ifTrueActivities\": [\n",
        "      {\n",
        "        \"name\": \"SendAlert\",\n",
        "        \"type\": \"WebActivity\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### 4. Try-Catch Pattern\n",
        "\n",
        "Use activity dependencies to implement try-catch:\n",
        "\n",
        "```\n",
        "Try Activity\n",
        "    ↓ (Succeeded)\n",
        "Success Activity\n",
        "    ↓\n",
        "Continue Pipeline\n",
        "\n",
        "Try Activity\n",
        "    ↓ (Failed)\n",
        "Catch Activity (Error Handling)\n",
        "    ↓\n",
        "Continue or Fail Pipeline\n",
        "```\n",
        "\n",
        "#### 5. Set Variable for Error Tracking\n",
        "\n",
        "Track errors in variables:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"SetErrorVariable\",\n",
        "  \"type\": \"SetVariable\",\n",
        "  \"typeProperties\": {\n",
        "    \"variableName\": \"errorMessage\",\n",
        "    \"value\": {\n",
        "      \"value\": \"@activity('CopyData').error.message\",\n",
        "      \"type\": \"Expression\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Monitoring\n",
        "\n",
        "#### Monitor Hub in ADF Studio\n",
        "\n",
        "**Pipeline Runs:**\n",
        "- View all pipeline executions\n",
        "- Filter by status, time range, pipeline name\n",
        "- See execution details and duration\n",
        "- View activity-level details\n",
        "\n",
        "**Activity Runs:**\n",
        "- View individual activity executions\n",
        "- See input/output data\n",
        "- View error messages and stack traces\n",
        "- Monitor performance metrics\n",
        "\n",
        "**Trigger Runs:**\n",
        "- View trigger executions\n",
        "- See trigger status and timing\n",
        "- Monitor trigger failures\n",
        "\n",
        "#### Key Metrics to Monitor\n",
        "\n",
        "**Pipeline Metrics:**\n",
        "- Success rate\n",
        "- Execution duration\n",
        "- Failure rate\n",
        "- Average execution time\n",
        "\n",
        "**Activity Metrics:**\n",
        "- Data volume processed\n",
        "- Rows copied/transformed\n",
        "- Execution time\n",
        "- Throughput (rows/second)\n",
        "\n",
        "**Integration Runtime Metrics:**\n",
        "- CPU usage\n",
        "- Memory usage\n",
        "- Network throughput\n",
        "- Queue length\n",
        "\n",
        "#### Alerts and Notifications\n",
        "\n",
        "**Azure Monitor Integration:**\n",
        "- Create alerts for pipeline failures\n",
        "- Set up email notifications\n",
        "- Configure webhook notifications\n",
        "- Monitor costs\n",
        "\n",
        "**Alert Conditions:**\n",
        "- Pipeline failure\n",
        "- Activity failure\n",
        "- Execution time threshold\n",
        "- Data volume threshold\n",
        "\n",
        "#### Logging\n",
        "\n",
        "**Activity Logging:**\n",
        "- Enable logging for debugging\n",
        "- Log input/output data\n",
        "- Log variable values\n",
        "- Log custom messages\n",
        "\n",
        "**Diagnostic Settings:**\n",
        "- Enable diagnostic logs\n",
        "- Send logs to Log Analytics\n",
        "- Archive logs to storage\n",
        "- Stream logs to Event Hub\n",
        "\n",
        "### Best Practices for Error Handling\n",
        "\n",
        "✅ **Retry Policy**: Configure retry for transient failures\n",
        "\n",
        "✅ **Timeout**: Set appropriate timeouts\n",
        "\n",
        "✅ **Error Handling**: Implement comprehensive error handling\n",
        "\n",
        "✅ **Logging**: Enable logging for debugging\n",
        "\n",
        "✅ **Alerts**: Set up alerts for failures\n",
        "\n",
        "✅ **Monitoring**: Regularly monitor pipeline health\n",
        "\n",
        "✅ **Documentation**: Document error handling strategies\n",
        "\n",
        "✅ **Testing**: Test error scenarios\n",
        "\n",
        "✅ **Notifications**: Configure notifications for critical failures\n",
        "\n",
        "✅ **Recovery**: Plan for data recovery after failures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Patterns and Use Cases\n",
        "\n",
        "Understanding common patterns helps you design effective data pipelines.\n",
        "\n",
        "### Pattern 1: Incremental Load\n",
        "\n",
        "**Scenario**: Load only new or changed data since last run\n",
        "\n",
        "**Approach:**\n",
        "1. Use watermark column (last modified date, ID)\n",
        "2. Store last watermark value\n",
        "3. Query source for records > watermark\n",
        "4. Update watermark after successful load\n",
        "\n",
        "**Components:**\n",
        "- Lookup activity to get watermark\n",
        "- Copy activity with filtered query\n",
        "- Stored procedure to update watermark\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Lookup (Get LastLoadDate)\n",
        "    ↓\n",
        "Copy (SELECT * FROM Sales WHERE SaleDate > @LastLoadDate)\n",
        "    ↓\n",
        "Stored Procedure (Update LastLoadDate)\n",
        "```\n",
        "\n",
        "### Pattern 2: File Processing Loop\n",
        "\n",
        "**Scenario**: Process multiple files in a folder\n",
        "\n",
        "**Approach:**\n",
        "1. Get list of files using Get Metadata\n",
        "2. Use ForEach activity to loop through files\n",
        "3. Process each file in parallel or sequentially\n",
        "\n",
        "**Components:**\n",
        "- Get Metadata activity\n",
        "- ForEach activity\n",
        "- Copy or Data Flow activity\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Get Metadata (List Files)\n",
        "    ↓\n",
        "ForEach (Loop through files)\n",
        "    ├── Copy File\n",
        "    └── Archive File\n",
        "```\n",
        "\n",
        "### Pattern 3: Data Validation\n",
        "\n",
        "**Scenario**: Validate data before loading\n",
        "\n",
        "**Approach:**\n",
        "1. Load data to staging\n",
        "2. Validate data quality\n",
        "3. Load to target only if validation passes\n",
        "\n",
        "**Components:**\n",
        "- Copy to staging\n",
        "- Validation activities\n",
        "- If Condition for validation result\n",
        "- Copy to target or error handling\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Copy to Staging\n",
        "    ↓\n",
        "Validation (Check row count, nulls, etc.)\n",
        "    ↓\n",
        "If Condition (Validation passed?)\n",
        "    ├── Yes → Copy to Target\n",
        "    └── No → Send Alert & Log Error\n",
        "```\n",
        "\n",
        "### Pattern 4: Slowly Changing Dimension (SCD)\n",
        "\n",
        "**Scenario**: Handle dimension table updates (Type 2 SCD)\n",
        "\n",
        "**Approach:**\n",
        "1. Compare source with target\n",
        "2. Identify new, changed, and unchanged records\n",
        "3. Insert new records\n",
        "4. Update changed records (end date old, insert new)\n",
        "\n",
        "**Components:**\n",
        "- Data Flow for comparison\n",
        "- Stored procedures for SCD logic\n",
        "- Multiple copy activities\n",
        "\n",
        "### Pattern 5: Data Lake to Data Warehouse\n",
        "\n",
        "**Scenario**: Load data from Data Lake to Data Warehouse\n",
        "\n",
        "**Approach:**\n",
        "1. Read from Data Lake (Parquet/CSV)\n",
        "2. Transform data (Data Flow)\n",
        "3. Load to staging table\n",
        "4. Merge to final table\n",
        "\n",
        "**Components:**\n",
        "- Copy or Data Flow from Data Lake\n",
        "- Data Flow for transformation\n",
        "- Copy to staging\n",
        "- Stored procedure for merge\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Data Flow (Read from Data Lake & Transform)\n",
        "    ↓\n",
        "Copy to Staging Table\n",
        "    ↓\n",
        "Stored Procedure (Merge to Final Table)\n",
        "```\n",
        "\n",
        "### Pattern 6: Parallel Processing\n",
        "\n",
        "**Scenario**: Process multiple data sources in parallel\n",
        "\n",
        "**Approach:**\n",
        "1. Create multiple parallel branches\n",
        "2. No dependencies between branches\n",
        "3. All branches execute simultaneously\n",
        "\n",
        "**Components:**\n",
        "- Multiple activities with no dependencies\n",
        "- Or use ForEach with parallel execution\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Pipeline Start\n",
        "    ├── Copy Source1 → Target1\n",
        "    ├── Copy Source2 → Target2\n",
        "    └── Copy Source3 → Target3\n",
        "    ↓\n",
        "All Complete → Final Activity\n",
        "```\n",
        "\n",
        "### Pattern 7: Conditional Execution\n",
        "\n",
        "**Scenario**: Execute activities based on conditions\n",
        "\n",
        "**Approach:**\n",
        "1. Use Lookup to get configuration\n",
        "2. Use If Condition based on lookup result\n",
        "3. Execute different paths\n",
        "\n",
        "**Components:**\n",
        "- Lookup activity\n",
        "- If Condition activity\n",
        "- Conditional activities\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Lookup (Get Config: ProcessType)\n",
        "    ↓\n",
        "If Condition (ProcessType == 'Full')\n",
        "    ├── Yes → Full Load\n",
        "    └── No → Incremental Load\n",
        "```\n",
        "\n",
        "### Pattern 8: Wait for File\n",
        "\n",
        "**Scenario**: Wait for file to arrive before processing\n",
        "\n",
        "**Approach:**\n",
        "1. Use Until activity\n",
        "2. Check for file existence\n",
        "3. Wait if file doesn't exist\n",
        "4. Process when file arrives\n",
        "\n",
        "**Components:**\n",
        "- Until activity\n",
        "- Get Metadata activity\n",
        "- Wait activity\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Until (File Exists)\n",
        "    ├── Get Metadata (Check File)\n",
        "    └── Wait (30 seconds)\n",
        "    ↓\n",
        "File Found → Process File\n",
        "```\n",
        "\n",
        "### Pattern 9: Data Quality Checks\n",
        "\n",
        "**Scenario**: Ensure data quality before processing\n",
        "\n",
        "**Approach:**\n",
        "1. Load to staging\n",
        "2. Run data quality checks\n",
        "3. Generate quality report\n",
        "4. Proceed or fail based on results\n",
        "\n",
        "**Components:**\n",
        "- Data Flow for quality checks\n",
        "- Validation activities\n",
        "- Reporting activities\n",
        "\n",
        "### Pattern 10: Master Pipeline\n",
        "\n",
        "**Scenario**: Orchestrate multiple pipelines\n",
        "\n",
        "**Approach:**\n",
        "1. Create master pipeline\n",
        "2. Execute child pipelines in sequence or parallel\n",
        "3. Handle errors from child pipelines\n",
        "\n",
        "**Components:**\n",
        "- Execute Pipeline activity\n",
        "- Activity dependencies\n",
        "- Error handling\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Master Pipeline\n",
        "    ├── Execute Pipeline (Load Customers)\n",
        "    ├── Execute Pipeline (Load Products)\n",
        "    └── Execute Pipeline (Load Sales)\n",
        "    ↓\n",
        "All Complete → Final Processing\n",
        "```\n",
        "\n",
        "### Best Practices for Patterns\n",
        "\n",
        "✅ **Reusability**: Create reusable patterns as templates\n",
        "\n",
        "✅ **Documentation**: Document pattern purposes and usage\n",
        "\n",
        "✅ **Error Handling**: Include error handling in patterns\n",
        "\n",
        "✅ **Parameterization**: Make patterns parameterized\n",
        "\n",
        "✅ **Testing**: Test patterns thoroughly\n",
        "\n",
        "✅ **Performance**: Optimize patterns for performance\n",
        "\n",
        "✅ **Monitoring**: Add monitoring to patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices Summary\n",
        "\n",
        "Following best practices ensures reliable, maintainable, and performant data pipelines.\n",
        "\n",
        "### Design Best Practices\n",
        "\n",
        "✅ **Single Responsibility**: Each pipeline should have one clear purpose\n",
        "\n",
        "✅ **Modularity**: Break complex pipelines into smaller, reusable pipelines\n",
        "\n",
        "✅ **Parameterization**: Parameterize everything (paths, table names, configurations)\n",
        "\n",
        "✅ **Naming Conventions**: Use consistent naming (prefixes: LS_, DS_, PL_, TR_)\n",
        "\n",
        "✅ **Documentation**: Add descriptions and annotations to all components\n",
        "\n",
        "✅ **Version Control**: Use Git integration for version control\n",
        "\n",
        "✅ **Environment Separation**: Separate dev, test, and prod environments\n",
        "\n",
        "### Performance Best Practices\n",
        "\n",
        "✅ **Parallel Copy**: Enable parallel copy for large data volumes\n",
        "\n",
        "✅ **Staging**: Use staging for better performance in database copies\n",
        "\n",
        "✅ **Partitioning**: Use partitioned datasets for large files\n",
        "\n",
        "✅ **Filter at Source**: Filter data at source to reduce data volume\n",
        "\n",
        "✅ **Compression**: Use compression for file transfers\n",
        "\n",
        "✅ **Data Flow Optimization**: Optimize Data Flows (partitioning, caching)\n",
        "\n",
        "✅ **Integration Runtime**: Choose appropriate IR and scale appropriately\n",
        "\n",
        "✅ **Batch Size**: Configure appropriate batch sizes\n",
        "\n",
        "### Security Best Practices\n",
        "\n",
        "✅ **Azure Key Vault**: Store all secrets in Azure Key Vault\n",
        "\n",
        "✅ **Managed Identity**: Use Managed Identity when possible\n",
        "\n",
        "✅ **Least Privilege**: Grant minimum required permissions\n",
        "\n",
        "✅ **Network Security**: Use private endpoints and VNet integration\n",
        "\n",
        "✅ **Audit Logging**: Enable audit logs and monitoring\n",
        "\n",
        "✅ **Credential Rotation**: Regularly rotate credentials\n",
        "\n",
        "✅ **Access Control**: Use RBAC for access control\n",
        "\n",
        "### Error Handling Best Practices\n",
        "\n",
        "✅ **Retry Policy**: Configure retry for transient failures\n",
        "\n",
        "✅ **Timeout**: Set appropriate timeouts\n",
        "\n",
        "✅ **Error Handling**: Implement comprehensive error handling\n",
        "\n",
        "✅ **Alerts**: Set up alerts for failures\n",
        "\n",
        "✅ **Logging**: Enable logging for debugging\n",
        "\n",
        "✅ **Notifications**: Configure notifications for critical failures\n",
        "\n",
        "✅ **Recovery**: Plan for data recovery after failures\n",
        "\n",
        "### Monitoring Best Practices\n",
        "\n",
        "✅ **Regular Monitoring**: Monitor pipeline health regularly\n",
        "\n",
        "✅ **Metrics**: Track key metrics (success rate, duration, throughput)\n",
        "\n",
        "✅ **Alerts**: Set up proactive alerts\n",
        "\n",
        "✅ **Dashboards**: Create monitoring dashboards\n",
        "\n",
        "✅ **Cost Monitoring**: Monitor and optimize costs\n",
        "\n",
        "✅ **Performance Monitoring**: Track and optimize performance\n",
        "\n",
        "### Development Best Practices\n",
        "\n",
        "✅ **Testing**: Test pipelines thoroughly before production\n",
        "\n",
        "✅ **Debug Mode**: Use debug mode for Data Flows\n",
        "\n",
        "✅ **Sample Data**: Use sample data during development\n",
        "\n",
        "✅ **Incremental Development**: Build pipelines incrementally\n",
        "\n",
        "✅ **Code Review**: Review pipeline designs and code\n",
        "\n",
        "✅ **Documentation**: Maintain up-to-date documentation\n",
        "\n",
        "### Cost Optimization Best Practices\n",
        "\n",
        "✅ **Right-Sizing**: Use appropriate compute sizes\n",
        "\n",
        "✅ **Scheduling**: Schedule pipelines during off-peak hours when possible\n",
        "\n",
        "✅ **Data Volume**: Minimize unnecessary data movement\n",
        "\n",
        "✅ **Caching**: Use caching in Data Flows\n",
        "\n",
        "✅ **Auto-Pause**: Configure auto-pause for compute resources\n",
        "\n",
        "✅ **Monitoring**: Monitor and optimize costs regularly\n",
        "\n",
        "### Maintenance Best Practices\n",
        "\n",
        "✅ **Regular Updates**: Keep components updated\n",
        "\n",
        "✅ **Cleanup**: Remove unused pipelines, datasets, and linked services\n",
        "\n",
        "✅ **Documentation**: Keep documentation updated\n",
        "\n",
        "✅ **Review**: Regularly review and optimize pipelines\n",
        "\n",
        "✅ **Backup**: Backup pipeline definitions\n",
        "\n",
        "✅ **Disaster Recovery**: Plan for disaster recovery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ADF Studio Navigation Guide\n",
        "\n",
        "When you open ADF Studio, here's what you'll see and how to navigate:\n",
        "\n",
        "### Main Tabs\n",
        "\n",
        "#### 1. Author Tab\n",
        "**Purpose**: Design and create pipelines, datasets, linked services, and data flows\n",
        "\n",
        "**Left Pane - Factory Resources:**\n",
        "- **Pipelines**: Create and manage pipelines\n",
        "- **Data flows**: Create and manage data flows\n",
        "- **Datasets**: Create and manage datasets\n",
        "- **Linked services**: Create and manage linked services\n",
        "- **Integration runtimes**: Manage integration runtimes\n",
        "- **Triggers**: Create and manage triggers\n",
        "- **Power Query**: Create Power Query data flows\n",
        "\n",
        "**Canvas Area:**\n",
        "- Visual pipeline designer\n",
        "- Drag-and-drop activities\n",
        "- Configure activity properties\n",
        "- Set up dependencies\n",
        "\n",
        "**Properties Pane:**\n",
        "- Configure component properties\n",
        "- Set parameters\n",
        "- Add annotations\n",
        "\n",
        "#### 2. Monitor Tab\n",
        "**Purpose**: Monitor pipeline runs, activity executions, and trigger runs\n",
        "\n",
        "**Views:**\n",
        "- **Pipeline runs**: All pipeline executions\n",
        "- **Trigger runs**: All trigger executions\n",
        "- **Integration runtime**: IR status and metrics\n",
        "- **Data flow debug sessions**: Active debug sessions\n",
        "\n",
        "**Filters:**\n",
        "- Filter by status (Succeeded, Failed, In Progress)\n",
        "- Filter by time range\n",
        "- Filter by pipeline/trigger name\n",
        "\n",
        "**Details:**\n",
        "- View execution details\n",
        "- See activity-level information\n",
        "- View input/output data\n",
        "- Check error messages\n",
        "\n",
        "#### 3. Manage Tab\n",
        "**Purpose**: Manage factory settings, Git configuration, and factory resources\n",
        "\n",
        "**Sections:**\n",
        "- **Git configuration**: Connect to Git repository\n",
        "- **Global parameters**: Define factory-level parameters\n",
        "- **Managed private endpoints**: Manage private endpoints\n",
        "- **Customer-managed keys**: Configure encryption keys\n",
        "\n",
        "#### 4. Gallery Tab\n",
        "**Purpose**: Browse templates and samples\n",
        "\n",
        "**Content:**\n",
        "- Pipeline templates\n",
        "- Data flow templates\n",
        "- Sample pipelines\n",
        "- Quick start guides\n",
        "\n",
        "### Key Actions in ADF Studio\n",
        "\n",
        "#### Creating a Pipeline\n",
        "1. Go to Author tab\n",
        "2. Click \"+\" next to Pipelines\n",
        "3. Name your pipeline\n",
        "4. Drag activities to canvas\n",
        "5. Configure activities\n",
        "6. Set dependencies\n",
        "7. Publish\n",
        "\n",
        "#### Creating a Linked Service\n",
        "1. Go to Author tab\n",
        "2. Click \"+\" next to Linked services\n",
        "3. Choose connector type\n",
        "4. Configure connection details\n",
        "5. Test connection\n",
        "6. Create\n",
        "\n",
        "#### Creating a Dataset\n",
        "1. Go to Author tab\n",
        "2. Click \"+\" next to Datasets\n",
        "3. Choose data store type\n",
        "4. Select linked service\n",
        "5. Configure data structure\n",
        "6. Create\n",
        "\n",
        "#### Creating a Data Flow\n",
        "1. Go to Author tab\n",
        "2. Click \"+\" next to Data flows\n",
        "3. Add source transformation\n",
        "4. Add transformation steps\n",
        "5. Add sink transformation\n",
        "6. Configure transformations\n",
        "\n",
        "#### Creating a Trigger\n",
        "1. Go to Author tab\n",
        "2. Click \"+\" next to Triggers\n",
        "3. Choose trigger type\n",
        "4. Configure schedule/event\n",
        "5. Attach pipelines\n",
        "6. Start trigger\n",
        "\n",
        "#### Monitoring Pipeline Runs\n",
        "1. Go to Monitor tab\n",
        "2. Select Pipeline runs\n",
        "3. Filter as needed\n",
        "4. Click on run to see details\n",
        "5. View activity runs\n",
        "6. Check logs and errors\n",
        "\n",
        "### Tips for Using ADF Studio\n",
        "\n",
        "💡 **Use Search**: Search for pipelines, datasets, or linked services\n",
        "💡 **Use Templates**: Start with templates from Gallery\n",
        "💡 **Debug Mode**: Use debug mode for Data Flows\n",
        "💡 **Validate**: Always validate before publishing\n",
        "💡 **Test Connections**: Test linked service connections\n",
        "💡 **Use Expressions**: Use expression builder for dynamic values\n",
        "💡 **Keyboard Shortcuts**: Learn keyboard shortcuts for efficiency\n",
        "💡 **Auto-save**: Enable auto-save for drafts\n",
        "💡 **Version History**: Use Git for version history\n",
        "💡 **Export/Import**: Export pipelines for backup or sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This comprehensive guide has covered:\n",
        "\n",
        "✅ **Azure Data Factory Overview**: What ADF is and its key features\n",
        "✅ **Architecture**: Components and their relationships\n",
        "✅ **Linked Services**: Connection definitions to data sources and compute\n",
        "✅ **Datasets**: Data structure definitions and locations\n",
        "✅ **Pipelines**: Workflow orchestration and activity coordination\n",
        "✅ **Activities**: Various activity types (Copy, Data Flow, Control Flow, etc.)\n",
        "✅ **Data Flows**: Visual data transformations on Spark\n",
        "✅ **Integration Runtimes**: Compute infrastructure (Azure, Self-Hosted, SSIS)\n",
        "✅ **Triggers**: Scheduling and event-based execution\n",
        "✅ **Parameters & Variables**: Making pipelines dynamic and reusable\n",
        "✅ **Error Handling**: Retry policies, dependencies, and error management\n",
        "✅ **Monitoring**: Tracking pipeline health and performance\n",
        "✅ **Common Patterns**: Real-world use cases and solutions\n",
        "✅ **Best Practices**: Design, performance, security, and maintenance guidelines\n",
        "✅ **ADF Studio Navigation**: How to use the ADF Studio interface\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **ADF is Serverless**: No infrastructure to manage, scales automatically\n",
        "2. **Visual Design**: Build pipelines using drag-and-drop interface\n",
        "3. **90+ Connectors**: Connect to various data sources out of the box\n",
        "4. **Code-Free ETL**: Build data pipelines without writing code\n",
        "5. **Hybrid Integration**: Connect to both cloud and on-premises data sources\n",
        "6. **Parameterization**: Make pipelines flexible and reusable\n",
        "7. **Monitoring**: Built-in monitoring and alerting capabilities\n",
        "8. **Best Practices**: Follow best practices for reliable pipelines\n",
        "\n",
        "### Component Hierarchy Recap\n",
        "\n",
        "```\n",
        "Azure Data Factory\n",
        "│\n",
        "├── Linked Services (Connections)\n",
        "│   ├── Data Store Linked Services\n",
        "│   └── Compute Linked Services\n",
        "│\n",
        "├── Datasets (Data Definitions)\n",
        "│   ├── Source Datasets\n",
        "│   └── Sink Datasets\n",
        "│\n",
        "├── Pipelines (Workflows)\n",
        "│   ├── Activities\n",
        "│   │   ├── Data Movement (Copy)\n",
        "│   │   ├── Data Transformation (Data Flow, Stored Procedure)\n",
        "│   │   └── Control Flow (If, ForEach, Wait, Until)\n",
        "│   ├── Parameters\n",
        "│   └── Variables\n",
        "│\n",
        "├── Data Flows (Transformations)\n",
        "│   ├── Source Transformations\n",
        "│   ├── Transform Steps\n",
        "│   └── Sink Transformations\n",
        "│\n",
        "├── Integration Runtimes (Compute)\n",
        "│   ├── Azure IR\n",
        "│   ├── Self-Hosted IR\n",
        "│   └── Azure-SSIS IR\n",
        "│\n",
        "└── Triggers (Scheduling)\n",
        "    ├── Schedule Triggers\n",
        "    ├── Tumbling Window Triggers\n",
        "    └── Event-Based Triggers\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Now that you understand the concepts:\n",
        "\n",
        "1. **Open ADF Studio**: Navigate to your Azure Data Factory instance\n",
        "2. **Explore the Interface**: Familiarize yourself with Author, Monitor, and Manage tabs\n",
        "]\n",
        "### Additional Resources\n",
        "\n",
        "- **Azure Data Factory Documentation**: Official Microsoft documentation\n",
        "- **ADF Templates**: Browse templates in the Gallery tab\n",
        "- **Azure Data Factory Blog**: Latest updates and best practices\n",
        "- **Community Forums**: Get help from the community\n",
        "- **Training Modules**: Microsoft Learn modules on ADF\n",
        "\n",
        "---\n",
        "\n",
        "The best way to learn Azure Data Factory is by doing. Use this guide as a reference, then practice in ADF Studio to build real pipelines!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
